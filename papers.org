* TODO Swish: a Self-Gated Activation Function
[[cite:ramachandran17_swish][Ramachandran et al. 2017: Swish]]
* TODO Related Pins At Pinterest: The Evolution of a Real-World
[[cite:liu17_relat_pins_at_pinter][Liu et al. 2017: Related Pins At Pinterest]]
* TODO Efficient Natural Language Response Suggestion for
[[cite:henderson17_effic_natur_languag_respon_sugges_smart_reply][Henderson et al. 2017: Efficient Natural Language]]
* TODO Deep Reinforcement Learning From Human Preferences
[[cite:christiano-2017-deep-reinf][Christiano et al. 2017: Deep Reinforcement Learning]]
* TODO Understanding Generalization and Stochastic Gradient Descent
[[cite:smith-2017-under-gener][Smith & Le 2017: Understanding Generalization Stochastic]]
* TODO Do Convolutional Neural Networks Learn Class Hierarchy?
[[cite:alsallakh-2017-do-convol][Alsallakh et al. 2017: Do Convolutional Neural]]
* TODO Google's Neural Machine Translation System: Bridging the Gap
[[cite:wu-2016-googl-neural][Wu et al. 2016: Googles Neural Machine]]
* TODO Unsupervised Machine Translation Using Monolingual
[[cite:lample-2017-unsup-machin][Lample et al. 2017: Unsupervised Machine Translation]]
* TODO Attention Is All You Need
[[cite:vaswani-2017-atten-is][Vaswani et al. 2017: Attention Is All]]
* TODO One Model To Learn Them All
[[cite:kaiser-2017-one-model][Kaiser et al. 2017: One Model To]]
* TODO Scalable Trust-Region Method for Deep Reinforcement Learning
[[cite:wu-2017-scalab-trust][Wu et al. 2017: Scalable Trust Region]]
* TODO Replace Or Retrieve Keywords In Documents At Scale
[[cite:singh-2017-replac-or][Singh 2017: Replace Or Retrieve]]
* TODO A Conflict-Free Replicated Json Datatype
[[cite:kleppmann-2016-confl-free][Kleppmann & Beresford 2016: Conflict Free Replicated]]
* TODO Wasserstein Auto-Encoders                       
[[cite:tolstikhin-2017-wasser-auto-encod][Tolstikhin et al. 2017: Wasserstein Auto Encoders]]
* TODO Wider and Deeper, Cheaper and Faster: Tensorized Lstms 
[[cite:he-2017-wider-deeper][He et al. 2017: Wider Deeper Cheaper Faster]]
* TODO Neural Speed Reading Via Skim-Rnn               
[[cite:seo-2017-neural-speed][Seo et al. 2017: Neural Speed Reading]]
* TODO Robust Speech Recognition Using Generative Adversarial Networks 
[[cite:sriram-2017-robus-speec][Sriram et al. 2017: Robust Speech Recognition]]
* TODO A Survey on Dialogue Systems: Recent Advances and New Frontiers 
[[cite:chen-2017-survey-dialog-system][Chen et al. 2017: Survey Dialogue Systems]]
* TODO Visually-Aware Fashion Recommendation and Design 
[[cite:kang-2017-visual-aware][Kang et al. 2017: Visually Aware Fashion]]
* TODO Can Maxout Units Downsize Restoration Networks? 
[[cite:choi-2017-can-maxout][Choi & Kim 2017: Can Maxout Units]]
* TODO Food Recommender Systems: Important Contributions, Challenges and Future Re 
[[cite:trattner-2017-food-recom-system][Trattner & Elsweiler 2017: Food Recommender Systems]]
* TODO On the State of the Art of Evaluation in Neural Language Models 
[[cite:melis-2017-state-art][Melis et al. 2017: State Art Evaluation]]
* DONE Continuous Integration: The Silver Bullet?      
  CLOSED: [2017-11-13 Mon 21:59]
[[cite:rahman-2017-contin-integ][Rahman et al. 2017: Continuous Integration]]
* TODO Squeezenet: Alexnet-Level Accuracy With 50x Fewer Parameters and $<$0.5MB M 
[[cite:iandola-2016-squeez][Iandola et al. 2016: Squeezenet]]
* TODO End-To-End Learning for Music Audio Tagging At Scale 
[[cite:pons-2017-end-to][Pons et al. 2017: End To End]]
* TODO An Iterative School Decomposition Algorithm for Solving the Multi-School Bu 
[[cite:wang-2017-iterat-school][Wang et al. 2017: Iterative School Decomposition]]
* TODO Augmenting End-To-End Dialog Systems With Commonsense Knowledge 
[[cite:young-2017-augmen-end][Young et al. 2017: Augmenting End To]]
* TODO Squeeze-Segnet: A New Fast Deep Convolutional Neural Network for Semantic S 
[[cite:nanfack-2017-squeez-segnet][Nanfack et al. 2017: Squeeze Segnet]]
* TODO Sling: A Framework for Frame Semantic Parsing   
[[cite:ringgaard-2017-sling][Ringgaard et al. 2017: Sling]]
* TODO Generative Adversarial Networks: An Overview    
[[cite:creswell-2017-gener-adver-networ][Creswell et al. 2017: Generative Adversarial Networks]]
* TODO Data Augmentation Generative Adversarial Networks 
[[cite:antoniou-2017-data-augmen][Antoniou et al. 2017: Data Augmentation Generative]]
* TODO Advances in Variational Inference               
[[cite:zhang-2017-advan-variat-infer][Zhang et al. 2017: Advances Variational Inference]]
* TODO Fixing Weight Decay Regularization in Adam      
[[cite:loshchilov-2017-fixin-weigh][Loshchilov & Hutter 2017: Fixing Weight Decay]]
* TODO A New Method for Performance Analysis in Nonlinear Dimensionality Reduction 
[[cite:liang-2017-new-method][Liang et al. 2017: New Method Performance]]
* TODO Giraffe: Using Deep Reinforcement Learning To Play Chess 
[[cite:lai-2015-giraf][Lai 2015: Giraffe]]
* TODO Efficient Optimization for Rank-Based Loss Functions 
[[cite:mohapatra-2016-effic-optim][Mohapatra et al. 2016: Efficient Optimization Rank]]
* TODO Application of Natural Language Processing To Determine User Satisfaction i 
 [[cite:kowalski-2017-applic-natur][Kowalski et al. 2017: Application Natural Language]]
** DONE Read Out of the Tar Pit
   CLOSED: [2018-01-04 Thu 19:34]
   :LOGBOOK:
   CLOCK: [2018-01-04 Thu 17:42]--[2018-01-04 Thu 18:52] =>  1:10
   :END:
 https://github.com/papers-we-love/papers-we-love/blob/master/design/out-of-the-tar-pit.pdf
    Captured 2017-11-25 22:57
* TODO Shift: A Zero Flop, Zero Parameter Alternative To Spatial Convolutions 
[[cite:wu-2017-shift][Wu et al. 2017: Shift]]
* TODO Light-Head R-Cnn: In Defense of Two-Stage Object Detector 
[[cite:li-2017-light-head][Li et al. 2017: Light Head R Cnn]]
* TODO Non-Local Neural Networks                       
[[cite:wang-2017-non-local][Wang et al. 2017: Non Local Neural Networks]]
* TODO Rubystar: A Non-Task-Oriented Mixture Model Dialog System 
[[cite:liu-2017-rubys][Liu et al. 2017: Rubystar]]
* TODO Actor-Critic Sequence Training for Image Captioning 
[[cite:zhang-2017-actor-critic][Zhang et al. 2017: Actor Critic Sequence]]
* TODO Backprop As Functor: A Compositional Perspective on Supervised Learning 
[[cite:fong-2017-backp-as-funct][Fong et al. 2017: Backprop As Functor]]
* TODO Inferring Users' Preferences Through Leveraging Their Social Relationships 
[[cite:deng-2017-infer-users][Deng et al. 2017: Inferring Users Preferences]]
* TODO Building Machines That Learn and Think for Themselves: Commentary on Lake E 
[[cite:botvinick-2017-build-machin][Botvinick et al. 2017: Building Machines That]]
* TODO Stargan: Unified Generative Adversarial Networks for Multi-Domain Image-To- 
[[cite:choi-2017-starg][Choi et al. 2017: Stargan]]
* TODO Agile Software Engineering and Systems Engineering At Ska Scale 
[[cite:santander-vela-2017-agile-softw][Santander-Vela 2017: Agile Software Engineering]]
* TODO Fast Top-K Area Topics Extraction With Knowledge Base
[[cite:zhang-2017-fast-top][Zhang et al. 2017: Fast Top K]]
* TODO On the Effects of Batch and Weight Normalization in Generative Adversarial 
[[cite:xiang-2017-effec-batch][Xiang & Li 2017: Effects Batch Weight]]
* TODO Learning By Asking Questions                    
[[cite:misra-2017-learn-by][Misra et al. 2017: Learning By Asking Questions]]
* TODO The Mind As a Computational System              
[[cite:adami-2017-mind-as][Adami 2017: Mind As Computational System]]
* DONE Will Humans Even Write Code in 2040 and What Would That Mean for Extreme He 
  CLOSED: [2018-01-03 Wed 13:45]
  :LOGBOOK:
  CLOCK: [2018-01-03 Wed 12:59]--[2018-01-03 Wed 13:45] =>  0:46
  :END:
[[cite:billings-2017-will-human][Billings et al. 2017: Will Humans Even]]
* TODO Explainable Ai: Beware of Inmates Running the Asylum Or: How I Learnt To St 
[[cite:miller-2017-explain-ai][Miller et al. 2017: Explainable Ai]]
* TODO Take It in Your Stride: Do We Need Striding in Cnns? 
[[cite:kong-2017-take-it][Kong & Lucey 2017: Take It Your Stride]]
* TODO Imagenet Training in Minutes                    
[[cite:you-2017-imagen-train-minut][You et al. 2017: Imagenet Training Minutes]]
* TODO Shufflenet: An Extremely Efficient Convolutional Neural Network for Mobile 
[[cite:zhang-2017-shuff][Zhang et al. 2017: Shufflenet]]
* TODO A Deep Network Model for Paraphrase Detection in Short Text Messages 
[[cite:agarwal-2017-deep-networ][Agarwal et al. 2017: Deep Network Model]]
* TODO Mastering Chess and Shogi By Self-Play With a General Reinforcement Learnin 
[[cite:silver-2017-master-chess][Silver et al. 2017: Mastering Chess Shogi]]
* TODO Sgan: An Alternative Training of Generative Adversarial Networks 
[[cite:chavdarova-2017-sgan][Chavdarova & Fleuret 2017: Sgan]]
* TODO Automan: a Simple, Python-Based, Automation Framework for Numerical Computi 
[[cite:ramachandran-2017-autom][Ramachandran 2017: Automan]]
* TODO Hierarchical Bloom Filter Trees for Approximate Matching 
[[cite:lillis-2017-hierar-bloom][Lillis et al. 2017: Hierarchical Bloom Filter]]
* TODO Hash Embeddings for Efficient Word Representations 
[[cite:svenstrup-2017-hash-embed][Svenstrup et al. 2017: Hash Embeddings Efficient]]
* TODO Rasa: Open Source Language Understanding and Dialogue Management 
[[cite:bocklisch-2017-rasa][Bocklisch et al. 2017: Rasa]]
* TODO Fourteen Years of Software Engineering At Eth Zurich 
[[cite:meyer-2017-fourt-years][Meyer 2017: Fourteen Years Software]]
* TODO Leveraging Long and Short-Term Information in Content-Aware Movie Recommend
[[Cite:zhao-2017-lever-long]]
* TODO Document Spanners for Extracting Incomplete Information: Expressiveness and
[[cite:maturana-2017-docum-spann]]
* DONE Safe Mutations for Deep and Recurrent Neural Networks Through Output Gradie
  CLOSED: [2018-01-01 Mon 18:38]
[[Cite:lehman-2017-safe-mutat]]
* TODO Improved Distributed Algorithms for Exact Shortest Paths
[[cite:ghaffari-2017-improv-distr]]
* TODO Learning To Learn By Gradient Descent By Gradient Descent
[[cite:andrychowicz-2016-learn-to]]
* DONE Cnn Is All You Need                            
  CLOSED: [2018-01-02 Tue 20:30]
  :LOGBOOK:
  CLOCK: [2018-01-02 Tue 20:21]--[2018-01-02 Tue 20:30] =>  0:09
  :END:
[[Cite:chen-2017-cnn-is]]
* TODO On the Challenges of Detecting Rude Conversational Behaviour
[[Cite:grewal-2017-chall-detec]]
* TODO Robust Loss Functions Under Label Noise for Deep Neural Networks
[[Cite:ghosh-2017-robus-loss]]
* TODO One-Shot and Few-Shot Learning of Word Embeddings
  [[Cite:lampinen-2017-one-shot]]
* TODO Beyond Word Embeddings: Learning Entity and Concept Representations From La
  [[cite:shalaby-2018-beyon-word-embed]]
* TODO Theory of Deep Learning Iii: Explaining the Non-Overfitting Puzzle
  [[cite:poggio-2017-theor-deep]]
* TODO What Do We Need To Build Explainable Ai Systems for the Medical Domain?
[[cite:holzinger-2017-what-do]]
* TODO Vizdoom: Drqn With Prioritized Experience Replay, Double-Q Learning, & Snap
[[cite:schulze-2018-vizdoom]]
* TODO Deepj: Style-Specific Music Generation
[[cite:mao-2018-deepj]]
* DONE Deep Learning: A Critical Appraisal
  CLOSED: [2018-01-07 Sun 19:38]
  :LOGBOOK:
  CLOCK: [2018-01-07 Sun 19:38]--[2018-01-07 Sun 20:38] =>  1:00
  :END:
[[cite:marcus-2018-deep-learn]]
* DONE Recent Advances in Recurrent Neural Networks
  CLOSED: [2018-01-09 Tue 20:03]
  :LOGBOOK:
  CLOCK: [2018-01-09 Tue 19:41]--[2018-01-09 Tue 20:03] =>  0:22
  :END:
[[cite:salehinejad-2017-recen-advan][Salehinejad et al. 2017: Recent Advances Recurrent]]
* TODO A Survey of Model Compression and Acceleration for Deep Neural Networks
[[cite:cheng-2017-survey-model][Cheng et al. 2017: Survey Model Compression]]
* TODO Artificial Intelligence and Statistics
[[cite:yu-2017-artif-intel-statis][Yu & Kumbier 2017: Artificial Intelligence Statistics]]
* TODO Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey
[[cite:akhtar-2018-threat-adver][Akhtar & Mian 2018: Threat Adversarial Attacks]]
* TODO A Study of the Classification of Low-Dimensional Data With Supervised Manif
[[cite:vural-2015-study-class][Vural & Guillemot 2015: Study Classification Low]]
* TODO Shielding Google's Language Toxicity Model Against Adversarial Attacks
[[cite:rodriguez-2018-shiel-googl][Rodriguez & Rojas-Galeano 2018: Shielding Googles Language]]
* TODO Towards Understanding and Answering Multi-Sentence Recommendation Questions
[[cite:contractor-2018-towar-under][Contractor et al. 2018: Towards Understanding Answering]]
* DONE Learning Feature Representations for Keyphrase Extraction
  CLOSED: [2018-01-10 Wed 14:01]
  :LOGBOOK:
  CLOCK: [2018-01-10 Wed 13:44]--[2018-01-10 Wed 14:01] =>  0:17
  :END:
[[cite:florescu-2018-learn-featur][Florescu & Jin 2018: Learning Feature Representations]]
* TODO A Practical Tutorial on Autoencoders for Nonlinear Feature Fusion: Taxonomy
[[cite:charte-2018-pract-tutor][Charte et al. 2018: Practical Tutorial Autoencoders]]
* TODO Clustering of Data With Missing Entries
[[cite:poddar-2018-clust-data][Poddar & Jacob 2018: Clustering Data With]]
* TODO How To Beat Science and Influence People: Policy Makers and Propaganda in E
[[cite:weatherall-2018-how-to][Weatherall et al. 2018: How To Beat]]
* TODO Meltdown
[[cite:lipp-2018-meltd][Lipp et al. 2018: Meltdown]]
* TODO Spectre Attacks: Exploiting Speculative Execution
[[cite:kocher-2018-spect-attac][Kocher et al. 2018: Spectre Attacks]]
* DONE Building Machines That Learn and Think Like People
  CLOSED: [2018-01-08 Mon 23:38]
  :LOGBOOK:
  CLOCK: [2018-01-08 Mon 21:40]--[2018-01-08 Mon 23:38] =>  1:58
  CLOCK: [2018-01-08 Mon 20:04]--[2018-01-08 Mon 20:37] =>  0:33
  :END:
[[cite:lake-2016-build-machin][Lake et al. 2016: Building Machines That]]
* TODO Parameter-Free Online Learning Via Model Selection
[[cite:foster-2017-param-free][Foster et al. 2017: Parameter Free Online]]
* TODO Imagination-Augmented Agents for Deep Reinforcement Learning
[[cite:weber-2017-imagin-augmen][Weber et al. 2017: Imagination Augmented Agents]]
* TODO Scaling Memory-Augmented Neural Networks With Sparse Reads and Writes
[[cite:rae-2016-scalin-memor][Rae et al. 2016: Scaling Memory Augmented]]
* TODO Has the Online Discussion Been Manipulated? Quantifying Online Discussion A
[[cite:elyashar-2017-has-onlin][Elyashar et al. 2017: Has Online Discussion]]
* TODO The Case for Learned Index Structures
[[cite:kraska-2017-case-learn][Kraska et al. 2017: Case Learned Index Structures]]
* TODO Deeptriage: Exploring the Effectiveness of Deep Learning for Bug Triaging
[[cite:mani-2018-deept][Mani et al. 2018: Deeptriage]]
* TODO Unsupervised Low-Dimensional Vector Representations for Words, Phrases and
[[cite:smalheiser-2018-unsup-low]]
* TODO Less Is More: Culling the Training Set To Improve Robustness of Deep Neural
[[cite:liu-2018-less-is-more]]
* TODO NixOS: A purely functional Linux distribution
[[cite:dolstra2008nixos]]
* TODO Deepseek: Content Based Image Search \& Retrieval
[[cite:piplani-2018-deeps]]
* DONE Cyclical Learning Rates for Training Neural Networks
  CLOSED: [2018-01-13 Sat 10:51]
  :LOGBOOK:
  CLOCK: [2018-01-13 Sat 10:43]--[2018-01-13 Sat 10:51] =>  0:08
  :END:
[[cite:smith-2015-cyclic-learn]]
* TODO Don't Decay the Learning Rate, Increase the Batch Size
[[cite:smith-2017-dont-decay]]
* TODO No More Pesky Learning Rates
* DONE Efficient Estimation of Word Representations in Vector Space
  CLOSED: [2018-01-13 Sat 14:56]
  :LOGBOOK:
  CLOCK: [2018-01-13 Sat 14:50]--[2018-01-13 Sat 14:56] =>  0:06
  :END:
[[cite:mikolov-2013-effic-estim]]
* DONE GloVe: Global Vectors for Word Representation
  CLOSED: [2018-01-13 Sat 15:46]
  :LOGBOOK:
  CLOCK: [2018-01-13 Sat 15:23]--[2018-01-13 Sat 15:46] =>  0:23
  :END:
[[cite:pennington2014glove]]
* TODO A Survey of Neural Network Techniques for Feature Extraction From Text
[[cite:john-2017-survey-neural]]
* DONE High-risk learning: acquiring new word vectors from tiny data
  CLOSED: [2018-01-13 Sat 16:12]
  :LOGBOOK:
  CLOCK: [2018-01-13 Sat 16:04]--[2018-01-13 Sat 16:12] =>  0:08
  :END:
[[cite:herbelot-baroni2017]]
* CANCELLED A Mathematical Theory of Deep Convolutional Neural Networks for Feature Ext
  CLOSED: [2018-01-13 Sat 17:08]
  :LOGBOOK:
  - State "CANCELLED"  from "DONE"       [2018-01-13 Sat 17:08] \\
    too difficult, don't know continuous frame theory
  CLOCK: [2018-01-13 Sat 16:23]--[2018-01-13 Sat 17:08] =>  0:45
  :END:
[[cite:wiatowski-2015-mathem-theor]]
* TODO Conversational Ai: The Science Behind the Alexa Prize
[[cite:ram-2018-conver-ai]]
* TODO Distributed Deep Reinforcement Learning: Learn How To Play Atari Games in 2
[[cite:adamski-2018-distr-deep]]
* TODO Deeptraffic: Driving Fast Through Dense Traffic With Deep Reinforcement Lea
[[cite:fridman-2018-deept]]
