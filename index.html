<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html>

<head>
<title>index</title>
</head>

<body>

<!-- This document was automatically generated with bibtex2html 1.98
     (see http://www.lri.fr/~filliatr/bibtex2html/),
     with the following command:
     bibtex2html index.bib  -->


<table>

<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ramachandran17_swish">1</a>]
</td>
<td class="bibtexitem">
Prajit Ramachandran, Barret Zoph, and Quoc&nbsp;V. Le.
 Swish: a self-gated activation function.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#ramachandran17_swish">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.05941">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.05941v1">http</a>&nbsp;]
<blockquote><font size="-1">
The choice of activation functions in deep networks
                  has a significant effect on the training dynamics
                  and task performance. Currently, the most successful
                  and widely-used activation function is the Rectified
                  Linear Unit (ReLU). Although various alternatives to
                  ReLU have been proposed, none have managed to
                  replace it due to inconsistent gains. In this work,
                  we propose a new activation function, named Swish,
                  which is simply <em>f</em>(<em>x</em>) = <em>x</em> &#183;<em>sigmoid</em>(<em>x</em>).
                  Our experiments show that Swish tends to work better
                  than ReLU on deeper models across a number of
                  challenging datasets. For example, simply replacing
                  ReLUs with Swish units improves top-1 classification
                  accuracy on ImageNet by 0.9 % for Mobile NASNet-A
                  and 0.6 % for Inception-ResNet-v2. The simplicity
                  of Swish and its similarity to ReLU make it easy for
                  practitioners to replace ReLUs with Swish units in
                  any neural network.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="liu17_relat_pins_at_pinter">2</a>]
</td>
<td class="bibtexitem">
David&nbsp;C. Liu, Stephanie Rogers, Raymond Shiau, Dmitry Kislyuk, Kevin&nbsp;C. Ma,
  Zhigang Zhong, Jenny Liu, and Yushi Jing.
 Related pins at pinterest: The evolution of a real-world recommender
  system.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#liu17_relat_pins_at_pinter">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1702.07969">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1702.07969v1">http</a>&nbsp;]
<blockquote><font size="-1">
Related Pins is the Web-scale recommender system
                  that powers over 40 % of user engagement on
                  Pinterest. This paper is a longitudinal study of
                  three years of its development, exploring the
                  evolution of the system and its components from
                  prototypes to present state. Each component was
                  originally built with many constraints on
                  engineering effort and computational resources, so
                  we prioritized the simplest and highest-leverage
                  solutions. We show how organic growth led to a
                  complex system and how we managed this complexity.
                  Many challenges arose while building this system,
                  such as avoiding feedback loops, evaluating
                  performance, activating content, and eliminating
                  legacy heuristics. Finally, we offer suggestions for
                  tackling these challenges when engineering Web-scale
                  recommender systems.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="henderson17_effic_natur_languag_respon_sugges_smart_reply">3</a>]
</td>
<td class="bibtexitem">
Matthew Henderson, Rami Al-Rfou, Brian Strope, Yun-hsuan Sung, Laszlo Lukacs,
  Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil.
 Efficient natural language response suggestion for smart reply.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#henderson17_effic_natur_languag_respon_sugges_smart_reply">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1705.00652">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1705.00652v1">http</a>&nbsp;]
<blockquote><font size="-1">
This paper presents a computationally efficient
                  machine-learned method for natural language response
                  suggestion. Feed-forward neural networks using
                  n-gram embedding features encode messages into
                  vectors which are optimized to give message-response
                  pairs a high dot-product value. An optimized search
                  finds response suggestions. The method is evaluated
                  in a large-scale commercial e-mail application,
                  Inbox by Gmail. Compared to a sequence-to-sequence
                  approach, the new system achieves the same quality
                  at a small fraction of the computational
                  requirements and latency.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mahendran14_under_deep_image_repres_by_inver_them">4</a>]
</td>
<td class="bibtexitem">
Aravindh Mahendran and Andrea Vedaldi.
 Understanding deep image representations by inverting them.
 <em>CoRR</em>, 2014.
[&nbsp;<a href="index_bib.html#mahendran14_under_deep_image_repres_by_inver_them">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1412.0035">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1412.0035v1">http</a>&nbsp;]
<blockquote><font size="-1">
Image representations, from SIFT and Bag of Visual
                  Words to Convolutional Neural Networks (CNNs), are a
                  crucial component of almost any image understanding
                  system. Nevertheless, our understanding of them
                  remains limited. In this paper we conduct a direct
                  analysis of the visual information contained in
                  representations by asking the following question:
                  given an encoding of an image, to which extent is it
                  possible to reconstruct the image itself? To answer
                  this question we contribute a general framework to
                  invert representations. We show that this method can
                  invert representations such as HOG and SIFT more
                  accurately than recent alternatives while being
                  applicable to CNNs too. We then use this technique
                  to study the inverse of recent state-of-the-art CNN
                  image representations for the first time. Among our
                  findings, we show that several layers in CNNs retain
                  photographically accurate information about the
                  image, with different degrees of geometric and
                  photometric invariance.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="gatys15_textur_synth_using_convol_neural_networ">5</a>]
</td>
<td class="bibtexitem">
Leon&nbsp;A. Gatys, Alexander&nbsp;S. Ecker, and Matthias Bethge.
 Texture synthesis using convolutional neural networks.
 <em>CoRR</em>, 2015.
[&nbsp;<a href="index_bib.html#gatys15_textur_synth_using_convol_neural_networ">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1505.07376">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1505.07376v3">http</a>&nbsp;]
<blockquote><font size="-1">
Here we introduce a new model of natural textures
                  based on the feature spaces of convolutional neural
                  networks optimised for object recognition. Samples
                  from the model are of high perceptual quality
                  demonstrating the generative power of neural
                  networks trained in a purely discriminative fashion.
                  Within the model, textures are represented by the
                  correlations between feature maps in several layers
                  of the network. We show that across layers the
                  texture representations increasingly capture the
                  statistical properties of natural images while
                  making object information more and more explicit.
                  The model provides a new tool to generate stimuli
                  for neuroscience and might offer insights into the
                  deep representations learned by convolutional neural
                  networks.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="christiano-2017-deep-reinf">6</a>]
</td>
<td class="bibtexitem">
Paul Christiano, Jan Leike, Tom&nbsp;B. Brown, Miljan Martic, Shane Legg, and Dario
  Amodei.
 Deep reinforcement learning from human preferences.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#christiano-2017-deep-reinf">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.03741">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.03741v3">http</a>&nbsp;]
<blockquote><font size="-1">
For sophisticated reinforcement learning (RL)
                  systems to interact usefully with real-world
                  environments, we need to communicate complex goals
                  to these systems. In this work, we explore goals
                  defined in terms of (non-expert) human preferences
                  between pairs of trajectory segments. We show that
                  this approach can effectively solve complex RL tasks
                  without access to the reward function, including
                  Atari games and simulated robot locomotion, while
                  providing feedback on less than one percent of our
                  agent's interactions with the environment. This
                  reduces the cost of human oversight far enough that
                  it can be practically applied to state-of-the-art RL
                  systems. To demonstrate the flexibility of our
                  approach, we show that we can successfully train
                  complex novel behaviors with about an hour of human
                  time. These behaviors and environments are
                  considerably more complex than any that have been
                  previously learned from human feedback.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="smith-2017-under-gener">7</a>]
</td>
<td class="bibtexitem">
Samuel&nbsp;L. Smith and Quoc&nbsp;V. Le.
 Understanding generalization and stochastic gradient descent.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#smith-2017-under-gener">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.06451">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.06451v1">http</a>&nbsp;]
<blockquote><font size="-1">
This paper tackles two related questions at the
                  heart of machine learning; how can we predict if a
                  minimum will generalize to the test set, and why
                  does stochastic gradient descent find minima that
                  generalize well? Our work is inspired by Zhang et
                  al. (2017), who showed deep networks can easily
                  memorize randomly labeled training data, despite
                  generalizing well when shown real labels of the same
                  inputs. We show here that the same phenomenon occurs
                  in small linear models. These observations are
                  explained by evaluating the Bayesian evidence in
                  favor of each model, which penalizes sharp minima.
                  Next, we explore the "generalization gap" between
                  small and large batch training, identifying an
                  optimum batch size which maximizes the test set
                  accuracy. Noise in the gradient updates is
                  beneficial, driving the dynamics towards robust
                  minima for which the evidence is large. Interpreting
                  stochastic gradient descent as a stochastic
                  differential equation, we predict the optimum batch
                  size is proportional to both the learning rate and
                  the size of the training set, and verify these
                  predictions empirically.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="alsallakh-2017-do-convol">8</a>]
</td>
<td class="bibtexitem">
Bilal Alsallakh, Amin Jourabloo, Mao Ye, Xiaoming Liu, and Liu Ren.
 Do convolutional neural networks learn class hierarchy?
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#alsallakh-2017-do-convol">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.06501">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.06501v1">http</a>&nbsp;]
<blockquote><font size="-1">
Convolutional Neural Networks (CNNs) currently
                  achieve state-of-the-art accuracy in image
                  classification. With a growing number of classes,
                  the accuracy usually drops as the possibilities of
                  confusion increase. Interestingly, the class
                  confusion patterns follow a hierarchical structure
                  over the classes. We present visual-analytics
                  methods to reveal and analyze this hierarchy of
                  similar classes in relation with CNN-internal data.
                  We found that this hierarchy not only dictates the
                  confusion patterns between the classes, it
                  furthermore dictates the learning behavior of CNNs.
                  In particular, the early layers in these networks
                  develop feature detectors that can separate
                  high-level groups of classes quite well, even after
                  a few training epochs. In contrast, the latter
                  layers require substantially more epochs to develop
                  specialized feature detectors that can separate
                  individual classes. We demonstrate how these
                  insights are key to significant improvement in
                  accuracy by designing hierarchy-aware CNNs that
                  accelerate model convergence and alleviate
                  overfitting. We further demonstrate how our methods
                  help in identifying various quality issues in the
                  training data.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="holden-2017-phasefunctionnn">9</a>]
</td>
<td class="bibtexitem">
Last accessed Fri Oct 27 09:30:15 2017.
[&nbsp;<a href="index_bib.html#holden-2017-phasefunctionnn">bib</a>&nbsp;| 
<a href="http://theorangeduck.com/media/uploads/other_stuff/phasefunction.pdf">.pdf</a>&nbsp;]

</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu-2016-googl-neural">10</a>]
</td>
<td class="bibtexitem">
Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc&nbsp;V. Le, Mohammad Norouzi, Wolfgang
  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner,
  Apurva Shah, Melvin Johnson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws,
  Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian,
  Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick,
  Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean.
 Google's neural machine translation system: Bridging the gap between
  human and machine translation.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#wu-2016-googl-neural">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1609.08144">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1609.08144v2">http</a>&nbsp;]
<blockquote><font size="-1">
Neural Machine Translation (NMT) is an end-to-end
                  learning approach for automated translation, with
                  the potential to overcome many of the weaknesses of
                  conventional phrase-based translation systems.
                  Unfortunately, NMT systems are known to be
                  computationally expensive both in training and in
                  translation inference. Also, most NMT systems have
                  difficulty with rare words. These issues have
                  hindered NMT's use in practical deployments and
                  services, where both accuracy and speed are
                  essential. In this work, we present GNMT, Google's
                  Neural Machine Translation system, which attempts to
                  address many of these issues. Our model consists of
                  a deep LSTM network with 8 encoder and 8 decoder
                  layers using attention and residual connections. To
                  improve parallelism and therefore decrease training
                  time, our attention mechanism connects the bottom
                  layer of the decoder to the top layer of the
                  encoder. To accelerate the final translation speed,
                  we employ low-precision arithmetic during inference
                  computations. To improve handling of rare words, we
                  divide words into a limited set of common sub-word
                  units ("wordpieces") for both input and output. This
                  method provides a good balance between the
                  flexibility of "character"-delimited models and the
                  efficiency of "word"-delimited models, naturally
                  handles translation of rare words, and ultimately
                  improves the overall accuracy of the system. Our
                  beam search technique employs a length-normalization
                  procedure and uses a coverage penalty, which
                  encourages generation of an output sentence that is
                  most likely to cover all the words in the source
                  sentence. On the WMT'14 English-to-French and
                  English-to-German benchmarks, GNMT achieves
                  competitive results to state-of-the-art. Using a
                  human side-by-side evaluation on a set of isolated
                  simple sentences, it reduces translation errors by
                  an average of 60 % compared to Google's
                  phrase-based production system.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="moosavi-dezfooli-2016-univer-adver-pertur">11</a>]
</td>
<td class="bibtexitem">
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
  Frossard.
 Universal adversarial perturbations.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#moosavi-dezfooli-2016-univer-adver-pertur">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1610.08401">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1610.08401v3">http</a>&nbsp;]
<blockquote><font size="-1">
Given a state-of-the-art deep neural network
                  classifier, we show the existence of a universal
                  (image-agnostic) and very small perturbation vector
                  that causes natural images to be misclassified with
                  high probability. We propose a systematic algorithm
                  for computing universal perturbations, and show that
                  state-of-the-art deep neural networks are highly
                  vulnerable to such perturbations, albeit being
                  quasi-imperceptible to the human eye. We further
                  empirically analyze these universal perturbations
                  and show, in particular, that they generalize very
                  well across neural networks. The surprising
                  existence of universal perturbations reveals
                  important geometric correlations among the
                  high-dimensional decision boundary of classifiers.
                  It further outlines potential security breaches with
                  the existence of single directions in the input
                  space that adversaries can possibly exploit to break
                  a classifier on most natural images.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kingma-2013-auto-encod">12</a>]
</td>
<td class="bibtexitem">
Diederik&nbsp;P Kingma and Max Welling.
 Auto-encoding variational bayes.
 <em>CoRR</em>, 2013.
[&nbsp;<a href="index_bib.html#kingma-2013-auto-encod">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6114">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1312.6114v10">http</a>&nbsp;]
<blockquote><font size="-1">
How can we perform efficient inference and learning
                  in directed probabilistic models, in the presence of
                  continuous latent variables with intractable
                  posterior distributions, and large datasets? We
                  introduce a stochastic variational inference and
                  learning algorithm that scales to large datasets
                  and, under some mild differentiability conditions,
                  even works in the intractable case. Our
                  contributions is two-fold. First, we show that a
                  reparameterization of the variational lower bound
                  yields a lower bound estimator that can be
                  straightforwardly optimized using standard
                  stochastic gradient methods. Second, we show that
                  for i.i.d. datasets with continuous latent variables
                  per datapoint, posterior inference can be made
                  especially efficient by fitting an approximate
                  inference model (also called a recognition model) to
                  the intractable posterior using the proposed lower
                  bound estimator. Theoretical advantages are
                  reflected in experimental results.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="vaswani-2017-atten-is">13</a>]
</td>
<td class="bibtexitem">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
  Aidan&nbsp;N. Gomez, Lukasz Kaiser, and Illia Polosukhin.
 Attention is all you need.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#vaswani-2017-atten-is">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.03762">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.03762v4">http</a>&nbsp;]
<blockquote><font size="-1">
The dominant sequence transduction models are based
                  on complex recurrent or convolutional neural
                  networks in an encoder-decoder configuration. The
                  best performing models also connect the encoder and
                  decoder through an attention mechanism. We propose a
                  new simple network architecture, the Transformer,
                  based solely on attention mechanisms, dispensing
                  with recurrence and convolutions entirely.
                  Experiments on two machine translation tasks show
                  these models to be superior in quality while being
                  more parallelizable and requiring significantly less
                  time to train. Our model achieves 28.4 BLEU on the
                  WMT 2014 English-to-German translation task,
                  improving over the existing best results, including
                  ensembles by over 2 BLEU. On the WMT 2014
                  English-to-French translation task, our model
                  establishes a new single-model state-of-the-art BLEU
                  score of 41.0 after training for 3.5 days on eight
                  GPUs, a small fraction of the training costs of the
                  best models from the literature. We show that the
                  Transformer generalizes well to other tasks by
                  applying it successfully to English constituency
                  parsing both with large and limited training data.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kaiser-2017-one-model">14</a>]
</td>
<td class="bibtexitem">
Lukasz Kaiser, Aidan&nbsp;N. Gomez, Noam Shazeer, Ashish Vaswani, Niki Parmar, Llion
  Jones, and Jakob Uszkoreit.
 One model to learn them all.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#kaiser-2017-one-model">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.05137">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.05137v1">http</a>&nbsp;]
<blockquote><font size="-1">
Deep learning yields great results across many
                  fields, from speech recognition, image
                  classification, to translation. But for each
                  problem, getting a deep model to work well involves
                  research into the architecture and a long period of
                  tuning. We present a single model that yields good
                  results on a number of problems spanning multiple
                  domains. In particular, this single model is trained
                  concurrently on ImageNet, multiple translation
                  tasks, image captioning (COCO dataset), a speech
                  recognition corpus, and an English parsing task. Our
                  model architecture incorporates building blocks from
                  multiple domains. It contains convolutional layers,
                  an attention mechanism, and sparsely-gated layers.
                  Each of these computational blocks is crucial for a
                  subset of the tasks we train on. Interestingly, even
                  if a block is not crucial for a task, we observe
                  that adding it never hurts performance and in most
                  cases improves it on all tasks. We also show that
                  tasks with less data benefit largely from joint
                  training with other tasks, while performance on
                  large tasks degrades only slightly if at all.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu-2017-scalab-trust">15</a>]
</td>
<td class="bibtexitem">
Yuhuai Wu, Elman Mansimov, Shun Liao, Roger Grosse, and Jimmy Ba.
 Scalable trust-region method for deep reinforcement learning using
  kronecker-factored approximation.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#wu-2017-scalab-trust">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.05144">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.05144v2">http</a>&nbsp;]
<blockquote><font size="-1">
In this work, we propose to apply trust region
                  optimization to deep reinforcement learning using a
                  recently proposed Kronecker-factored approximation
                  to the curvature. We extend the framework of natural
                  policy gradient and propose to optimize both the
                  actor and the critic using Kronecker-factored
                  approximate curvature (K-FAC) with trust region;
                  hence we call our method Actor Critic using
                  Kronecker-Factored Trust Region (ACKTR). To the best
                  of our knowledge, this is the first scalable trust
                  region natural gradient method for actor-critic
                  methods. It is also a method that learns non-trivial
                  tasks in continuous control as well as discrete
                  control policies directly from raw pixel inputs. We
                  tested our approach across discrete domains in Atari
                  games as well as continuous domains in the MuJoCo
                  environment. With the proposed methods, we are able
                  to achieve higher rewards and a 2- to 3-fold
                  improvement in sample efficiency on average,
                  compared to previous state-of-the-art on-policy
                  actor-critic methods. Code is available at
                  https://github.com/openai/baselines
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wen-2016-networ-based">16</a>]
</td>
<td class="bibtexitem">
Tsung-Hsien Wen, David Vandyke, Nikola Mrksic, Milica Gasic, Lina&nbsp;M.
  Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young.
 A network-based end-to-end trainable task-oriented dialogue system.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#wen-2016-networ-based">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1604.04562">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1604.04562v3">http</a>&nbsp;]
<blockquote><font size="-1">
Teaching machines to accomplish tasks by conversing
                  naturally with humans is challenging. Currently,
                  developing task-oriented dialogue systems requires
                  creating multiple components and typically this
                  involves either a large amount of handcrafting, or
                  acquiring costly labelled datasets to solve a
                  statistical learning problem for each component. In
                  this work we introduce a neural network-based
                  text-in, text-out end-to-end trainable goal-oriented
                  dialogue system along with a new way of collecting
                  dialogue data based on a novel pipe-lined
                  Wizard-of-Oz framework. This approach allows us to
                  develop dialogue systems easily and without making
                  too many assumptions about the task at hand. The
                  results show that the model can converse with human
                  subjects naturally whilst helping them to accomplish
                  tasks in a restaurant search domain.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="singh-2017-replac-or">17</a>]
</td>
<td class="bibtexitem">
Vikash Singh.
 Replace or retrieve keywords in documents at scale.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#singh-2017-replac-or">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00046">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00046v2">http</a>&nbsp;]
<blockquote><font size="-1">
In this paper we introduce, the FlashText algorithm
                  for replacing keywords or finding keywords in a
                  given text. FlashText can search or replace keywords
                  in one pass over a document. The time complexity of
                  this algorithm is not dependent on the number of
                  terms being searched or replaced. For a document of
                  size N (characters) and a dictionary of M keywords,
                  the time complexity will be O(N). This algorithm is
                  much faster than Regex, because regex time
                  complexity is O(MxN). It is also different from Aho
                  Corasick Algorithm, as it doesn't match substrings.
                  FlashText is designed to only match complete words
                  (words with boundary characters on both sides). For
                  an input dictionary of Apple, this algorithm won't
                  match it to 'I like Pineapple'. This algorithm is
                  also designed to go for the longest match first. For
                  an input dictionary Machine, Learning, Machine
                  learning on a string 'I like Machine learning', it
                  will only consider the longest match, which is
                  Machine Learning. We have made python implementation
                  of this algorithm available as open-source on
                  GitHub, released under the permissive MIT License.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kleppmann-2016-confl-free">18</a>]
</td>
<td class="bibtexitem">
Martin Kleppmann and Alastair&nbsp;R. Beresford.
 A conflict-free replicated json datatype.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#kleppmann-2016-confl-free">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1608.03960">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1608.03960v3">http</a>&nbsp;]
<blockquote><font size="-1">
Many applications model their data in a
                  general-purpose storage format such as JSON. This
                  data structure is modified by the application as a
                  result of user input. Such modifications are well
                  understood if performed sequentially on a single
                  copy of the data, but if the data is replicated and
                  modified concurrently on multiple devices, it is
                  unclear what the semantics should be. In this paper
                  we present an algorithm and formal semantics for a
                  JSON data structure that automatically resolves
                  concurrent modifications such that no updates are
                  lost, and such that all replicas converge towards
                  the same state (a conflict-free replicated datatype
                  or CRDT). It supports arbitrarily nested list and
                  map types, which can be modified by insertion,
                  deletion and assignment. The algorithm performs all
                  merging client-side and does not depend on ordering
                  guarantees from the network, making it suitable for
                  deployment on mobile devices with poor network
                  connectivity, in peer-to-peer networks, and in
                  messaging systems with end-to-end encryption.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lample-2017-unsup-machin">19</a>]
</td>
<td class="bibtexitem">
Guillaume Lample, Ludovic Denoyer, and Marc'Aurelio Ranzato.
 Unsupervised machine translation using monolingual corpora only.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#lample-2017-unsup-machin">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00043">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00043v1">http</a>&nbsp;]
<blockquote><font size="-1">
Machine translation has recently achieved impressive
                  performance thanks to recent advances in deep
                  learning and the availability of large-scale
                  parallel corpora. There have been numerous attempts
                  to extend these successes to low-resource language
                  pairs, yet requiring tens of thousands of parallel
                  sentences. In this work, we take this research
                  direction to the extreme and investigate whether it
                  is possible to learn to translate even without any
                  parallel data. We propose a model that takes
                  sentences from monolingual corpora in two different
                  languages and maps them into the same latent space.
                  By learning to reconstruct in both languages from
                  this shared feature space, the model effectively
                  learns to translate without using any labeled data.
                  We demonstrate our model on two widely used datasets
                  and two language pairs, reporting BLEU scores up to
                  32.8, without using even a single parallel sentence
                  at training time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chaudhuri-2017-fast-precis">20</a>]
</td>
<td class="bibtexitem">
Avik Chaudhuri, Panagiotis Vekris, Sam Goldman, Marshall Roch, and Gabriel
  Levi.
 Fast and precise type checking for javascript.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#chaudhuri-2017-fast-precis">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.08021">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1708.08021v2">http</a>&nbsp;]
<blockquote><font size="-1">
In this paper we present the design and
                  implementation of Flow, a fast and precise type
                  checker for JavaScript that is used by thousands of
                  developers on millions of lines of code at Facebook
                  every day. Flow uses sophisticated type inference to
                  understand common JavaScript idioms precisely. This
                  helps it find non-trivial bugs in code and provide
                  code intelligence to editors without requiring
                  significant rewriting or annotations from the
                  developer. We formalize an important fragment of
                  Flow's analysis and prove its soundness.
                  Furthermore, Flow uses aggressive parallelization
                  and incrementalization to deliver near-instantaneous
                  response times. This helps it avoid introducing any
                  latency in the usual edit-refresh cycle of rapid
                  JavaScript development. We describe the algorithms
                  and systems infrastructure that we built to scale
                  Flow's analysis.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="tolstikhin-2017-wasser-auto-encod">21</a>]
</td>
<td class="bibtexitem">
Ilya Tolstikhin, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf.
 Wasserstein auto-encoders.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#tolstikhin-2017-wasser-auto-encod">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01558v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01558v1">http</a>&nbsp;]
<blockquote><font size="-1">
We propose the Wasserstein Auto-Encoder (WAE)---a
                  new algorithm for building a generative model of the
                  data distribution. WAE minimizes a penalized form of
                  the Wasserstein distance between the model
                  distribution and the target distribution, which
                  leads to a different regularizer than the one used
                  by the Variational Auto-Encoder (VAE). This
                  regularizer encourages the encoded training
                  distribution to match the prior. We compare our
                  algorithm with several other techniques and show
                  that it is a generalization of adversarial
                  auto-encoders (AAE). Our experiments show that WAE
                  shares many of the properties of VAEs (stable
                  training, encoder-decoder architecture, nice latent
                  manifold structure) while generating samples of
                  better quality, as measured by the FID score.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="he-2017-wider-deeper">22</a>]
</td>
<td class="bibtexitem">
Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber.
 Wider and deeper, cheaper and faster: Tensorized lstms for sequence
  learning.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#he-2017-wider-deeper">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01577v2">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01577v2">http</a>&nbsp;]
<blockquote><font size="-1">
Long Short-Term Memory (LSTM) is a popular approach
                  to boosting the ability of Recurrent Neural Networks
                  to store longer term temporal information. The
                  capacity of an LSTM network can be increased by
                  widening and adding layers. However, usually the
                  former introduces additional parameters, while the
                  latter increases the runtime. As an alternative we
                  propose the Tensorized LSTM in which the hidden
                  states are represented by tensors and updated via a
                  cross-layer convolution. By increasing the tensor
                  size, the network can be widened efficiently without
                  additional parameters since the parameters are
                  shared across different locations in the tensor; by
                  delaying the output, the network can be deepened
                  implicitly with little additional runtime since deep
                  computations for each timestep are merged into
                  temporal computations of the sequence. Experiments
                  conducted on five challenging sequence learning
                  tasks show the potential of the proposed model.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="sriram-2017-robus-speec">23</a>]
</td>
<td class="bibtexitem">
Anuroop Sriram, Heewoo Jun, Yashesh Gaur, and Sanjeev Satheesh.
 Robust speech recognition using generative adversarial networks.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#sriram-2017-robus-speec">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01567v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01567v1">http</a>&nbsp;]
<blockquote><font size="-1">
This paper describes a general, scalable, end-to-end
                  framework that uses the generative adversarial
                  network (GAN) objective to enable robust speech
                  recognition. Encoders trained with the proposed
                  approach enjoy improved invariance by learning to
                  map noisy audio to the same embedding space as that
                  of clean audio. Unlike previous methods, the new
                  framework does not rely on domain expertise or
                  simplifying assumptions as are often needed in
                  signal processing, and directly encourages
                  robustness in a data-driven way. We show the new
                  approach improves simulated far-field speech
                  recognition of vanilla sequence-to-sequence models
                  without specialized front-ends or preprocessing.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kang-2017-visual-aware">24</a>]
</td>
<td class="bibtexitem">
Wang-Cheng Kang, Chen Fang, Zhaowen Wang, and Julian McAuley.
 Visually-aware fashion recommendation and design with generative
  image models.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#kang-2017-visual-aware">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02231v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02231v1">http</a>&nbsp;]
<blockquote><font size="-1">
Building effective recommender systems for domains
                  like fashion is challenging due to the high level of
                  subjectivity and the semantic complexity of the
                  features involved (i.e., fashion styles). Recent
                  work has shown that approaches to `visual'
                  recommendation (e.g.&nbsp;clothing, art, etc.) can be
                  made more accurate by incorporating visual signals
                  directly into the recommendation objective, using
                  `off-the-shelf' feature representations derived from
                  deep networks. Here, we seek to extend this
                  contribution by showing that recommendation
                  performance can be significantly improved by
                  learning `fashion aware' image representations
                  directly, i.e., by training the image representation
                  (from the pixel level) and the recommender system
                  jointly; this contribution is related to recent work
                  using Siamese CNNs, though we are able to show
                  improvements over state-of-the-art recommendation
                  techniques such as BPR and variants that make use of
                  pre-trained visual features. Furthermore, we show
                  that our model can be used <em>generatively</em>,
                  i.e., given a user and a product category, we can
                  generate new images (i.e., clothing items) that are
                  most consistent with their personal taste. This
                  represents a first step towards building systems
                  that go beyond recommending existing items from a
                  product corpus, but which can be used to suggest
                  styles and aid the design of new products.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="choi-2017-can-maxout">25</a>]
</td>
<td class="bibtexitem">
Jae-Seok Choi and Munchurl Kim.
 Can maxout units downsize restoration networks? - single image
  super-resolution using lightweight cnn with maxout units.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#choi-2017-can-maxout">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02321v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02321v1">http</a>&nbsp;]
<blockquote><font size="-1">
Rectified linear units (ReLU) are well-known to be
                  helpful in obtaining faster convergence and thus
                  higher performance for many deep-learning-based
                  applications. However, networks with ReLU tend to
                  perform poorly when the number of filter parameters
                  is constrained to a small number. To overcome it, in
                  this paper, we propose a novel network utilizing
                  maxout units (MU), and show its effectiveness on
                  super-resolution (SR) applications. In general, the
                  MU has been known to make the filter sizes doubled
                  in generating the feature maps of the same sizes in
                  classification problems. In this paper, we first
                  reveal that the MU can even make the filter sizes
                  halved in restoration problems thus leading to
                  compaction of the network sizes. To show this, our
                  SR network is designed without increasing the filter
                  sizes with MU, which outperforms the state of the
                  art SR methods with a smaller number of filter
                  parameters. To the best of our knowledge, we are the
                  first to incorporate MU into SR applications and
                  show promising performance results. In MU, feature
                  maps from a previous convolutional layer are divided
                  into two parts along channels, which are then
                  compared element-wise and only their max values are
                  passed to a next layer. Along with some interesting
                  properties of MU to be analyzed, we further
                  investigate other variants of MU and their effects.
                  In addition, while ReLU have a trouble for learning
                  in networks with a very small number of
                  convolutional filter parameters, MU do not. For SR
                  applications, our MU-based network reconstructs
                  high-resolution images with comparable quality
                  compared to previous deep-learning-based SR methods,
                  with lower filter parameters.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="trattner-2017-food-recom-system">26</a>]
</td>
<td class="bibtexitem">
Christoph Trattner and David Elsweiler.
 Food recommender systems: Important contributions, challenges and
  future research directions.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#trattner-2017-food-recom-system">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02760">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02760v2">http</a>&nbsp;]
<blockquote><font size="-1">
The recommendation of food items is important for
                  many reasons. Attaining cooking inspiration via
                  digital sources is becoming evermore popular; as are
                  systems, which recommend other types of food, such
                  as meals in restaurants or products in supermarkets.
                  Researchers have been studying these kinds of
                  systems for many years, suggesting not only that can
                  they be a means to help people find food they might
                  want to eat, but also help them nourish themselves
                  more healthily. This paper provides a summary of the
                  state-of-the-art of so-called food recommender
                  systems, highlighting both seminal and most recent
                  approaches to the problem, as well as important
                  specializations, such as food recommendation systems
                  for groups of users or systems which promote healthy
                  eating. We moreover discuss the diverse challenges
                  involved in designing recsys for food, summarise the
                  lessons learned from past research and outline what
                  we believe to be important future directions and
                  open questions for the field. In providing these
                  contributions we hope to provide a useful resource
                  for researchers and practitioners alike.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="melis-2017-state-art">27</a>]
</td>
<td class="bibtexitem">
G&aacute;bor Melis, Chris Dyer, and Phil Blunsom.
 On the state of the art of evaluation in neural language models.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#melis-2017-state-art">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.05589">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.05589v1">http</a>&nbsp;]
<blockquote><font size="-1">
Ongoing innovations in recurrent neural network
                  architectures have provided a steady influx of
                  apparently state-of-the-art results on language
                  modelling benchmarks. However, these have been
                  evaluated using differing code bases and limited
                  computational resources, which represent
                  uncontrolled sources of experimental variation. We
                  reevaluate several popular architectures and
                  regularisation methods with large-scale automatic
                  black-box hyperparameter tuning and arrive at the
                  somewhat surprising conclusion that standard LSTM
                  architectures, when properly regularised, outperform
                  more recent models. We establish a new state of the
                  art on the Penn Treebank and Wikitext-2 corpora, as
                  well as strong baselines on the Hutter Prize
                  dataset.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="rahman-2017-contin-integ">28</a>]
</td>
<td class="bibtexitem">
Akond Rahman, Amritanshu Agrawal, Rahul Krishna, Alexander Sobran, and Tim
  Menzies.
 Continuous integration: The silver bullet?
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#rahman-2017-contin-integ">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.03933">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.03933v1">http</a>&nbsp;]
<blockquote><font size="-1">
Continuous integration (CI) tools integrate code
                  changes by automatically compiling, building, and
                  executing test cases upon submission of code
                  changes. Use of CI tools is getting increasingly
                  popular, yet how proprietary projects reap the
                  benefits of CI remains unknown. To investigate the
                  influence of CI on software development, we mine 661
                  open source software (OSS) projects, and 171
                  proprietary projects. For OSS projects, we observe
                  the expected benefits after CI adoption, i.e. more
                  bugs are resolved, and more issues are resolved.
                  However, for the proprietary projects, we cannot
                  make similar observations. Therefore, we cannot
                  claim that CI is the `silver bullet' for software
                  development. Why is this so? Our findings indicate
                  that only adoption of CI might not be enough to
                  improve software development. CI can be effective
                  for software development if practitioners use CI's
                  feedback mechanism efficiently, by applying the
                  practice of making frequent commits. For proprietary
                  projects we observe practitioners to commit less
                  frequently, and hence not use CI effectively, for
                  obtaining feedback on the submitted code changes. We
                  recommend practitioners to (i) apply the CI best
                  practices along with adoption of CI tools, (ii)
                  consider their team's development context before
                  adopting CI tools, and (iii) after adoption of CI,
                  investigate if CI satisfies their needs by applying
                  software analytics.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="iandola-2016-squeez">29</a>]
</td>
<td class="bibtexitem">
Forrest&nbsp;N. Iandola, Song Han, Matthew&nbsp;W. Moskewicz, Khalid Ashraf, William&nbsp;J.
  Dally, and Kurt Keutzer.
 Squeezenet: Alexnet-level accuracy with 50x fewer parameters and
  &lt;0.5mb model size.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#iandola-2016-squeez">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1602.07360">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1602.07360v4">http</a>&nbsp;]
<blockquote><font size="-1">
Recent research on deep neural networks has focused
                  primarily on improving accuracy. For a given
                  accuracy level, it is typically possible to identify
                  multiple DNN architectures that achieve that
                  accuracy level. With equivalent accuracy, smaller
                  DNN architectures offer at least three advantages:
                  (1) Smaller DNNs require less communication across
                  servers during distributed training. (2) Smaller
                  DNNs require less bandwidth to export a new model
                  from the cloud to an autonomous car. (3) Smaller
                  DNNs are more feasible to deploy on FPGAs and other
                  hardware with limited memory. To provide all of
                  these advantages, we propose a small DNN
                  architecture called SqueezeNet. SqueezeNet achieves
                  AlexNet-level accuracy on ImageNet with 50x fewer
                  parameters. Additionally, with model compression
                  techniques we are able to compress SqueezeNet to
                  less than 0.5MB (510x smaller than AlexNet). The
                  SqueezeNet architecture is available for download
                  here: https://github.com/DeepScale/SqueezeNet
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="pons-2017-end-to">30</a>]
</td>
<td class="bibtexitem">
Jordi Pons, Oriol Nieto, Matthew Prockup, Erik&nbsp;M. Schmidt, Andreas&nbsp;F. Ehmann,
  and Xavier Serra.
 End-to-end learning for music audio tagging at scale.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#pons-2017-end-to">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02520">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02520v2">http</a>&nbsp;]
<blockquote><font size="-1">
The lack of data tends to limit the outcomes of deep
                  learning research - specially, when dealing with
                  end-to-end learning stacks processing raw data such
                  as waveforms. In this study we make use of musical
                  labels annotated for 1.2 million tracks. This large
                  amount of data allows us to unrestrictedly explore
                  different front-end paradigms: from assumption-free
                  models - using waveforms as input with very small
                  convolutional filters; to models that rely on domain
                  knowledge - log-mel spectrograms with a
                  convolutional neural network designed to learn
                  temporal and timbral features. Results suggest that
                  while spectrogram-based models surpass their
                  waveform-based counterparts, the difference in
                  performance shrinks as more data are employed.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="seo-2017-neural-speed">31</a>]
</td>
<td class="bibtexitem">
Minjoon Seo, Sewon Min, Ali Farhadi, and Hannaneh Hajishirzi.
 Neural speed reading via skim-rnn.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#seo-2017-neural-speed">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02085">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02085v2">http</a>&nbsp;]
<blockquote><font size="-1">
Inspired by the principles of speed reading, we
                  introduce Skim-RNN, a recurrent neural network (RNN)
                  that dynamically decides to update only a small
                  fraction of the hidden state for relatively
                  unimportant input tokens. Skim-RNN gives
                  computational advantage over an RNN that always
                  updates the entire hidden state. Skim-RNN uses the
                  same input and output interfaces as a standard RNN
                  and can be easily used instead of RNNs in existing
                  models. In our experiments, we show that Skim-RNN
                  can achieve significantly reduced computational cost
                  without losing accuracy compared to standard RNNs
                  across five different natural language tasks. In
                  addition, we demonstrate that the trade-off between
                  accuracy and speed of Skim-RNN can be dynamically
                  controlled during inference time in a stable manner.
                  Our analysis also shows that Skim-RNN running on a
                  single CPU offers lower latency compared to standard
                  RNNs on GPUs.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wang-2017-iterat-school">32</a>]
</td>
<td class="bibtexitem">
Zhongxiang Wang, Ali Shafahi, and Ali Haghani.
 An iterative school decomposition algorithm for solving the
  multi-school bus routing and scheduling problem.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#wang-2017-iterat-school">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00532">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.00532v2">http</a>&nbsp;]
<blockquote><font size="-1">
Servicing the school transportation demand safely
                  with a minimum number of buses is one of the highest
                  financial goals for school transportation directors.
                  To achieve that objective, a good and efficient way
                  to solve the routing and scheduling problem is
                  required. Due to the growth of the computing power,
                  the spotlight has been shed on solving the combined
                  problem of the school bus routing and scheduling. A
                  recent attempt tried to model the routing problem by
                  maximizing the trip compatibilities with the hope of
                  requiring fewer buses in the scheduling problem.
                  However, an over-counting problem associated with
                  trip compatibility could diminish the performance of
                  this approach. An extended model is proposed in this
                  paper to resolve this issue along with an iterative
                  solution algorithm. This extended model is an
                  integrated model for multi-school bus routing and
                  scheduling problem. The result shows better
                  solutions for 8 test problems can be found with a
                  fewer number of buses (up to 25 %) and shorter
                  travel time (up to 7 % per trip).
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="young-2017-augmen-end">33</a>]
</td>
<td class="bibtexitem">
Tom Young, Erik Cambria, Iti Chaturvedi, Minlie Huang, Hao Zhou, and Subham
  Biswas.
 Augmenting end-to-end dialog systems with commonsense knowledge.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#young-2017-augmen-end">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.05453">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.05453v2">http</a>&nbsp;]
<blockquote><font size="-1">
Building dialog agents that can converse naturally
                  with humans is a challenging yet intriguing problem
                  of artificial intelligence. In open-domain
                  human-computer conversation, where the
                  conversational agent is expected to respond to human
                  responses in an interesting and engaging way,
                  commonsense knowledge has to be integrated into the
                  model effectively. In this paper, we investigate the
                  impact of providing commonsense knowledge about the
                  concepts covered in the dialog. Our model represents
                  the first attempt to integrating a large commonsense
                  knowledge base into end-to-end conversational
                  models. In the retrieval-based scenario, we propose
                  the Tri-LSTM model to jointly take into account
                  message and commonsense for selecting an appropriate
                  response. Our experiments suggest that the
                  knowledge-augmented models are superior to their
                  knowledge-free counterparts in automatic
                  evaluation.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="nanfack-2017-squeez-segnet">34</a>]
</td>
<td class="bibtexitem">
Geraldin Nanfack, Azeddine Elhassouny, and Rachid Oulad&nbsp;Haj Thami.
 Squeeze-segnet: A new fast deep convolutional neural network for
  semantic segmentation.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#nanfack-2017-squeez-segnet">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05491">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05491v1">http</a>&nbsp;]
<blockquote><font size="-1">
The recent researches in Deep Convolutional Neural
                  Network have focused their attention on improving
                  accuracy that provide significant advances. However,
                  if they were limited to classification tasks,
                  nowadays with contributions from Scientific
                  Communities who are embarking in this field, they
                  have become very useful in higher level tasks such
                  as object detection and pixel-wise semantic
                  segmentation. Thus, brilliant ideas in the field of
                  semantic segmentation with deep learning have
                  completed the state of the art of accuracy, however
                  this architectures become very difficult to apply in
                  embedded systems as is the case for autonomous
                  driving. We present a new Deep fully Convolutional
                  Neural Network for pixel-wise semantic segmentation
                  which we call Squeeze-SegNet. The architecture is
                  based on Encoder-Decoder style. We use a
                  SqueezeNet-like encoder and a decoder formed by our
                  proposed squeeze-decoder module and upsample layer
                  using downsample indices like in SegNet and we add a
                  deconvolution layer to provide final multi-channel
                  feature map. On datasets like Camvid or City-states,
                  our net gets SegNet-level accuracy with less than 10
                  times fewer parameters than SegNet.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ringgaard-2017-sling">35</a>]
</td>
<td class="bibtexitem">
Michael Ringgaard, Rahul Gupta, and Fernando C.&nbsp;N. Pereira.
 Sling: A framework for frame semantic parsing.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#ringgaard-2017-sling">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.07032">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.07032v1">http</a>&nbsp;]
<blockquote><font size="-1">
We describe SLING, a framework for parsing natural
                  language into semantic frames. SLING supports
                  general transition-based, neural-network parsing
                  with bidirectional LSTM input encoding and a
                  Transition Based Recurrent Unit (TBRU) for output
                  decoding. The parsing model is trained end-to-end
                  using only the text tokens as input. The transition
                  system has been designed to output frame graphs
                  directly without any intervening symbolic
                  representation. The SLING framework includes an
                  efficient and scalable frame store implementation as
                  well as a neural network JIT compiler for fast
                  inference during parsing. SLING is implemented in
                  C++ and it is available for download on GitHub.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="creswell-2017-gener-adver-networ">36</a>]
</td>
<td class="bibtexitem">
Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta,
  and Anil&nbsp;A Bharath.
 Generative adversarial networks: An overview.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#creswell-2017-gener-adver-networ">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.07035">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.07035v1">http</a>&nbsp;]
<blockquote><font size="-1">
Generative adversarial networks (GANs) provide a way
                  to learn deep representations without extensively
                  annotated training data. They achieve this through
                  deriving backpropagation signals through a
                  competitive process involving a pair of networks.
                  The representations that can be learned by GANs may
                  be used in a variety of applications, including
                  image synthesis, semantic image editing, style
                  transfer, image super-resolution and classification.
                  The aim of this review paper is to provide an
                  overview of GANs for the signal processing
                  community, drawing on familiar analogies and
                  concepts where possible. In addition to identifying
                  different methods for training and constructing
                  GANs, we also point to remaining challenges in their
                  theory and application.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="antoniou-2017-data-augmen">37</a>]
</td>
<td class="bibtexitem">
Antreas Antoniou, Amos Storkey, and Harrison Edwards.
 Data augmentation generative adversarial networks.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#antoniou-2017-data-augmen">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.04340v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.04340v1">http</a>&nbsp;]
<blockquote><font size="-1">
Effective training of neural networks requires much
                  data. In the low-data regime, parameters are
                  underdetermined, and learnt networks generalise
                  poorly. Data Augmentation
                  [?] alleviates this by
                  using existing data more effectively. However
                  standard data augmentation produces only limited
                  plausible alternative data. Given there is potential
                  to generate a much broader set of augmentations, we
                  design and train a generative model to do data
                  augmentation. The model, based on image conditional
                  Generative Adversarial Networks, takes data from a
                  source domain and learns to take any data item and
                  generalise it to generate other within-class data
                  items. As this generative process does not depend on
                  the classes themselves, it can be applied to novel
                  unseen classes of data. We show that a Data
                  Augmentation Generative Adversarial Network (DAGAN)
                  augments standard vanilla classifiers well. We also
                  show a DAGAN can enhance few-shot learning systems
                  such as Matching Networks. We demonstrate these
                  approaches on Omniglot, on EMNIST having learnt the
                  DAGAN on Omniglot, and VGG-Face data. In our
                  experiments we can see over 13 % increase in
                  accuracy in the low-data regime experiments in
                  Omniglot (from 69 % to 82 %), EMNIST (73.9 %
                  to 76 %) and VGG-Face (4.5 % to 12 %); in
                  Matching Networks for Omniglot we observe an
                  increase of 0.5 % (from 96.9 % to 97.4 %) and
                  an increase of 1.8 % in EMNIST (from 59.5 % to
                  61.3 %).
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhang-2017-advan-variat-infer">38</a>]
</td>
<td class="bibtexitem">
Cheng Zhang, Judith Butepage, Hedvig Kjellstrom, and Stephan Mandt.
 Advances in variational inference.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#zhang-2017-advan-variat-infer">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05597v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05597v1">http</a>&nbsp;]
<blockquote><font size="-1">
Many modern unsupervised or semi-supervised machine
                  learning algorithms rely on Bayesian probabilistic
                  models. These models are usually intractable and
                  thus require approximate inference. Variational
                  inference (VI) lets us approximate a
                  high-dimensional Bayesian posterior with a simpler
                  variational distribution by solving an optimization
                  problem. This approach has been successfully used in
                  various models and large-scale applications. In this
                  review, we give an overview of recent trends in
                  variational inference. We first introduce standard
                  mean field variational inference, then review recent
                  advances focusing on the following aspects: (a)
                  scalable VI, which includes stochastic
                  approximations, (b) generic VI, which extends the
                  applicability of VI to a large class of otherwise
                  intractable models, such as non-conjugate models,
                  (c) accurate VI, which includes variational models
                  beyond the mean field approximation or with atypical
                  divergences, and (d) amortized VI, which implements
                  the inference over local latent variables with
                  inference networks. Finally, we provide a summary of
                  promising future research directions.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="loshchilov-2017-fixin-weigh">39</a>]
</td>
<td class="bibtexitem">
Ilya Loshchilov and Frank Hutter.
 Fixing weight decay regularization in adam.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#loshchilov-2017-fixin-weigh">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05101v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.05101v1">http</a>&nbsp;]
<blockquote><font size="-1">
We note that common implementations of adaptive
                  gradient algorithms, such as Adam, limit the
                  potential benefit of weight decay regularization,
                  because the weights do not decay multiplicatively
                  (as would be expected for standard weight decay) but
                  by an additive constant factor. We propose a simple
                  way to resolve this issue by decoupling weight decay
                  and the optimization steps taken w.r.t. the loss
                  function. We provide empirical evidence that our
                  proposed modification (i) decouples the optimal
                  choice of weight decay factor from the setting of
                  the learning rate for both standard SGD and Adam,
                  and (ii) substantially improves Adam's
                  generalization performance, allowing it to compete
                  with SGD with momentum on image classification
                  datasets (on which it was previously typically
                  outperformed by the latter). We also demonstrate
                  that longer optimization runs require smaller weight
                  decay values for optimal results and introduce a
                  normalized variant of weight decay to reduce this
                  dependence. Finally, we propose a version of Adam
                  with warm restarts (AdamWR) that has strong anytime
                  performance while achieving state-of-the-art results
                  on CIFAR-10 and ImageNet32x32. Our source code is
                  available at
                  https://github.com/loshchil/AdamW-and-SGDW
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="liang-2017-new-method">40</a>]
</td>
<td class="bibtexitem">
Jiaxi Liang, Shojaeddin Chenouri, and Christopher&nbsp;G. Small.
 A new method for performance analysis in nonlinear dimensionality
  reduction.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#liang-2017-new-method">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.06252v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.06252v1">http</a>&nbsp;]
<blockquote><font size="-1">
In this paper, we develop a local rank correlation
                  measure which quantifies the performance of
                  dimension reduction methods. The local rank
                  correlation is easily interpretable, and robust
                  against the extreme skewness of nearest neighbor
                  distributions in high dimensions. Some benchmark
                  datasets are studied. We find that the local rank
                  correlation closely corresponds to our visual
                  interpretation of the quality of the output. In
                  addition, we demonstrate that the local rank
                  correlation is useful in estimating the intrinsic
                  dimensionality of the original data, and in
                  selecting a suitable value of tuning parameters used
                  in some algorithms.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lai-2015-giraf">41</a>]
</td>
<td class="bibtexitem">
Matthew Lai.
 Giraffe: Using deep reinforcement learning to play chess.
 <em>CoRR</em>, 2015.
[&nbsp;<a href="index_bib.html#lai-2015-giraf">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1509.01549">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1509.01549v2">http</a>&nbsp;]
<blockquote><font size="-1">
This report presents Giraffe, a chess engine that
                  uses self-play to discover all its domain-specific
                  knowledge, with minimal hand-crafted knowledge given
                  by the programmer. Unlike previous attempts using
                  machine learning only to perform parameter-tuning on
                  hand-crafted evaluation functions, Giraffe's
                  learning system also performs automatic feature
                  extraction and pattern recognition. The trained
                  evaluation function performs comparably to the
                  evaluation functions of state-of-the-art chess
                  engines - all of which containing thousands of lines
                  of carefully hand-crafted pattern recognizers, tuned
                  over many years by both computer chess experts and
                  human chess masters. Giraffe is the most successful
                  attempt thus far at using end-to-end machine
                  learning to play chess.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chen-2017-survey-dialog-system">42</a>]
</td>
<td class="bibtexitem">
Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang.
 A survey on dialogue systems: Recent advances and new frontiers.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#chen-2017-survey-dialog-system">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01731">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.01731v2">http</a>&nbsp;]
<blockquote><font size="-1">
Dialogue systems have attracted more and more
                  attention. Recent advances on dialogue systems are
                  overwhelmingly contributed by deep learning
                  techniques, which have been employed to enhance a
                  wide range of big data applications such as computer
                  vision, natural language processing, and recommender
                  systems. For dialogue systems, deep learning can
                  leverage a massive amount of data to learn
                  meaningful feature representations and response
                  generation strategies, while requiring a minimum
                  amount of hand-crafting. In this article, we give an
                  overview to these recent advances on dialogue
                  systems from various perspectives and discuss some
                  possible research directions. In particular, we
                  generally divide existing dialogue systems into
                  task-oriented and non-task-oriented models, then
                  detail how deep learning techniques help them with
                  representative algorithms and finally discuss some
                  appealing research directions that can bring the
                  dialogue system research into a new frontier.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mohapatra-2016-effic-optim">43</a>]
</td>
<td class="bibtexitem">
Pritish Mohapatra, Michal Rolinek, C.&nbsp;V. Jawahar, Vladimir Kolmogorov, and
  M.&nbsp;Pawan Kumar.
 Efficient optimization for rank-based loss functions.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#mohapatra-2016-effic-optim">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1604.08269">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1604.08269v2">http</a>&nbsp;]
<blockquote><font size="-1">
The accuracy of information retrieval systems is
                  often measured using complex loss functions such as
                  the average precision (AP) or the normalized
                  discounted cumulative gain (NDCG). Given a set of
                  positive (relevant) and negative (non-relevant)
                  samples, the parameters of a retrieval system can be
                  estimated by minimizing these loss functions.
                  However, the non-differentiability and
                  non-decomposability of these loss functions does not
                  allow for simple gradient based optimization
                  algorithms. This issue is generally circumvented by
                  either optimizing a structured hinge-loss upper
                  bound to the loss function or by using asymptotic
                  methods like the direct-loss minimization framework.
                  Yet, the high computational complexity of
                  loss-augmented inference, which is necessary for
                  both the frameworks, prohibits its use in large
                  training data sets. To alleviate this deficiency, we
                  present a novel quicksort flavored algorithm for a
                  large class of non-decomposable loss functions. We
                  provide a complete characterization of the loss
                  functions that are amenable to our algorithm, and
                  show that it includes both AP and NDCG based loss
                  functions. Furthermore, we prove that no comparison
                  based algorithm can improve upon the computational
                  complexity of our approach asymptotically. We
                  demonstrate the effectiveness of our approach in the
                  context of optimizing the structured hinge loss
                  upper bound of AP and NDCG loss for learning models
                  for a variety of vision tasks. Using the CIFAR-10
                  data set and the PASCAL VOC action recognition and
                  object detection data sets, we show that our
                  approach provides significantly better results than
                  simpler decomposable loss functions, while requiring
                  a comparable training time.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kowalski-2017-applic-natur">44</a>]
</td>
<td class="bibtexitem">
Radoslaw Kowalski, Marc Esteve, and Slava&nbsp;J. Mikhaylov.
 Application of natural language processing to determine user
  satisfaction in public services.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#kowalski-2017-applic-natur">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08083">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08083v1">http</a>&nbsp;]
<blockquote><font size="-1">
Research on customer satisfaction has increased
                  substantially in recent years. However, the relative
                  importance and relationships between different
                  determinants of satisfaction remains uncertain.
                  Moreover, quantitative studies to date tend to test
                  for significance of pre-determined factors thought
                  to have an influence with no scalable means to
                  identify other causes of user satisfaction. The gaps
                  in knowledge make it difficult to use available
                  knowledge on user preference for public service
                  improvement. Meanwhile, digital technology
                  development has enabled new methods to collect user
                  feedback, for example through online forums where
                  users can comment freely on their experience. New
                  tools are needed to analyze large volumes of such
                  feedback. Use of topic models is proposed as a
                  feasible solution to aggregate open-ended user
                  opinions that can be easily deployed in the public
                  sector. Generated insights can contribute to a more
                  inclusive decision-making process in public service
                  provision. This novel methodological approach is
                  applied to a case of service reviews of
                  publicly-funded primary care practices in England.
                  Findings from the analysis of 145,000 reviews
                  covering almost 7,700 primary care centers indicate
                  that the quality of interactions with staff and
                  bureaucratic exigencies are the key issues driving
                  user satisfaction across England.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu-2017-shift">45</a>]
</td>
<td class="bibtexitem">
Bichen Wu, Alvin Wan, Xiangyu Yue, Peter Jin, Sicheng Zhao, Noah Golmant, Amir
  Gholaminejad, Joseph Gonzalez, and Kurt Keutzer.
 Shift: A zero flop, zero parameter alternative to spatial
  convolutions.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#wu-2017-shift">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08141v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08141v1">http</a>&nbsp;]
<blockquote><font size="-1">
Neural networks rely on convolutions to aggregate
                  spatial information. However, spatial convolutions
                  are expensive in terms of model size and
                  computation, both of which grow quadratically with
                  respect to kernel size. In this paper, we present a
                  parameter-free, FLOP-free "shift" operation as an
                  alternative to spatial convolutions. We fuse shifts
                  and point-wise convolutions to construct end-to-end
                  trainable shift-based modules, with a hyperparameter
                  characterizing the tradeoff between accuracy and
                  efficiency. To demonstrate the operation's efficacy,
                  we replace ResNet's 3x3 convolutions with
                  shift-based modules for improved CIFAR10 and
                  CIFAR100 accuracy using 60 % fewer parameters; we
                  additionally demonstrate the operation's resilience
                  to parameter reduction on ImageNet, outperforming
                  ResNet family members. We finally show the shift
                  operation's applicability across domains, achieving
                  strong performance with fewer parameters on
                  classification, face verification and style
                  transfer.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="li-2017-light-head">46</a>]
</td>
<td class="bibtexitem">
Zeming Li, Chao Peng, Gang Yu, Xiangyu Zhang, Yangdong Deng, and Jian Sun.
 Light-head r-cnn: In defense of two-stage object detector.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#li-2017-light-head">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07264v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07264v1">http</a>&nbsp;]
<blockquote><font size="-1">
In this paper, we first investigate why typical
                  two-stage methods are not as fast as single-stage,
                  fast detectors like YOLO and SSD. We find that
                  Faster R-CNN and R-FCN perform an intensive
                  computation after or before RoI warping. Faster
                  R-CNN involves two fully connected layers for RoI
                  recognition, while R-FCN produces a large score
                  maps. Thus, the speed of these networks is slow due
                  to the heavy-head design in the architecture. Even
                  if we significantly reduce the base model, the
                  computation cost cannot be largely decreased
                  accordingly. We propose a new two-stage detector,
                  Light-Head R-CNN, to address the shortcoming in
                  current two-stage approaches. In our design, we make
                  the head of network as light as possible, by using a
                  thin feature map and a cheap R-CNN subnet (pooling
                  and single fully-connected layer). Our ResNet-101
                  based light-head R-CNN outperforms state-of-art
                  object detectors on COCO while keeping time
                  efficiency. More importantly, simply replacing the
                  backbone with a tiny network (e.g, Xception), our
                  Light-Head R-CNN gets 30.7 mmAP at 102 FPS on COCO,
                  significantly outperforming the single-stage, fast
                  detectors like YOLO and SSD on both speed and
                  accuracy. Code will be make publicly available.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wang-2017-non-local">47</a>]
</td>
<td class="bibtexitem">
Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He.
 Non-local neural networks.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#wang-2017-non-local">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07971v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07971v1">http</a>&nbsp;]
<blockquote><font size="-1">
Both convolutional and recurrent operations are
                  building blocks that process one local neighborhood
                  at a time. In this paper, we present non-local
                  operations as a generic family of building blocks
                  for capturing long-range dependencies. Inspired by
                  the classical non-local means method in computer
                  vision, our non-local operation computes the
                  response at a position as a weighted sum of the
                  features at all positions. This building block can
                  be plugged into many computer vision architectures.
                  On the task of video classification, even without
                  any bells and whistles, our non-local models can
                  compete or outperform current competition winners on
                  both Kinetics and Charades datasets. In static image
                  recognition, our non-local models improve object
                  detection/segmentation and pose estimation on the
                  COCO suite of tasks. Code will be made available.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="wu-2017-are-you">48</a>]
</td>
<td class="bibtexitem">
Qi&nbsp;Wu, Peng Wang, Chunhua Shen, Ian Reid, and Anton van&nbsp;den Hengel.
 Are you talking to me? reasoned visual dialog generation through
  adversarial learning.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#wu-2017-are-you">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07613v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.07613v1">http</a>&nbsp;]
<blockquote><font size="-1">
The Visual Dialogue task requires an agent to engage
                  in a conversation about an image with a human. It
                  represents an extension of the Visual Question
                  Answering task in that the agent needs to answer a
                  question about an image, but it needs to do so in
                  light of the previous dialogue that has taken place.
                  The key challenge in Visual Dialogue is thus
                  maintaining a consistent, and natural dialogue while
                  continuing to answer questions correctly. We present
                  a novel approach that combines Reinforcement
                  Learning and Generative Adversarial Networks (GANs)
                  to generate more human-like responses to questions.
                  The GAN helps overcome the relative paucity of
                  training data, and the tendency of the typical
                  MLE-based approach to generate overly terse answers.
                  Critically, the GAN is tightly integrated into the
                  attention mechanism that generates
                  human-interpretable reasons for each answer. This
                  means that the discriminative model of the GAN has
                  the task of assessing whether a candidate answer is
                  generated by a human or not, given the provided
                  reason. This is significant because it drives the
                  generative model to produce high quality answers
                  that are well supported by the associated reasoning.
                  The method also generates the state-of-the-art
                  results on the primary benchmark.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="liu-2017-rubys">49</a>]
</td>
<td class="bibtexitem">
Huiting Liu, Tao Lin, Hanfei Sun, Weijian Lin, Chih-Wei Chang, Teng Zhong, and
  Alexander Rudnicky.
 Rubystar: A non-task-oriented mixture model dialog system.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#liu-2017-rubys">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02781">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.02781v2">http</a>&nbsp;]
<blockquote><font size="-1">
RubyStar is a dialog system designed to create
                  "human-like" conversation by combining different
                  response generation strategies. RubyStar conducts a
                  non-task-oriented conversation on general topics by
                  using an ensemble of rule-based, retrieval-based and
                  generative methods. Topic detection, engagement
                  monitoring, and context tracking are used for
                  managing interaction. Predictable elements of
                  conversation, such as the bot's backstory and simple
                  question answering are handled by separate modules.
                  We describe a rating scheme we developed for
                  evaluating response generation. We find that
                  character-level RNN is an effective generation model
                  for general responses, with proper parameter
                  settings; however other kinds of conversation topics
                  might benefit from using other models.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhang-2017-actor-critic">50</a>]
</td>
<td class="bibtexitem">
Li&nbsp;Zhang, Flood Sung, Feng Liu, Tao Xiang, Shaogang Gong, Yongxin Yang, and
  Timothy&nbsp;M. Hospedales.
 Actor-critic sequence training for image captioning.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#zhang-2017-actor-critic">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.09601">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1706.09601v2">http</a>&nbsp;]
<blockquote><font size="-1">
Generating natural language descriptions of images
                  is an important capability for a robot or other
                  visual-intelligence driven AI agent that may need to
                  communicate with human users about what it is
                  seeing. Such image captioning methods are typically
                  trained by maximising the likelihood of ground-truth
                  annotated caption given the image. While simple and
                  easy to implement, this approach does not directly
                  maximise the language quality metrics we care about
                  such as CIDEr. In this paper we investigate training
                  image captioning methods based on actor-critic
                  reinforcement learning in order to directly optimise
                  non-differentiable quality metrics of interest. By
                  formulating a per-token advantage and value
                  computation strategy in this novel reinforcement
                  learning based captioning model, we show that it is
                  possible to achieve the state of the art performance
                  on the widely used MSCOCO benchmark.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="fong-2017-backp-as-funct">51</a>]
</td>
<td class="bibtexitem">
Brendan Fong, David&nbsp;I. Spivak, and R&eacute;my Tuy&eacute;ras.
 Backprop as functor: A compositional perspective on supervised
  learning.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#fong-2017-backp-as-funct">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10455">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10455v1">http</a>&nbsp;]
<blockquote><font size="-1">
A supervised learning algorithm searches over a set
                  of functions <em>A</em> -&gt;<em>B</em> parametrised by a space <em>P</em>
                  to find the best approximation to some ideal
                  function <em>f</em><em>A</em> -&gt;<em>B</em>. It does this by taking
                  examples (<em>a</em>,<em>f</em>(<em>a</em>)) in<em>A</em>&#215;<em>B</em>, and updating the
                  parameter according to some rule. We define a
                  category where these update rules may be composed,
                  and show that gradient descent---with respect to a
                  fixed step size and an error function satisfying a
                  certain property---defines a monoidal functor from a
                  category of parametrised functions to this category
                  of update rules. This provides a structural
                  perspective on backpropagation, as well as a broad
                  generalisation of neural networks.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="deng-2017-infer-users">52</a>]
</td>
<td class="bibtexitem">
Xiaofang Deng, Leilei Wu, Xiaolong Ren, Chunxiao Jia, Yuansheng Zhong, and
  Linyuan L&uuml;.
 Inferring users' preferences through leveraging their social
  relationships.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#deng-2017-infer-users">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10399">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.10399v1">http</a>&nbsp;]
<blockquote><font size="-1">
Recommender systems, inferring users' preferences
                  from their historical activities and personal
                  profiles, have been an enormous success in the last
                  several years. Most of the existing works are based
                  on the similarities of users, objects or both that
                  derived from their purchases records in the online
                  shopping platforms. Such approaches, however, are
                  facing bottlenecks when the known information is
                  limited. The extreme case is how to recommend
                  products to new users, namely the so-called
                  cold-start problem. The rise of the online social
                  networks gives us a chance to break the glass
                  ceiling. Birds of a feather flock together. Close
                  friends may have similar hidden pattern of selecting
                  products and the advices from friends are more
                  trustworthy. In this paper, we integrate the
                  individual's social relationships into recommender
                  systems and propose a new method, called Social Mass
                  Diffusion (SMD), based on a mass diffusion process
                  in the combined network of users' social network and
                  user-item bipartite network. The results show that
                  the SMD algorithm can achieve higher recommendation
                  accuracy than the Mass Diffusion (MD) purely on the
                  bipartite network. Especially, the improvement is
                  striking for small degree users. Moreover, SMD
                  provides a good solution to the cold-start problem.
                  The recommendation accuracy for new users
                  significantly higher than that of the conventional
                  popularity-based algorithm. These results may shed
                  some light on the new designs of better personalized
                  recommender systems and information services.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="botvinick-2017-build-machin">53</a>]
</td>
<td class="bibtexitem">
M.&nbsp;Botvinick, D.&nbsp;G.&nbsp;T. Barrett, P.&nbsp;Battaglia, N.&nbsp;de Freitas, D.&nbsp;Kumaran, J.&nbsp;Z
  Leibo, T.&nbsp;Lillicrap, J.&nbsp;Modayil, S.&nbsp;Mohamed, N.&nbsp;C. Rabinowitz, D.&nbsp;J. Rezende,
  A.&nbsp;Santoro, T.&nbsp;Schaul, C.&nbsp;Summerfield, G.&nbsp;Wayne, T.&nbsp;Weber, D.&nbsp;Wierstra,
  S.&nbsp;Legg, and D.&nbsp;Hassabis.
 Building machines that learn and think for themselves: Commentary on
  lake et al., behavioral and brain sciences, 2017.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#botvinick-2017-build-machin">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08378v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.08378v1">http</a>&nbsp;]
<blockquote><font size="-1">
We agree with Lake and colleagues on their list of
                  key ingredients for building humanlike intelligence,
                  including the idea that model-based reasoning is
                  essential. However, we favor an approach that
                  centers on one additional ingredient: autonomy. In
                  particular, we aim toward agents that can both build
                  and exploit their own internal models, with minimal
                  human hand-engineering. We believe an approach
                  centered on autonomous learning has the greatest
                  chance of success as we scale toward real-world
                  complexity, tackling domains for which ready-made
                  formal models are not available. Here we survey
                  several important examples of the progress that has
                  been made toward building autonomous agents with
                  humanlike abilities, and highlight some outstanding
                  challenges.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="choi-2017-starg">54</a>]
</td>
<td class="bibtexitem">
Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul
  Choo.
 Stargan: Unified generative adversarial networks for multi-domain
  image-to-image translation.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#choi-2017-starg">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.09020">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1711.09020v1">http</a>&nbsp;]
<blockquote><font size="-1">
Recent studies have shown remarkable success in
                  image-to-image translation for two domains. However,
                  existing approaches have limited scalability and
                  robustness in handling more than two domains, since
                  different models should be built independently for
                  every pair of image domains. To address this
                  limitation, we propose StarGAN, a novel and scalable
                  approach that can perform image-to-image
                  translations for multiple domains using only a
                  single model. Such a unified model architecture of
                  StarGAN allows simultaneous training of multiple
                  datasets with different domains within a single
                  network. This leads to StarGAN's superior quality of
                  translated images compared to existing models as
                  well as the novel capability of flexibly translating
                  an input image to any desired target domain. We
                  empirically demonstrate the effectiveness of our
                  approach on a facial attribute transfer and a facial
                  expression synthesis tasks.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="santander-vela-2017-agile-softw">55</a>]
</td>
<td class="bibtexitem">
Juande Santander-Vela.
 Agile software engineering and systems engineering at ska scale.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#santander-vela-2017-agile-softw">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00061">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00061v2">http</a>&nbsp;]
<blockquote><font size="-1">
Systems Engineering (SE) is the set of processes and
                  documentation required for successfully realising
                  large-scale engineering projects, but the classical
                  approach is not a good fit for software-intensive
                  projects, especially when the needs of the different
                  stakeholders are not fully known from the beginning,
                  and requirement priorities might change. The SKA is
                  the ultimate software-enabled telescope, with
                  enormous amounts of computing hardware and software
                  required to perform its data reduction. We give an
                  overview of the system and software engineering
                  processes in the SKA1 development, and the tension
                  between classical and agile SE.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhang-2017-fast-top">56</a>]
</td>
<td class="bibtexitem">
Fang Zhang, Xiaochen Wang, Jingfei Han, Jie Tang, Shiyin Wang, and
  Marie-Francine Moens.
 Fast top-k area topics extraction with knowledge base.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#zhang-2017-fast-top">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.04822">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.04822v2">http</a>&nbsp;]
<blockquote><font size="-1">
What are the most popular research topics in
                  Artificial Intelligence (AI)? We formulate the
                  problem as extracting top-<em>k</em> topics that can best
                  represent a given area with the help of knowledge
                  base. We theoretically prove that the problem is
                  NP-hard and propose an optimization model, FastKATE,
                  to address this problem by combining both explicit
                  and latent representations for each topic. We
                  leverage a large-scale knowledge base (Wikipedia) to
                  generate topic embeddings using neural networks and
                  use this kind of representations to help capture the
                  representativeness of topics for given areas. We
                  develop a fast heuristic algorithm to efficiently
                  solve the problem with a provable error bound. We
                  evaluate the proposed model on three real-world
                  datasets. Experimental results demonstrate our
                  model's effectiveness, robustness, real-timeness
                  (return results in <blockquote>&lt;1<em>s</em>), <em>and</em> <em>its</em> <em>superiority</em>
                  <em>over</em> <em>several</em> <em>alternative</em> <em>methods</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="xiang-2017-effec-batch">57</a>]
</td>
<td class="bibtexitem">
<em>Sitao</em> <em>Xiang</em> <em>and</em> <em>Hao</em> <em>Li</em>.
 <em>On</em> <em>the</em> <em>effects</em> <em>of</em> <em>batch</em> <em>and</em> <em>weight</em> <em>normalization</em> <em>in</em> <em>generative</em>
  <em>adversarial</em> <em>networks</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#xiang-2017-effec-batch">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1704.03971">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1704.03971v4">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Generative</em> <em>adversarial</em> <em>networks</em> (<em>GANs</em>) <em>are</em> <em>highly</em>
                  <em>effective</em> <em>unsupervised</em> <em>learning</em> <em>frameworks</em> <em>that</em> <em>can</em>
                  <em>generate</em> <em>very</em> <em>sharp</em> <em>data</em>, <em>even</em> <em>for</em> <em>data</em> <em>such</em> <em>as</em>
                  <em>images</em> <em>with</em> <em>complex</em>, <em>highly</em> <em>multimodal</em>
                  <em>distributions</em>. <em>However</em> <em>GANs</em> <em>are</em> <em>known</em> <em>to</em> <em>be</em> <em>very</em>
                  <em>hard</em> <em>to</em> <em>train</em>, <em>suffering</em> <em>from</em> <em>problems</em> <em>such</em> <em>as</em> <em>mode</em>
                  <em>collapse</em> <em>and</em> <em>disturbing</em> <em>visual</em> <em>artifacts</em>. <em>Batch</em>
                  <em>normalization</em> (<em>BN</em>) <em>techniques</em> <em>have</em> <em>been</em> <em>introduced</em>
                  <em>to</em> <em>address</em> <em>the</em> <em>training</em>. <em>Though</em> <em>BN</em> <em>accelerates</em> <em>the</em>
                  <em>training</em> <em>in</em> <em>the</em> <em>beginning</em>, <em>our</em> <em>experiments</em> <em>show</em> <em>that</em>
                  <em>the</em> <em>use</em> <em>of</em> <em>BN</em> <em>can</em> <em>be</em> <em>unstable</em> <em>and</em> <em>negatively</em> <em>impact</em>
                  <em>the</em> <em>quality</em> <em>of</em> <em>the</em> <em>trained</em> <em>model</em>. <em>The</em> <em>evaluation</em> <em>of</em>
                  <em>BN</em> <em>and</em> <em>numerous</em> <em>other</em> <em>recent</em> <em>schemes</em> <em>for</em> <em>improving</em>
                  <em>GAN</em> <em>training</em> <em>is</em> <em>hindered</em> <em>by</em> <em>the</em> <em>lack</em> <em>of</em> <em>an</em> <em>effective</em>
                  <em>objective</em> <em>quality</em> <em>measure</em> <em>for</em> <em>GAN</em> <em>models</em>. <em>To</em> <em>address</em>
                  <em>these</em> <em>issues</em>, <em>we</em> <em>first</em> <em>introduce</em> <em>a</em> <em>weight</em>
                  <em>normalization</em> (<em>WN</em>) <em>approach</em> <em>for</em> <em>GAN</em> <em>training</em> <em>that</em>
                  <em>significantly</em> <em>improves</em> <em>the</em> <em>stability</em>, <em>efficiency</em> <em>and</em>
                  <em>the</em> <em>quality</em> <em>of</em> <em>the</em> <em>generated</em> <em>samples</em>. <em>To</em> <em>allow</em> <em>a</em>
                  <em>methodical</em> <em>evaluation</em>, <em>we</em> <em>introduce</em> <em>squared</em>
                  <em>Euclidean</em> <em>reconstruction</em> <em>error</em> <em>on</em> <em>a</em> <em>test</em> <em>set</em> <em>as</em> <em>a</em>
                  <em>new</em> <em>objective</em> <em>measure</em>, <em>to</em> <em>assess</em> <em>training</em>
                  <em>performance</em> <em>in</em> <em>terms</em> <em>of</em> <em>speed</em>, <em>stability</em>, <em>and</em>
                  <em>quality</em> <em>of</em> <em>generated</em> <em>samples</em>. <em>Our</em> <em>experiments</em> <em>with</em> <em>a</em>
                  <em>standard</em> <em>DCGAN</em> <em>architecture</em> <em>on</em> <em>commonly</em> <em>used</em>
                  <em>datasets</em> (<em>CelebA</em>, <em>LSUN</em> <em>bedroom</em>, <em>and</em> <em>CIFAR</em>-10)
                  <em>indicate</em> <em>that</em> <em>training</em> <em>using</em> <em>WN</em> <em>is</em> <em>generally</em>
                  <em>superior</em> <em>to</em> <em>BN</em> <em>for</em> <em>GANs</em>, <em>achieving</em> 10 % <em>lower</em> <em>mean</em>
                  <em>squared</em> <em>loss</em> <em>for</em> <em>reconstruction</em> <em>and</em> <em>significantly</em>
                  <em>better</em> <em>qualitative</em> <em>results</em> <em>than</em> <em>BN</em>. <em>We</em> <em>further</em>
                  <em>demonstrate</em> <em>the</em> <em>stability</em> <em>of</em> <em>WN</em> <em>on</em> <em>a</em> 21-<em>layer</em> <em>ResNet</em>
                  <em>trained</em> <em>with</em> <em>the</em> <em>CelebA</em> <em>data</em> <em>set</em>. <em>The</em> <em>code</em> <em>for</em> <em>this</em>
                  <em>paper</em> <em>is</em> <em>available</em> <em>at</em>
                  <em>https</em>://<em>github</em>.<em>com</em>/<em>stormraiser</em>/<em>gan</em>-<em>weightnorm</em>-<em>resnet</em>
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="misra-2017-learn-by">58</a>]
</td>
<td class="bibtexitem">
<em>Ishan</em> <em>Misra</em>, <em>Ross</em> <em>Girshick</em>, <em>Rob</em> <em>Fergus</em>, <em>Martial</em> <em>Hebert</em>, <em>Abhinav</em> <em>Gupta</em>, <em>and</em>
  <em>Laurens</em> <em>van</em>&nbsp;<em>der</em> <em>Maaten</em>.
 <em>Learning</em> <em>by</em> <em>asking</em> <em>questions</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#misra-2017-learn-by">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01238">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01238v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>We</em> <em>introduce</em> <em>an</em> <em>interactive</em> <em>learning</em> <em>framework</em> <em>for</em>
                  <em>the</em> <em>development</em> <em>and</em> <em>testing</em> <em>of</em> <em>intelligent</em> <em>visual</em>
                  <em>systems</em>, <em>called</em> <em>learning</em>-<em>by</em>-<em>asking</em> (<em>LBA</em>). <em>We</em> <em>explore</em>
                  <em>LBA</em> <em>in</em> <em>context</em> <em>of</em> <em>the</em> <em>Visual</em> <em>Question</em> <em>Answering</em>
                  (<em>VQA</em>) <em>task</em>. <em>LBA</em> <em>differs</em> <em>from</em> <em>standard</em> <em>VQA</em> <em>training</em>
                  <em>in</em> <em>that</em> <em>most</em> <em>questions</em> <em>are</em> <em>not</em> <em>observed</em> <em>during</em>
                  <em>training</em> <em>time</em>, <em>and</em> <em>the</em> <em>learner</em> <em>must</em> <em>ask</em> <em>questions</em> <em>it</em>
                  <em>wants</em> <em>answers</em> <em>to</em>. <em>Thus</em>, <em>LBA</em> <em>more</em> <em>closely</em> <em>mimics</em>
                  <em>natural</em> <em>learning</em> <em>and</em> <em>has</em> <em>the</em> <em>potential</em> <em>to</em> <em>be</em> <em>more</em>
                  <em>data</em>-<em>efficient</em> <em>than</em> <em>the</em> <em>traditional</em> <em>VQA</em> <em>setting</em>. <em>We</em>
                  <em>present</em> <em>a</em> <em>model</em> <em>that</em> <em>performs</em> <em>LBA</em> <em>on</em> <em>the</em> <em>CLEVR</em>
                  <em>dataset</em>, <em>and</em> <em>show</em> <em>that</em> <em>it</em> <em>automatically</em> <em>discovers</em> <em>an</em>
                  <em>easy</em>-<em>to</em>-<em>hard</em> <em>curriculum</em> <em>when</em> <em>learning</em> <em>interactively</em>
                  <em>from</em> <em>an</em> <em>oracle</em>. <em>Our</em> <em>LBA</em> <em>generated</em> <em>data</em> <em>consistently</em>
                  <em>matches</em> <em>or</em> <em>outperforms</em> <em>the</em> <em>CLEVR</em> <em>train</em> <em>data</em> <em>and</em> <em>is</em>
                  <em>more</em> <em>sample</em> <em>efficient</em>. <em>We</em> <em>also</em> <em>show</em> <em>that</em> <em>our</em> <em>model</em>
                  <em>asks</em> <em>questions</em> <em>that</em> <em>generalize</em> <em>to</em> <em>state</em>-<em>of</em>-<em>the</em>-<em>art</em>
                  <em>VQA</em> <em>models</em> <em>and</em> <em>to</em> <em>novel</em> <em>test</em> <em>time</em> <em>distributions</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="adami-2017-mind-as">59</a>]
</td>
<td class="bibtexitem">
<em>Christoph</em> <em>Adami</em>.
 <em>The</em> <em>mind</em> <em>as</em> <em>a</em> <em>computational</em> <em>system</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#adami-2017-mind-as">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01093">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01093v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>The</em> <em>present</em> <em>document</em> <em>is</em> <em>an</em> <em>excerpt</em> <em>of</em> <em>an</em> <em>essay</em> <em>that</em>
                  <em>I</em> <em>wrote</em> <em>as</em> <em>part</em> <em>of</em> <em>my</em> <em>application</em> <em>material</em> <em>to</em>
                  <em>graduate</em> <em>school</em> <em>in</em> <em>Computer</em> <em>Science</em> (<em>with</em> <em>a</em> <em>focus</em> <em>on</em>
                  <em>Artificial</em> <em>Intelligence</em>), <em>in</em> 1986. <em>I</em> <em>was</em> <em>not</em> <em>invited</em>
                  <em>by</em> <em>any</em> <em>of</em> <em>the</em> <em>schools</em> <em>that</em> <em>received</em> <em>it</em>, <em>so</em> <em>I</em> <em>became</em>
                  <em>a</em> <em>theoretical</em> <em>physicist</em> <em>instead</em>. <em>The</em> <em>essay</em>'<em>s</em> <em>full</em>
                  <em>title</em> <em>was</em> "<em>Some</em> <em>Topics</em> <em>in</em> <em>Philosophy</em> <em>and</em> <em>Computer</em>
                  <em>Science</em>". <em>I</em> <em>am</em> <em>making</em> <em>this</em> <em>text</em> (<em>unchanged</em> <em>from</em>
                  1985, <em>preserving</em> <em>the</em> <em>typesetting</em> <em>as</em> <em>much</em> <em>as</em>
                  <em>possible</em>) <em>available</em> <em>now</em> <em>in</em> <em>memory</em> <em>of</em> <em>Jerry</em> <em>Fodor</em>,
                  <em>whose</em> <em>writings</em> <em>had</em> <em>influenced</em> <em>me</em> <em>significantly</em> <em>at</em>
                  <em>the</em> <em>time</em> (<em>even</em> <em>though</em> <em>I</em> <em>did</em> <em>not</em> <em>always</em> <em>agree</em>).
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="billings-2017-will-human">60</a>]
</td>
<td class="bibtexitem">
<em>Jay</em>&nbsp;<em>Jay</em> <em>Billings</em>, <em>Alexander</em>&nbsp;<em>J</em>. <em>McCaskey</em>, <em>Geoffroy</em> <em>Vallee</em>, <em>and</em> <em>Greg</em> <em>Watson</em>.
 <em>Will</em> <em>humans</em> <em>even</em> <em>write</em> <em>code</em> <em>in</em> 2040 <em>and</em> <em>what</em> <em>would</em> <em>that</em> <em>mean</em> <em>for</em>
  <em>extreme</em> <em>heterogeneity</em> <em>in</em> <em>computing</em>?
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#billings-2017-will-human">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00676">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00676v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Programming</em> <em>trends</em> <em>suggest</em> <em>that</em> <em>software</em> <em>development</em>
                  <em>will</em> <em>undergo</em> <em>a</em> <em>radical</em> <em>change</em> <em>in</em> <em>the</em> <em>future</em>: <em>the</em>
                  <em>combination</em> <em>of</em> <em>machine</em> <em>learning</em>, <em>artificial</em>
                  <em>intelligence</em>, <em>natural</em> <em>language</em> <em>processing</em>, <em>and</em> <em>code</em>
                  <em>generation</em> <em>technologies</em> <em>will</em> <em>improve</em> <em>in</em> <em>such</em> <em>a</em> <em>way</em>
                  <em>that</em> <em>machines</em>, <em>instead</em> <em>of</em> <em>humans</em>, <em>will</em> <em>write</em> <em>most</em> <em>of</em>
                  <em>their</em> <em>own</em> <em>code</em> <em>by</em> 2040. <em>This</em> <em>poses</em> <em>a</em> <em>number</em> <em>of</em>
                  <em>interesting</em> <em>challenges</em> <em>for</em> <em>scientific</em> <em>research</em>,
                  <em>especially</em> <em>as</em> <em>the</em> <em>hardware</em> <em>on</em> <em>which</em> <em>this</em> <em>Machine</em>
                  <em>Generated</em> <em>Code</em> <em>will</em> <em>run</em> <em>becomes</em> <em>extremely</em>
                  <em>heterogeneous</em>. <em>Indeed</em>, <em>extreme</em> <em>heterogeneity</em> <em>may</em>
                  <em>drive</em> <em>the</em> <em>creation</em> <em>of</em> <em>this</em> <em>technology</em> <em>because</em> <em>it</em>
                  <em>will</em> <em>allow</em> <em>humans</em> <em>to</em> <em>cope</em> <em>with</em> <em>the</em> <em>difficulty</em> <em>of</em>
                  <em>programming</em> <em>different</em> <em>devices</em> <em>efficiently</em> <em>and</em>
                  <em>easily</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="miller-2017-explain-ai">61</a>]
</td>
<td class="bibtexitem">
<em>Tim</em> <em>Miller</em>, <em>Piers</em> <em>Howe</em>, <em>and</em> <em>Liz</em> <em>Sonenberg</em>.
 <em>Explainable</em> <em>ai</em>: <em>Beware</em> <em>of</em> <em>inmates</em> <em>running</em> <em>the</em> <em>asylum</em> <em>or</em>: <em>How</em> <em>i</em> <em>learnt</em>
  <em>to</em> <em>stop</em> <em>worrying</em> <em>and</em> <em>love</em> <em>the</em> <em>social</em> <em>and</em> <em>behavioural</em> <em>sciences</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#miller-2017-explain-ai">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00547">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.00547v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>In</em> <em>his</em> <em>seminal</em> <em>book</em> `<em>The</em> <em>Inmates</em> <em>are</em> <em>Running</em> <em>the</em>
                  <em>Asylum</em>: <em>Why</em> <em>High</em>-<em>Tech</em> <em>Products</em> <em>Drive</em> <em>Us</em> <em>Crazy</em> <em>And</em>
                  <em>How</em> <em>To</em> <em>Restore</em> <em>The</em> <em>Sanity</em>' [2004, <em>Sams</em> <em>Indianapolis</em>,
                  <em>IN</em>, <em>USA</em>], <em>Alan</em> <em>Cooper</em> <em>argues</em> <em>that</em> <em>a</em> <em>major</em> <em>reason</em> <em>why</em>
                  <em>software</em> <em>is</em> <em>often</em> <em>poorly</em> <em>designed</em> (<em>from</em> <em>a</em> <em>user</em>
                  <em>perspective</em>) <em>is</em> <em>that</em> <em>programmers</em> <em>are</em> <em>in</em> <em>charge</em> <em>of</em>
                  <em>design</em> <em>decisions</em>, <em>rather</em> <em>than</em> <em>interaction</em> <em>designers</em>.
                  <em>As</em> <em>a</em> <em>result</em>, <em>programmers</em> <em>design</em> <em>software</em> <em>for</em>
                  <em>themselves</em>, <em>rather</em> <em>than</em> <em>for</em> <em>their</em> <em>target</em> <em>audience</em>; <em>a</em>
                  <em>phenomenon</em> <em>he</em> <em>refers</em> <em>to</em> <em>as</em> <em>the</em> `<em>inmates</em> <em>running</em> <em>the</em>
                  <em>asylum</em>'. <em>This</em> <em>paper</em> <em>argues</em> <em>that</em> <em>explainable</em> <em>AI</em> <em>risks</em>
                  <em>a</em> <em>similar</em> <em>fate</em>. <em>While</em> <em>the</em> <em>re</em>-<em>emergence</em> <em>of</em>
                  <em>explainable</em> <em>AI</em> <em>is</em> <em>positive</em>, <em>this</em> <em>paper</em> <em>argues</em> <em>most</em>
                  <em>of</em> <em>us</em> <em>as</em> <em>AI</em> <em>researchers</em> <em>are</em> <em>building</em> <em>explanatory</em>
                  <em>agents</em> <em>for</em> <em>ourselves</em>, <em>rather</em> <em>than</em> <em>for</em> <em>the</em> <em>intended</em>
                  <em>users</em>. <em>But</em> <em>explainable</em> <em>AI</em> <em>is</em> <em>more</em> <em>likely</em> <em>to</em> <em>succeed</em>
                  <em>if</em> <em>researchers</em> <em>and</em> <em>practitioners</em> <em>understand</em>, <em>adopt</em>,
                  <em>implement</em>, <em>and</em> <em>improve</em> <em>models</em> <em>from</em> <em>the</em> <em>vast</em> <em>and</em>
                  <em>valuable</em> <em>bodies</em> <em>of</em> <em>research</em> <em>in</em> <em>philosophy</em>,
                  <em>psychology</em>, <em>and</em> <em>cognitive</em> <em>science</em>; <em>and</em> <em>if</em> <em>evaluation</em>
                  <em>of</em> <em>these</em> <em>models</em> <em>is</em> <em>focused</em> <em>more</em> <em>on</em> <em>people</em> <em>than</em> <em>on</em>
                  <em>technology</em>. <em>From</em> <em>a</em> <em>light</em> <em>scan</em> <em>of</em> <em>literature</em>, <em>we</em>
                  <em>demonstrate</em> <em>that</em> <em>there</em> <em>is</em> <em>considerable</em> <em>scope</em> <em>to</em>
                  <em>infuse</em> <em>more</em> <em>results</em> <em>from</em> <em>the</em> <em>social</em> <em>and</em> <em>behavioural</em>
                  <em>sciences</em> <em>into</em> <em>explainable</em> <em>AI</em>, <em>and</em> <em>present</em> <em>some</em> <em>key</em>
                  <em>results</em> <em>from</em> <em>these</em> <em>fields</em> <em>that</em> <em>are</em> <em>relevant</em> <em>to</em>
                  <em>explainable</em> <em>AI</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="kong-2017-take-it">62</a>]
</td>
<td class="bibtexitem">
<em>Chen</em> <em>Kong</em> <em>and</em> <em>Simon</em> <em>Lucey</em>.
 <em>Take</em> <em>it</em> <em>in</em> <em>your</em> <em>stride</em>: <em>Do</em> <em>we</em> <em>need</em> <em>striding</em> <em>in</em> <em>cnns</em>?
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#kong-2017-take-it">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02502">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02502v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Since</em> <em>their</em> <em>inception</em>, <em>CNNs</em> <em>have</em> <em>utilized</em> <em>some</em> <em>type</em>
                  <em>of</em> <em>striding</em> <em>operator</em> <em>to</em> <em>reduce</em> <em>the</em> <em>overlap</em> <em>of</em>
                  <em>receptive</em> <em>fields</em> <em>and</em> <em>spatial</em> <em>dimensions</em>. <em>Although</em>
                  <em>having</em> <em>clear</em> <em>heuristic</em> <em>motivations</em> (<em>i</em>.<em>e</em>. <em>lowering</em>
                  <em>the</em> <em>number</em> <em>of</em> <em>parameters</em> <em>to</em> <em>learn</em>) <em>the</em> <em>mathematical</em>
                  <em>role</em> <em>of</em> <em>striding</em> <em>within</em> <em>CNN</em> <em>learning</em> <em>remains</em>
                  <em>unclear</em>. <em>This</em> <em>paper</em> <em>offers</em> <em>a</em> <em>novel</em> <em>and</em> <em>mathematical</em>
                  <em>rigorous</em> <em>perspective</em> <em>on</em> <em>the</em> <em>role</em> <em>of</em> <em>the</em> <em>striding</em>
                  <em>operator</em> <em>within</em> <em>modern</em> <em>CNNs</em>. <em>Specifically</em>, <em>we</em>
                  <em>demonstrate</em> <em>theoretically</em> <em>that</em> <em>one</em> <em>can</em> <em>always</em>
                  <em>represent</em> <em>a</em> <em>CNN</em> <em>that</em> <em>incorporates</em> <em>striding</em> <em>with</em> <em>an</em>
                  <em>equivalent</em> <em>non</em>-<em>striding</em> <em>CNN</em> <em>which</em> <em>has</em> <em>more</em> <em>filters</em>
                  <em>and</em> <em>smaller</em> <em>size</em>. <em>Through</em> <em>this</em> <em>equivalence</em> <em>we</em> <em>are</em>
                  <em>then</em> <em>able</em> <em>to</em> <em>characterize</em> <em>striding</em> <em>as</em> <em>an</em> <em>additional</em>
                  <em>mechanism</em> <em>for</em> <em>parameter</em> <em>sharing</em> <em>among</em> <em>channels</em>, <em>thus</em>
                  <em>reducing</em> <em>training</em> <em>complexity</em>. <em>Finally</em>, <em>the</em> <em>framework</em>
                  <em>presented</em> <em>in</em> <em>this</em> <em>paper</em> <em>offers</em> <em>a</em> <em>new</em> <em>mathematical</em>
                  <em>perspective</em> <em>on</em> <em>the</em> <em>role</em> <em>of</em> <em>striding</em> <em>which</em> <em>we</em> <em>hope</em>
                  <em>shall</em> <em>facilitate</em> <em>and</em> <em>simplify</em> <em>the</em> <em>future</em> <em>theoretical</em>
                  <em>analysis</em> <em>of</em> <em>CNNs</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="you-2017-imagen-train-minut">63</a>]
</td>
<td class="bibtexitem">
<em>Yang</em> <em>You</em>, <em>Zhao</em> <em>Zhang</em>, <em>Cho</em>-<em>Jui</em> <em>Hsieh</em>, <em>James</em> <em>Demmel</em>, <em>and</em> <em>Kurt</em> <em>Keutzer</em>.
 <em>Imagenet</em> <em>training</em> <em>in</em> <em>minutes</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#you-2017-imagen-train-minut">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.05011v8">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.05011v8">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Finishing</em> 90-<em>epoch</em> <em>ImageNet</em>-1<em>k</em> <em>training</em> <em>with</em>
                  <em>ResNet</em>-50 <em>on</em> <em>a</em> <em>NVIDIA</em> <em>M</em>40 <em>GPU</em> <em>takes</em> 14 <em>days</em>. <em>This</em>
                  <em>training</em> <em>requires</em> 10<sup>1</sup>8 <em>single</em> <em>precision</em> <em>operations</em>
                  <em>in</em> <em>total</em>. <em>On</em> <em>the</em> <em>other</em> <em>hand</em>, <em>the</em> <em>world</em>'<em>s</em> <em>current</em>
                  <em>fastest</em> <em>supercomputer</em> <em>can</em> <em>finish</em> 2 * 10<sup>1</sup>7 <em>single</em>
                  <em>precision</em> <em>operations</em> <em>per</em> <em>second</em> (<em>Dongarra</em> <em>et</em> <em>al</em>
                  2017, <em>https</em>://<em>www</em>.<em>top</em>500.<em>org</em>/<em>lists</em>/2017/06/). <em>If</em> <em>we</em>
                  <em>can</em> <em>make</em> <em>full</em> <em>use</em> <em>of</em> <em>the</em> <em>supercomputer</em> <em>for</em> <em>DNN</em>
                  <em>training</em>, <em>we</em> <em>should</em> <em>be</em> <em>able</em> <em>to</em> <em>finish</em> <em>the</em> 90-<em>epoch</em>
                  <em>ResNet</em>-50 <em>training</em> <em>in</em> <em>one</em> <em>minute</em>. <em>However</em>, <em>the</em>
                  <em>current</em> <em>bottleneck</em> <em>for</em> <em>fast</em> <em>DNN</em> <em>training</em> <em>is</em> <em>in</em> <em>the</em>
                  <em>algorithm</em> <em>level</em>. <em>Specifically</em>, <em>the</em> <em>current</em> <em>batch</em>
                  <em>size</em> (<em>e</em>.<em>g</em>. 512) <em>is</em> <em>too</em> <em>small</em> <em>to</em> <em>make</em> <em>efficient</em> <em>use</em>
                  <em>of</em> <em>many</em> <em>processors</em>. <em>For</em> <em>large</em>-<em>scale</em> <em>DNN</em> <em>training</em>, <em>we</em>
                  <em>focus</em> <em>on</em> <em>using</em> <em>large</em>-<em>batch</em> <em>data</em>-<em>parallelism</em>
                  <em>synchronous</em> <em>SGD</em> <em>without</em> <em>losing</em> <em>accuracy</em> <em>in</em> <em>the</em> <em>fixed</em>
                  <em>epochs</em>. <em>The</em> <em>LARS</em> <em>algorithm</em> (<em>You</em>, <em>Gitman</em>, <em>Ginsburg</em>,
                  2017, <em>arXiv</em>:1708.03888) <em>enables</em> <em>us</em> <em>to</em> <em>scale</em> <em>the</em>
                  <em>batch</em> <em>size</em> <em>to</em> <em>extremely</em> <em>large</em> <em>case</em> (<em>e</em>.<em>g</em>. 32<em>K</em>). <em>We</em>
                  <em>finish</em> <em>the</em> 100-<em>epoch</em> <em>ImageNet</em> <em>training</em> <em>with</em> <em>AlexNet</em>
                  <em>in</em> 11 <em>minutes</em> <em>on</em> 1024 <em>CPUs</em>. <em>About</em> <em>three</em> <em>times</em> <em>faster</em>
                  <em>than</em> <em>Facebook</em>'<em>s</em> <em>result</em> (<em>Goyal</em> <em>et</em> <em>al</em> 2017,
                  <em>arXiv</em>:1706.02677), <em>we</em> <em>finish</em> <em>the</em> 90-<em>epoch</em> <em>ImageNet</em>
                  <em>training</em> <em>with</em> <em>ResNet</em>-50 <em>in</em> 20 <em>minutes</em> <em>on</em> 2048 <em>KNLs</em>
                  <em>without</em> <em>losing</em> <em>accuracy</em>. <em>We</em> <em>got</em> 74.7 % <em>top</em>-1 <em>test</em>
                  <em>accuracy</em> <em>in</em> 64 <em>epochs</em>, <em>which</em> <em>only</em> <em>needs</em> 14 <em>minutes</em>.
                  <em>Furthermore</em>, <em>when</em> <em>we</em> <em>increase</em> <em>the</em> <em>batch</em> <em>size</em> <em>to</em>
                  <em>above</em> 16<em>K</em>, <em>our</em> <em>accuracy</em> <em>is</em> <em>much</em> <em>higher</em> <em>than</em>
                  <em>Facebook</em>'<em>s</em> <em>on</em> <em>corresponding</em> <em>batch</em> <em>sizes</em>. <em>Our</em> <em>source</em>
                  <em>code</em> <em>is</em> <em>available</em> <em>upon</em> <em>request</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhang-2017-shuff">64</a>]
</td>
<td class="bibtexitem">
<em>Xiangyu</em> <em>Zhang</em>, <em>Xinyu</em> <em>Zhou</em>, <em>Mengxiao</em> <em>Lin</em>, <em>and</em> <em>Jian</em> <em>Sun</em>.
 <em>Shufflenet</em>: <em>An</em> <em>extremely</em> <em>efficient</em> <em>convolutional</em> <em>neural</em> <em>network</em> <em>for</em>
  <em>mobile</em> <em>devices</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#zhang-2017-shuff">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.01083v2">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.01083v2">http</a>&nbsp;]
<blockquote><font size="-1">
<em>We</em> <em>introduce</em> <em>an</em> <em>extremely</em> <em>computation</em>-<em>efficient</em> <em>CNN</em>
                  <em>architecture</em> <em>named</em> <em>ShuffleNet</em>, <em>which</em> <em>is</em> <em>designed</em>
                  <em>specially</em> <em>for</em> <em>mobile</em> <em>devices</em> <em>with</em> <em>very</em> <em>limited</em>
                  <em>computing</em> <em>power</em> (<em>e</em>.<em>g</em>., 10-150 <em>MFLOPs</em>). <em>The</em> <em>new</em>
                  <em>architecture</em> <em>utilizes</em> <em>two</em> <em>new</em> <em>operations</em>, <em>pointwise</em>
                  <em>group</em> <em>convolution</em> <em>and</em> <em>channel</em> <em>shuffle</em>, <em>to</em> <em>greatly</em>
                  <em>reduce</em> <em>computation</em> <em>cost</em> <em>while</em> <em>maintaining</em> <em>accuracy</em>.
                  <em>Experiments</em> <em>on</em> <em>ImageNet</em> <em>classification</em> <em>and</em> <em>MS</em> <em>COCO</em>
                  <em>object</em> <em>detection</em> <em>demonstrate</em> <em>the</em> <em>superior</em>
                  <em>performance</em> <em>of</em> <em>ShuffleNet</em> <em>over</em> <em>other</em> <em>structures</em>,
                  <em>e</em>.<em>g</em>. <em>lower</em> <em>top</em>-1 <em>error</em> (<em>absolute</em> 7.8 %) <em>than</em> <em>recent</em>
                  <em>MobileNet</em> <em>on</em> <em>ImageNet</em> <em>classification</em> <em>task</em>, <em>under</em> <em>the</em>
                  <em>computation</em> <em>budget</em> <em>of</em> 40 <em>MFLOPs</em>. <em>On</em> <em>an</em> <em>ARM</em>-<em>based</em>
                  <em>mobile</em> <em>device</em>, <em>ShuffleNet</em> <em>achieves</em> &nbsp;13<em>x</em> <em>actual</em>
                  <em>speedup</em> <em>over</em> <em>AlexNet</em> <em>while</em> <em>maintaining</em> <em>comparable</em>
                  <em>accuracy</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="agarwal-2017-deep-networ">65</a>]
</td>
<td class="bibtexitem">
<em>Basant</em> <em>Agarwal</em>, <em>Heri</em> <em>Ramampiaro</em>, <em>Helge</em> <em>Langseth</em>, <em>and</em> <em>Massimiliano</em> <em>Ruocco</em>.
 <em>A</em> <em>deep</em> <em>network</em> <em>model</em> <em>for</em> <em>paraphrase</em> <em>detection</em> <em>in</em> <em>short</em> <em>text</em> <em>messages</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#agarwal-2017-deep-networ">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02820">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02820v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>This</em> <em>paper</em> <em>is</em> <em>concerned</em> <em>with</em> <em>paraphrase</em> <em>detection</em>.
                  <em>The</em> <em>ability</em> <em>to</em> <em>detect</em> <em>similar</em> <em>sentences</em> <em>written</em> <em>in</em>
                  <em>natural</em> <em>language</em> <em>is</em> <em>crucial</em> <em>for</em> <em>several</em>
                  <em>applications</em>, <em>such</em> <em>as</em> <em>text</em> <em>mining</em>, <em>text</em>
                  <em>summarization</em>, <em>plagiarism</em> <em>detection</em>, <em>authorship</em>
                  <em>authentication</em> <em>and</em> <em>question</em> <em>answering</em>. <em>Given</em> <em>two</em>
                  <em>sentences</em>, <em>the</em> <em>objective</em> <em>is</em> <em>to</em> <em>detect</em> <em>whether</em> <em>they</em>
                  <em>are</em> <em>semantically</em> <em>identical</em>. <em>An</em> <em>important</em> <em>insight</em>
                  <em>from</em> <em>this</em> <em>work</em> <em>is</em> <em>that</em> <em>existing</em> <em>paraphrase</em> <em>systems</em>
                  <em>perform</em> <em>well</em> <em>when</em> <em>applied</em> <em>on</em> <em>clean</em> <em>texts</em>, <em>but</em> <em>they</em>
                  <em>do</em> <em>not</em> <em>necessarily</em> <em>deliver</em> <em>good</em> <em>performance</em> <em>against</em>
                  <em>noisy</em> <em>texts</em>. <em>Challenges</em> <em>with</em> <em>paraphrase</em> <em>detection</em> <em>on</em>
                  <em>user</em> <em>generated</em> <em>short</em> <em>texts</em>, <em>such</em> <em>as</em> <em>Twitter</em>, <em>include</em>
                  <em>language</em> <em>irregularity</em> <em>and</em> <em>noise</em>. <em>To</em> <em>cope</em> <em>with</em> <em>these</em>
                  <em>challenges</em>, <em>we</em> <em>propose</em> <em>a</em> <em>novel</em> <em>deep</em> <em>neural</em>
                  <em>network</em>-<em>based</em> <em>approach</em> <em>that</em> <em>relies</em> <em>on</em> <em>coarse</em>-<em>grained</em>
                  <em>sentence</em> <em>modeling</em> <em>using</em> <em>a</em> <em>convolutional</em> <em>neural</em>
                  <em>network</em> <em>and</em> <em>a</em> <em>long</em> <em>short</em>-<em>term</em> <em>memory</em> <em>model</em>, <em>combined</em>
                  <em>with</em> <em>a</em> <em>specific</em> <em>fine</em>-<em>grained</em> <em>word</em>-<em>level</em> <em>similarity</em>
                  <em>matching</em> <em>model</em>. <em>Our</em> <em>experimental</em> <em>results</em> <em>show</em> <em>that</em>
                  <em>the</em> <em>proposed</em> <em>approach</em> <em>outperforms</em> <em>existing</em>
                  <em>state</em>-<em>of</em>-<em>the</em>-<em>art</em> <em>approaches</em> <em>on</em> <em>user</em>-<em>generated</em> <em>noisy</em>
                  <em>social</em> <em>media</em> <em>data</em>, <em>such</em> <em>as</em> <em>Twitter</em> <em>texts</em>, <em>and</em>
                  <em>achieves</em> <em>highly</em> <em>competitive</em> <em>performance</em> <em>on</em> <em>a</em> <em>cleaner</em>
                  <em>corpus</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="silver-2017-master-chess">66</a>]
</td>
<td class="bibtexitem">
<em>David</em> <em>Silver</em>, <em>Thomas</em> <em>Hubert</em>, <em>Julian</em> <em>Schrittwieser</em>, <em>Ioannis</em> <em>Antonoglou</em>, <em>Matthew</em>
  <em>Lai</em>, <em>Arthur</em> <em>Guez</em>, <em>Marc</em> <em>Lanctot</em>, <em>Laurent</em> <em>Sifre</em>, <em>Dharshan</em> <em>Kumaran</em>, <em>Thore</em>
  <em>Graepel</em>, <em>Timothy</em> <em>Lillicrap</em>, <em>Karen</em> <em>Simonyan</em>, <em>and</em> <em>Demis</em> <em>Hassabis</em>.
 <em>Mastering</em> <em>chess</em> <em>and</em> <em>shogi</em> <em>by</em> <em>self</em>-<em>play</em> <em>with</em> <em>a</em> <em>general</em> <em>reinforcement</em>
  <em>learning</em> <em>algorithm</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#silver-2017-master-chess">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01815v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.01815v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>The</em> <em>game</em> <em>of</em> <em>chess</em> <em>is</em> <em>the</em> <em>most</em> <em>widely</em>-<em>studied</em> <em>domain</em>
                  <em>in</em> <em>the</em> <em>history</em> <em>of</em> <em>artificial</em> <em>intelligence</em>. <em>The</em>
                  <em>strongest</em> <em>programs</em> <em>are</em> <em>based</em> <em>on</em> <em>a</em> <em>combination</em> <em>of</em>
                  <em>sophisticated</em> <em>search</em> <em>techniques</em>, <em>domain</em>-<em>specific</em>
                  <em>adaptations</em>, <em>and</em> <em>handcrafted</em> <em>evaluation</em> <em>functions</em>
                  <em>that</em> <em>have</em> <em>been</em> <em>refined</em> <em>by</em> <em>human</em> <em>experts</em> <em>over</em> <em>several</em>
                  <em>decades</em>. <em>In</em> <em>contrast</em>, <em>the</em> <em>AlphaGo</em> <em>Zero</em> <em>program</em>
                  <em>recently</em> <em>achieved</em> <em>superhuman</em> <em>performance</em> <em>in</em> <em>the</em> <em>game</em>
                  <em>of</em> <em>Go</em>, <em>by</em> <em>tabula</em> <em>rasa</em> <em>reinforcement</em> <em>learning</em> <em>from</em>
                  <em>games</em> <em>of</em> <em>self</em>-<em>play</em>. <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>generalise</em>
                  <em>this</em> <em>approach</em> <em>into</em> <em>a</em> <em>single</em> <em>AlphaZero</em> <em>algorithm</em> <em>that</em>
                  <em>can</em> <em>achieve</em>, <em>tabula</em> <em>rasa</em>, <em>superhuman</em> <em>performance</em> <em>in</em>
                  <em>many</em> <em>challenging</em> <em>domains</em>. <em>Starting</em> <em>from</em> <em>random</em> <em>play</em>,
                  <em>and</em> <em>given</em> <em>no</em> <em>domain</em> <em>knowledge</em> <em>except</em> <em>the</em> <em>game</em> <em>rules</em>,
                  <em>AlphaZero</em> <em>achieved</em> <em>within</em> 24 <em>hours</em> <em>a</em> <em>superhuman</em>
                  <em>level</em> <em>of</em> <em>play</em> <em>in</em> <em>the</em> <em>games</em> <em>of</em> <em>chess</em> <em>and</em> <em>shogi</em>
                  (<em>Japanese</em> <em>chess</em>) <em>as</em> <em>well</em> <em>as</em> <em>Go</em>, <em>and</em> <em>convincingly</em>
                  <em>defeated</em> <em>a</em> <em>world</em>-<em>champion</em> <em>program</em> <em>in</em> <em>each</em> <em>case</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chavdarova-2017-sgan">67</a>]
</td>
<td class="bibtexitem">
<em>Tatjana</em> <em>Chavdarova</em> <em>and</em> <em>Fran</em>&ccedil;<em>ois</em> <em>Fleuret</em>.
 <em>Sgan</em>: <em>An</em> <em>alternative</em> <em>training</em> <em>of</em> <em>generative</em> <em>adversarial</em> <em>networks</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#chavdarova-2017-sgan">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02330v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.02330v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>The</em> <em>Generative</em> <em>Adversarial</em> <em>Networks</em> (<em>GANs</em>) <em>have</em>
                  <em>demonstrated</em> <em>impressive</em> <em>performance</em> <em>for</em> <em>data</em>
                  <em>synthesis</em>, <em>and</em> <em>are</em> <em>now</em> <em>used</em> <em>in</em> <em>a</em> <em>wide</em> <em>range</em> <em>of</em>
                  <em>computer</em> <em>vision</em> <em>tasks</em>. <em>In</em> <em>spite</em> <em>of</em> <em>this</em> <em>success</em>,
                  <em>they</em> <em>gained</em> <em>a</em> <em>reputation</em> <em>for</em> <em>being</em> <em>difficult</em> <em>to</em>
                  <em>train</em>, <em>what</em> <em>results</em> <em>in</em> <em>a</em> <em>time</em>-<em>consuming</em> <em>and</em>
                  <em>human</em>-<em>involved</em> <em>development</em> <em>process</em> <em>to</em> <em>use</em> <em>them</em>. <em>We</em>
                  <em>consider</em> <em>an</em> <em>alternative</em> <em>training</em> <em>process</em>, <em>named</em>
                  <em>SGAN</em>, <em>in</em> <em>which</em> <em>several</em> <em>adversarial</em> "<em>local</em>" <em>pairs</em> <em>of</em>
                  <em>networks</em> <em>are</em> <em>trained</em> <em>independently</em> <em>so</em> <em>that</em> <em>a</em>
                  "<em>global</em>" <em>supervising</em> <em>pair</em> <em>of</em> <em>networks</em> <em>can</em> <em>be</em> <em>trained</em>
                  <em>against</em> <em>them</em>. <em>The</em> <em>goal</em> <em>is</em> <em>to</em> <em>train</em> <em>the</em> <em>global</em> <em>pair</em>
                  <em>with</em> <em>the</em> <em>corresponding</em> <em>ensemble</em> <em>opponent</em> <em>for</em>
                  <em>improved</em> <em>performances</em> <em>in</em> <em>terms</em> <em>of</em> <em>mode</em> <em>coverage</em>.
                  <em>This</em> <em>approach</em> <em>aims</em> <em>at</em> <em>increasing</em> <em>the</em> <em>chances</em> <em>that</em>
                  <em>learning</em> <em>will</em> <em>not</em> <em>stop</em> <em>for</em> <em>the</em> <em>global</em> <em>pair</em>,
                  <em>preventing</em> <em>both</em> <em>to</em> <em>be</em> <em>trapped</em> <em>in</em> <em>an</em> <em>unsatisfactory</em>
                  <em>local</em> <em>minimum</em>, <em>or</em> <em>to</em> <em>face</em> <em>oscillations</em> <em>often</em>
                  <em>observed</em> <em>in</em> <em>practice</em>. <em>To</em> <em>guarantee</em> <em>the</em> <em>latter</em>, <em>the</em>
                  <em>global</em> <em>pair</em> <em>never</em> <em>affects</em> <em>the</em> <em>local</em> <em>ones</em>. <em>The</em> <em>rules</em>
                  <em>of</em> <em>SGAN</em> <em>training</em> <em>are</em> <em>thus</em> <em>as</em> <em>follows</em>: <em>the</em> <em>global</em>
                  <em>generator</em> <em>and</em> <em>discriminator</em> <em>are</em> <em>trained</em> <em>using</em> <em>the</em>
                  <em>local</em> <em>discriminators</em> <em>and</em> <em>generators</em>, <em>respectively</em>,
                  <em>whereas</em> <em>the</em> <em>local</em> <em>networks</em> <em>are</em> <em>trained</em> <em>with</em> <em>their</em>
                  <em>fixed</em> <em>local</em> <em>opponent</em>. <em>Experimental</em> <em>results</em> <em>on</em> <em>both</em>
                  <em>toy</em> <em>and</em> <em>real</em>-<em>world</em> <em>problems</em> <em>demonstrate</em> <em>that</em> <em>this</em>
                  <em>approach</em> <em>outperforms</em> <em>standard</em> <em>training</em> <em>in</em> <em>terms</em> <em>of</em>
                  <em>better</em> <em>mitigating</em> <em>mode</em> <em>collapse</em>, <em>stability</em> <em>while</em>
                  <em>converging</em> <em>and</em> <em>that</em> <em>it</em> <em>surprisingly</em>, <em>increases</em> <em>the</em>
                  <em>convergence</em> <em>speed</em> <em>as</em> <em>well</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ramachandran-2017-autom">68</a>]
</td>
<td class="bibtexitem">
<em>Prabhu</em> <em>Ramachandran</em>.
 <em>Automan</em>: <em>a</em> <em>simple</em>, <em>python</em>-<em>based</em>, <em>automation</em> <em>framework</em> <em>for</em> <em>numerical</em>
  <em>computing</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#ramachandran-2017-autom">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04786">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04786v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>We</em> <em>present</em> <em>a</em> <em>simple</em> <em>framework</em> <em>that</em> <em>allows</em> <em>a</em>
                  <em>researcher</em> <em>to</em> <em>automate</em> <em>their</em> <em>computational</em>
                  <em>simulations</em>. <em>In</em> <em>particular</em> <em>the</em> <em>framework</em> <em>facilitates</em>
                  <em>assembling</em> <em>several</em> <em>long</em>-<em>running</em> <em>computations</em> <em>and</em>
                  <em>producing</em> <em>various</em> <em>plots</em> <em>as</em> <em>a</em> <em>result</em> <em>of</em> <em>these</em>
                  <em>computations</em>. <em>The</em> <em>framework</em> <em>makes</em> <em>it</em> <em>possible</em> <em>to</em>
                  <em>reproduce</em> <em>every</em> <em>figure</em> <em>made</em> <em>for</em> <em>a</em> <em>publication</em> <em>with</em> <em>a</em>
                  <em>single</em> <em>command</em>. <em>It</em> <em>also</em> <em>allows</em> <em>one</em> <em>to</em> <em>distribute</em> <em>the</em>
                  <em>computations</em> <em>across</em> <em>a</em> <em>network</em> <em>of</em> <em>computers</em>. <em>The</em>
                  <em>framework</em> <em>has</em> <em>been</em> <em>used</em> <em>to</em> <em>write</em> <em>a</em> <em>research</em> <em>paper</em> <em>in</em>
                  <em>the</em> <em>area</em> <em>of</em> <em>numerical</em> <em>computing</em>. <em>This</em> <em>paper</em>
                  <em>discusses</em> <em>the</em> <em>benefits</em> <em>of</em> <em>such</em> <em>a</em> <em>system</em> <em>both</em> <em>for</em> <em>the</em>
                  <em>researcher</em> <em>and</em> <em>to</em> <em>the</em> <em>research</em> <em>area</em> <em>in</em> <em>general</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lillis-2017-hierar-bloom">69</a>]
</td>
<td class="bibtexitem">
<em>David</em> <em>Lillis</em>, <em>Frank</em> <em>Breitinger</em>, <em>and</em> <em>Mark</em> <em>Scanlon</em>.
 <em>Hierarchical</em> <em>bloom</em> <em>filter</em> <em>trees</em> <em>for</em> <em>approximate</em> <em>matching</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#lillis-2017-hierar-bloom">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04544">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.04544v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Bytewise</em> <em>approximate</em> <em>matching</em> <em>algorithms</em> <em>have</em> <em>in</em>
                  <em>recent</em> <em>years</em> <em>shown</em> <em>significant</em> <em>promise</em> <em>in</em> <em>de</em>-
                  <em>tecting</em> <em>files</em> <em>that</em> <em>are</em> <em>similar</em> <em>at</em> <em>the</em> <em>byte</em> <em>level</em>.
                  <em>This</em> <em>is</em> <em>very</em> <em>useful</em> <em>for</em> <em>digital</em> <em>forensic</em>
                  <em>investigators</em>, <em>who</em> <em>are</em> <em>regularly</em> <em>faced</em> <em>with</em> <em>the</em>
                  <em>problem</em> <em>of</em> <em>searching</em> <em>through</em> <em>a</em> <em>seized</em> <em>device</em> <em>for</em>
                  <em>pertinent</em> <em>data</em>. <em>A</em> <em>common</em> <em>scenario</em> <em>is</em> <em>where</em> <em>an</em>
                  <em>investigator</em> <em>is</em> <em>in</em> <em>possession</em> <em>of</em> <em>a</em> <em>collection</em> <em>of</em>
                  "<em>known</em>-<em>illegal</em>" <em>files</em> (<em>e</em>.<em>g</em>. <em>a</em> <em>collection</em> <em>of</em> <em>child</em>
                  <em>abuse</em> <em>material</em>) <em>and</em> <em>wishes</em> <em>to</em> <em>find</em> <em>whether</em> <em>copies</em> <em>of</em>
                  <em>these</em> <em>are</em> <em>stored</em> <em>on</em> <em>the</em> <em>seized</em> <em>device</em>. <em>Approximate</em>
                  <em>matching</em> <em>addresses</em> <em>shortcomings</em> <em>in</em> <em>traditional</em>
                  <em>hashing</em>, <em>which</em> <em>can</em> <em>only</em> <em>find</em> <em>identical</em> <em>files</em>, <em>by</em>
                  <em>also</em> <em>being</em> <em>able</em> <em>to</em> <em>deal</em> <em>with</em> <em>cases</em> <em>of</em> <em>merged</em> <em>files</em>,
                  <em>embedded</em> <em>files</em>, <em>partial</em> <em>files</em>, <em>or</em> <em>if</em> <em>a</em> <em>file</em> <em>has</em> <em>been</em>
                  <em>changed</em> <em>in</em> <em>any</em> <em>way</em>. <em>Most</em> <em>approximate</em> <em>matching</em>
                  <em>algorithms</em> <em>work</em> <em>by</em> <em>comparing</em> <em>pairs</em> <em>of</em> <em>files</em>, <em>which</em>
                  <em>is</em> <em>not</em> <em>a</em> <em>scalable</em> <em>approach</em> <em>when</em> <em>faced</em> <em>with</em> <em>large</em>
                  <em>corpora</em>. <em>This</em> <em>paper</em> <em>demonstrates</em> <em>the</em> <em>effectiveness</em>
                  <em>of</em> <em>using</em> <em>a</em> "<em>Hierarchical</em> <em>Bloom</em> <em>Filter</em> <em>Tree</em>" (<em>HBFT</em>)
                  <em>data</em> <em>structure</em> <em>to</em> <em>reduce</em> <em>the</em> <em>running</em> <em>time</em> <em>of</em>
                  <em>collection</em>-<em>against</em>-<em>collection</em> <em>matching</em>, <em>with</em> <em>a</em>
                  <em>specific</em> <em>focus</em> <em>on</em> <em>the</em> <em>MRSH</em>-<em>v</em>2 <em>algorithm</em>. <em>Three</em>
                  <em>experiments</em> <em>are</em> <em>discussed</em>, <em>which</em> <em>explore</em> <em>the</em> <em>effects</em>
                  <em>of</em> <em>different</em> <em>configurations</em> <em>of</em> <em>HBFTs</em>. <em>The</em> <em>proposed</em>
                  <em>approach</em> <em>dramatically</em> <em>reduces</em> <em>the</em> <em>number</em> <em>of</em> <em>pairwise</em>
                  <em>comparisons</em> <em>required</em>, <em>and</em> <em>demonstrates</em> <em>substantial</em>
                  <em>speed</em> <em>gains</em>, <em>while</em> <em>maintaining</em> <em>effectiveness</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="svenstrup-2017-hash-embed">70</a>]
</td>
<td class="bibtexitem">
<em>Dan</em> <em>Svenstrup</em>, <em>Jonas</em>&nbsp;<em>Meinertz</em> <em>Hansen</em>, <em>and</em> <em>Ole</em> <em>Winther</em>.
 <em>Hash</em> <em>embeddings</em> <em>for</em> <em>efficient</em> <em>word</em> <em>representations</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#svenstrup-2017-hash-embed">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.03933">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1709.03933v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>We</em> <em>present</em> <em>hash</em> <em>embeddings</em>, <em>an</em> <em>efficient</em> <em>method</em> <em>for</em>
                  <em>representing</em> <em>words</em> <em>in</em> <em>a</em> <em>continuous</em> <em>vector</em> <em>form</em>. <em>A</em>
                  <em>hash</em> <em>embedding</em> <em>may</em> <em>be</em> <em>seen</em> <em>as</em> <em>an</em> <em>interpolation</em>
                  <em>between</em> <em>a</em> <em>standard</em> <em>word</em> <em>embedding</em> <em>and</em> <em>a</em> <em>word</em>
                  <em>embedding</em> <em>created</em> <em>using</em> <em>a</em> <em>random</em> <em>hash</em> <em>function</em> (<em>the</em>
                  <em>hashing</em> <em>trick</em>). <em>In</em> <em>hash</em> <em>embeddings</em> <em>each</em> <em>token</em> <em>is</em>
                  <em>represented</em> <em>by</em> k d-<em>dimensional</em> <em>embeddings</em>
                  <em>vectors</em> <em>and</em> <em>one</em> k <em>dimensional</em> <em>weight</em> <em>vector</em>. <em>The</em>
                  <em>final</em> d <em>dimensional</em> <em>representation</em> <em>of</em> <em>the</em> <em>token</em> <em>is</em>
                  <em>the</em> <em>product</em> <em>of</em> <em>the</em> <em>two</em>. <em>Rather</em> <em>than</em> <em>fitting</em> <em>the</em>
                  <em>embedding</em> <em>vectors</em> <em>for</em> <em>each</em> <em>token</em> <em>these</em> <em>are</em> <em>selected</em>
                  <em>by</em> <em>the</em> <em>hashing</em> <em>trick</em> <em>from</em> <em>a</em> <em>shared</em> <em>pool</em> <em>of</em> B
                  <em>embedding</em> <em>vectors</em>. <em>Our</em> <em>experiments</em> <em>show</em> <em>that</em> <em>hash</em>
                  <em>embeddings</em> <em>can</em> <em>easily</em> <em>deal</em> <em>with</em> <em>huge</em> <em>vocabularies</em>
                  <em>consisting</em> <em>of</em> <em>millions</em> <em>of</em> <em>tokens</em>. <em>When</em> <em>using</em> <em>a</em> <em>hash</em>
                  <em>embedding</em> <em>there</em> <em>is</em> <em>no</em> <em>need</em> <em>to</em> <em>create</em> <em>a</em> <em>dictionary</em>
                  <em>before</em> <em>training</em> <em>nor</em> <em>to</em> <em>perform</em> <em>any</em> <em>kind</em> <em>of</em>
                  <em>vocabulary</em> <em>pruning</em> <em>after</em> <em>training</em>. <em>We</em> <em>show</em> <em>that</em>
                  <em>models</em> <em>trained</em> <em>using</em> <em>hash</em> <em>embeddings</em> <em>exhibit</em> <em>at</em>
                  <em>least</em> <em>the</em> <em>same</em> <em>level</em> <em>of</em> <em>performance</em> <em>as</em> <em>models</em>
                  <em>trained</em> <em>using</em> <em>regular</em> <em>embeddings</em> <em>across</em> <em>a</em> <em>wide</em> <em>range</em>
                  <em>of</em> <em>tasks</em>. <em>Furthermore</em>, <em>the</em> <em>number</em> <em>of</em> <em>parameters</em>
                  <em>needed</em> <em>by</em> <em>such</em> <em>an</em> <em>embedding</em> <em>is</em> <em>only</em> <em>a</em> <em>fraction</em> <em>of</em>
                  <em>what</em> <em>is</em> <em>required</em> <em>by</em> <em>a</em> <em>regular</em> <em>embedding</em>. <em>Since</em>
                  <em>standard</em> <em>embeddings</em> <em>and</em> <em>embeddings</em> <em>constructed</em> <em>using</em>
                  <em>the</em> <em>hashing</em> <em>trick</em> <em>are</em> <em>actually</em> <em>just</em> <em>special</em> <em>cases</em> <em>of</em>
                  <em>a</em> <em>hash</em> <em>embedding</em>, <em>hash</em> <em>embeddings</em> <em>can</em> <em>be</em> <em>considered</em>
                  <em>an</em> <em>extension</em> <em>and</em> <em>improvement</em> <em>over</em> <em>the</em> <em>existing</em>
                  <em>regular</em> <em>embedding</em> <em>types</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="bocklisch-2017-rasa">71</a>]
</td>
<td class="bibtexitem">
<em>Tom</em> <em>Bocklisch</em>, <em>Joey</em> <em>Faulker</em>, <em>Nick</em> <em>Pawlowski</em>, <em>and</em> <em>Alan</em> <em>Nichol</em>.
 <em>Rasa</em>: <em>Open</em> <em>source</em> <em>language</em> <em>understanding</em> <em>and</em> <em>dialogue</em> <em>management</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#bocklisch-2017-rasa">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.05181">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.05181v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>We</em> <em>introduce</em> <em>a</em> <em>pair</em> <em>of</em> <em>tools</em>, <em>Rasa</em> <em>NLU</em> <em>and</em> <em>Rasa</em>
                  <em>Core</em>, <em>which</em> <em>are</em> <em>open</em> <em>source</em> <em>python</em> <em>libraries</em> <em>for</em>
                  <em>building</em> <em>conversational</em> <em>software</em>. <em>Their</em> <em>purpose</em> <em>is</em>
                  <em>to</em> <em>make</em> <em>machine</em>-<em>learning</em> <em>based</em> <em>dialogue</em> <em>management</em>
                  <em>and</em> <em>language</em> <em>understanding</em> <em>accessible</em> <em>to</em>
                  <em>non</em>-<em>specialist</em> <em>software</em> <em>developers</em>. <em>In</em> <em>terms</em> <em>of</em>
                  <em>design</em> <em>philosophy</em>, <em>we</em> <em>aim</em> <em>for</em> <em>ease</em> <em>of</em> <em>use</em>, <em>and</em>
                  <em>bootstrapping</em> <em>from</em> <em>minimal</em> (<em>or</em> <em>no</em>) <em>initial</em> <em>training</em>
                  <em>data</em>. <em>Both</em> <em>packages</em> <em>are</em> <em>extensively</em> <em>documented</em> <em>and</em>
                  <em>ship</em> <em>with</em> <em>a</em> <em>comprehensive</em> <em>suite</em> <em>of</em> <em>tests</em>. <em>The</em> <em>code</em>
                  <em>is</em> <em>available</em> <em>at</em> <em>https</em>://<em>github</em>.<em>com</em>/<em>RasaHQ</em>/
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="meyer-2017-fourt-years">72</a>]
</td>
<td class="bibtexitem">
<em>Bertrand</em> <em>Meyer</em>.
 <em>Fourteen</em> <em>years</em> <em>of</em> <em>software</em> <em>engineering</em> <em>at</em> <em>eth</em> <em>zurich</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#meyer-2017-fourt-years">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.05078">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.05078v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>A</em> <em>Chair</em> <em>of</em> <em>Software</em> <em>Engineering</em> <em>existed</em> <em>at</em> <em>ETH</em>
                  <em>Zurich</em>, <em>the</em> <em>Swiss</em> <em>Federal</em> <em>Insti</em>-<em>tute</em> <em>of</em> <em>Technology</em>,
                  <em>from</em> 1 <em>October</em> 2001 <em>to</em> 31 <em>January</em> 2016, <em>under</em> <em>my</em>
                  <em>leader</em>-<em>ship</em>. <em>Our</em> <em>work</em>, <em>summarized</em> <em>here</em>, <em>covered</em> <em>a</em>
                  <em>wide</em> <em>range</em> <em>of</em> <em>theoretical</em> <em>and</em> <em>practi</em>-<em>cal</em> <em>topics</em>,
                  <em>with</em> <em>object</em> <em>technology</em> <em>in</em> <em>the</em> <em>Eiffel</em> <em>method</em> <em>as</em> <em>the</em>
                  <em>unifying</em> <em>thread</em> .
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="zhao-2017-lever-long">73</a>]
</td>
<td class="bibtexitem">
<em>Wei</em> <em>Zhao</em>, <em>Benyou</em> <em>Wang</em>, <em>Jianbo</em> <em>Ye</em>, <em>Yongqiang</em> <em>Gao</em>, <em>Min</em> <em>Yang</em>, <em>Zhou</em> <em>Zhao</em>, <em>and</em>
  <em>Xiaojun</em> <em>Chen</em>.
 <em>Leveraging</em> <em>long</em> <em>and</em> <em>short</em>-<em>term</em> <em>information</em> <em>in</em> <em>content</em>-<em>aware</em> <em>movie</em>
  <em>recommendation</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#zhao-2017-lever-long">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09059">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09059v2">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Movie</em> <em>recommendation</em> <em>systems</em> <em>provide</em> <em>users</em> <em>with</em>
                  <em>ranked</em> <em>lists</em> <em>of</em> <em>movies</em> <em>based</em> <em>on</em> <em>individual</em>'<em>s</em>
                  <em>preferences</em> <em>and</em> <em>constraints</em>. <em>Two</em> <em>types</em> <em>of</em> <em>models</em> <em>are</em>
                  <em>commonly</em> <em>used</em> <em>to</em> <em>generate</em> <em>ranking</em> <em>results</em>: <em>long</em>-<em>term</em>
                  <em>models</em> <em>and</em> <em>session</em>-<em>based</em> <em>models</em>. <em>While</em> <em>long</em>-<em>term</em>
                  <em>models</em> <em>represent</em> <em>the</em> <em>interactions</em> <em>between</em> <em>users</em> <em>and</em>
                  <em>movies</em> <em>that</em> <em>are</em> <em>supposed</em> <em>to</em> <em>change</em> <em>slowly</em> <em>across</em>
                  <em>time</em>, <em>session</em>-<em>based</em> <em>models</em> <em>encode</em> <em>the</em> <em>information</em> <em>of</em>
                  <em>users</em>' <em>interests</em> <em>and</em> <em>changing</em> <em>dynamics</em> <em>of</em> <em>movies</em>'
                  <em>attributes</em> <em>in</em> <em>short</em> <em>terms</em>. <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>propose</em>
                  <em>an</em> <em>LSIC</em> <em>model</em>, <em>leveraging</em> <em>Long</em> <em>and</em> <em>Short</em>-<em>term</em>
                  <em>Information</em> <em>in</em> <em>Content</em>-<em>aware</em> <em>movie</em> <em>recommendation</em>
                  <em>using</em> <em>adversarial</em> <em>training</em>. <em>In</em> <em>the</em> <em>adversarial</em>
                  <em>process</em>, <em>we</em> <em>train</em> <em>a</em> <em>generator</em> <em>as</em> <em>an</em> <em>agent</em> <em>of</em>
                  <em>reinforcement</em> <em>learning</em> <em>which</em> <em>recommends</em> <em>the</em> <em>next</em>
                  <em>movie</em> <em>to</em> <em>a</em> <em>user</em> <em>sequentially</em>. <em>We</em> <em>also</em> <em>train</em> <em>a</em>
                  <em>discriminator</em> <em>which</em> <em>attempts</em> <em>to</em> <em>distinguish</em> <em>the</em>
                  <em>generated</em> <em>list</em> <em>of</em> <em>movies</em> <em>from</em> <em>the</em> <em>real</em> <em>records</em>. <em>The</em>
                  <em>poster</em> <em>information</em> <em>of</em> <em>movies</em> <em>is</em> <em>integrated</em> <em>to</em>
                  <em>further</em> <em>improve</em> <em>the</em> <em>performance</em> <em>of</em> <em>movie</em>
                  <em>recommendation</em>, <em>which</em> <em>is</em> <em>specifically</em> <em>essential</em> <em>when</em>
                  <em>few</em> <em>ratings</em> <em>are</em> <em>available</em>. <em>The</em> <em>experiments</em>
                  <em>demonstrate</em> <em>that</em> <em>the</em> <em>proposed</em> <em>model</em> <em>has</em> <em>robust</em>
                  <em>superiority</em> <em>over</em> <em>competitors</em> <em>and</em> <em>sets</em> <em>the</em>
                  <em>state</em>-<em>of</em>-<em>the</em>-<em>art</em>. <em>We</em> <em>will</em> <em>release</em> <em>the</em> <em>source</em> <em>code</em> <em>of</em>
                  <em>this</em> <em>work</em> <em>after</em> <em>publication</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="maturana-2017-docum-spann">74</a>]
</td>
<td class="bibtexitem">
<em>Francisco</em> <em>Maturana</em>, <em>Cristian</em> <em>Riveros</em>, <em>and</em> <em>Domagoj</em> <em>Vrgo</em>&#x010D;.
 <em>Document</em> <em>spanners</em> <em>for</em> <em>extracting</em> <em>incomplete</em> <em>information</em>:
  <em>Expressiveness</em> <em>and</em> <em>complexity</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#maturana-2017-docum-spann">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.00827">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1707.00827v2">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Rule</em>-<em>based</em> <em>information</em> <em>extraction</em> <em>has</em> <em>lately</em>
                  <em>received</em> <em>a</em> <em>fair</em> <em>amount</em> <em>of</em> <em>attention</em> <em>from</em> <em>the</em>
                  <em>database</em> <em>community</em>, <em>with</em> <em>several</em> <em>languages</em> <em>appearing</em>
                  <em>in</em> <em>the</em> <em>last</em> <em>few</em> <em>years</em>. <em>Although</em> <em>information</em>
                  <em>extraction</em> <em>systems</em> <em>are</em> <em>intended</em> <em>to</em> <em>deal</em> <em>with</em>
                  <em>semistructured</em> <em>data</em>, <em>all</em> <em>language</em> <em>proposals</em>
                  <em>introduced</em> <em>so</em> <em>far</em> <em>are</em> <em>designed</em> <em>to</em> <em>output</em> <em>relations</em>,
                  <em>thus</em> <em>making</em> <em>them</em> <em>incapable</em> <em>of</em> <em>handling</em> <em>incomplete</em>
                  <em>information</em>. <em>To</em> <em>remedy</em> <em>the</em> <em>situation</em>, <em>we</em> <em>propose</em> <em>to</em>
                  <em>extend</em> <em>information</em> <em>extraction</em> <em>languages</em> <em>with</em> <em>the</em>
                  <em>ability</em> <em>to</em> <em>use</em> <em>mappings</em>, <em>thus</em> <em>allowing</em> <em>us</em> <em>to</em> <em>work</em>
                  <em>with</em> <em>documents</em> <em>which</em> <em>have</em> <em>missing</em> <em>or</em> <em>optional</em> <em>parts</em>.
                  <em>Using</em> <em>this</em> <em>approach</em>, <em>we</em> <em>simplify</em> <em>the</em> <em>semantics</em> <em>of</em>
                  <em>regex</em> <em>formulas</em> <em>and</em> <em>extraction</em> <em>rules</em>, <em>two</em> <em>previously</em>
                  <em>defined</em> <em>methods</em> <em>for</em> <em>extracting</em> <em>information</em>, <em>extend</em>
                  <em>them</em> <em>with</em> <em>the</em> <em>ability</em> <em>to</em> <em>handle</em> <em>incomplete</em> <em>data</em>, <em>and</em>
                  <em>study</em> <em>how</em> <em>they</em> <em>compare</em> <em>in</em> <em>terms</em> <em>of</em> <em>expressive</em> <em>power</em>.
                  <em>We</em> <em>also</em> <em>study</em> <em>computational</em> <em>properties</em> <em>of</em> <em>these</em>
                  <em>languages</em>, <em>focusing</em> <em>on</em> <em>the</em> <em>query</em> <em>enumeration</em>
                  <em>problem</em>, <em>as</em> <em>well</em> <em>as</em> <em>satisfiability</em> <em>and</em> <em>containment</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lehman-2017-safe-mutat">75</a>]
</td>
<td class="bibtexitem">
<em>Joel</em> <em>Lehman</em>, <em>Jay</em> <em>Chen</em>, <em>Jeff</em> <em>Clune</em>, <em>and</em> <em>Kenneth</em>&nbsp;<em>O</em>. <em>Stanley</em>.
 <em>Safe</em> <em>mutations</em> <em>for</em> <em>deep</em> <em>and</em> <em>recurrent</em> <em>neural</em> <em>networks</em> <em>through</em> <em>output</em>
  <em>gradients</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#lehman-2017-safe-mutat">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.06563">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.06563v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>While</em> <em>neuroevolution</em> (<em>evolving</em> <em>neural</em> <em>networks</em>) <em>has</em>
                  <em>a</em> <em>successful</em> <em>track</em> <em>record</em> <em>across</em> <em>a</em> <em>variety</em> <em>of</em>
                  <em>domains</em> <em>from</em> <em>reinforcement</em> <em>learning</em> <em>to</em> <em>artificial</em>
                  <em>life</em>, <em>it</em> <em>is</em> <em>rarely</em> <em>applied</em> <em>to</em> <em>large</em>, <em>deep</em> <em>neural</em>
                  <em>networks</em>. <em>A</em> <em>central</em> <em>reason</em> <em>is</em> <em>that</em> <em>while</em> <em>random</em>
                  <em>mutation</em> <em>generally</em> <em>works</em> <em>in</em> <em>low</em> <em>dimensions</em>, <em>a</em> <em>random</em>
                  <em>perturbation</em> <em>of</em> <em>thousands</em> <em>or</em> <em>millions</em> <em>of</em> <em>weights</em> <em>is</em>
                  <em>likely</em> <em>to</em> <em>break</em> <em>existing</em> <em>functionality</em>, <em>providing</em> <em>no</em>
                  <em>learning</em> <em>signal</em> <em>even</em> <em>if</em> <em>some</em> <em>individual</em> <em>weight</em>
                  <em>changes</em> <em>were</em> <em>beneficial</em>. <em>This</em> <em>paper</em> <em>proposes</em> <em>a</em>
                  <em>solution</em> <em>by</em> <em>introducing</em> <em>a</em> <em>family</em> <em>of</em> <em>safe</em> <em>mutation</em>
                  (<em>SM</em>) <em>operators</em> <em>that</em> <em>aim</em> <em>within</em> <em>the</em> <em>mutation</em> <em>operator</em>
                  <em>itself</em> <em>to</em> <em>find</em> <em>a</em> <em>degree</em> <em>of</em> <em>change</em> <em>that</em> <em>does</em> <em>not</em>
                  <em>alter</em> <em>network</em> <em>behavior</em> <em>too</em> <em>much</em>, <em>but</em> <em>still</em>
                  <em>facilitates</em> <em>exploration</em>. <em>Importantly</em>, <em>these</em> <em>SM</em>
                  <em>operators</em> <em>do</em> <em>not</em> <em>require</em> <em>any</em> <em>additional</em> <em>interactions</em>
                  <em>with</em> <em>the</em> <em>environment</em>. <em>The</em> <em>most</em> <em>effective</em> <em>SM</em> <em>variant</em>
                  <em>capitalizes</em> <em>on</em> <em>the</em> <em>intriguing</em> <em>opportunity</em> <em>to</em> <em>scale</em>
                  <em>the</em> <em>degree</em> <em>of</em> <em>mutation</em> <em>of</em> <em>each</em> <em>individual</em> <em>weight</em>
                  <em>according</em> <em>to</em> <em>the</em> <em>sensitivity</em> <em>of</em> <em>the</em> <em>network</em>'<em>s</em>
                  <em>outputs</em> <em>to</em> <em>that</em> <em>weight</em>, <em>which</em> <em>requires</em> <em>computing</em> <em>the</em>
                  <em>gradient</em> <em>of</em> <em>outputs</em> <em>with</em> <em>respect</em> <em>to</em> <em>the</em> <em>weights</em>
                  (<em>instead</em> <em>of</em> <em>the</em> <em>gradient</em> <em>of</em> <em>error</em>, <em>as</em> <em>in</em>
                  <em>conventional</em> <em>deep</em> <em>learning</em>). <em>This</em> <em>safe</em> <em>mutation</em>
                  <em>through</em> <em>gradients</em> (<em>SM</em>-<em>G</em>) <em>operator</em> <em>dramatically</em>
                  <em>increases</em> <em>the</em> <em>ability</em> <em>of</em> <em>a</em> <em>simple</em> <em>genetic</em>
                  <em>algorithm</em>-<em>based</em> <em>neuroevolution</em> <em>method</em> <em>to</em> <em>find</em>
                  <em>solutions</em> <em>in</em> <em>high</em>-<em>dimensional</em> <em>domains</em> <em>that</em> <em>require</em>
                  <em>deep</em> <em>and</em>/<em>or</em> <em>recurrent</em> <em>neural</em> <em>networks</em> (<em>which</em> <em>tend</em> <em>to</em>
                  <em>be</em> <em>particularly</em> <em>brittle</em> <em>to</em> <em>mutation</em>), <em>including</em>
                  <em>domains</em> <em>that</em> <em>require</em> <em>processing</em> <em>raw</em> <em>pixels</em>. <em>By</em>
                  <em>improving</em> <em>our</em> <em>ability</em> <em>to</em> <em>evolve</em> <em>deep</em> <em>neural</em>
                  <em>networks</em>, <em>this</em> <em>new</em> <em>safer</em> <em>approach</em> <em>to</em> <em>mutation</em>
                  <em>expands</em> <em>the</em> <em>scope</em> <em>of</em> <em>domains</em> <em>amenable</em> <em>to</em>
                  <em>neuroevolution</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ghaffari-2017-improv-distr">76</a>]
</td>
<td class="bibtexitem">
<em>Mohsen</em> <em>Ghaffari</em> <em>and</em> <em>Jason</em> <em>Li</em>.
 <em>Improved</em> <em>distributed</em> <em>algorithms</em> <em>for</em> <em>exact</em> <em>shortest</em> <em>paths</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#ghaffari-2017-improv-distr">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09121">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09121v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Computing</em> <em>shortest</em> <em>paths</em> <em>is</em> <em>one</em> <em>of</em> <em>the</em> <em>central</em>
                  <em>problems</em> <em>in</em> <em>the</em> <em>theory</em> <em>of</em> <em>distributed</em> <em>computing</em>. <em>For</em>
                  <em>the</em> <em>last</em> <em>few</em> <em>years</em>, <em>substantial</em> <em>progress</em> <em>has</em> <em>been</em>
                  <em>made</em> <em>on</em> <em>the</em> <em>approximate</em> <em>single</em> <em>source</em> <em>shortest</em> <em>paths</em>
                  <em>problem</em>, <em>culminating</em> <em>in</em> <em>an</em> <em>algorithm</em> <em>of</em> <em>Henzinger</em>,
                  <em>Krinninger</em>, <em>and</em> <em>Nanongkai</em> [<em>STOC</em>'16] <em>which</em>
                  <em>deterministically</em> <em>computes</em> (1+o(1))-<em>approximate</em>
                  <em>shortest</em> <em>paths</em> <em>in</em> O(D+sqrt(n) <em>time</em>, <em>where</em>
                  D <em>is</em> <em>the</em> <em>hop</em>-<em>diameter</em> <em>of</em> <em>the</em> <em>graph</em>. <em>Up</em> <em>to</em>
                  <em>logarithmic</em> <em>factors</em>, <em>this</em> <em>time</em> <em>complexity</em> <em>is</em>
                  <em>optimal</em>, <em>matching</em> <em>the</em> <em>lower</em> <em>bound</em> <em>of</em> <em>Das</em> <em>Sarma</em> <em>et</em>
                  <em>al</em>. [<em>STOC</em>'11]. <em>The</em> <em>question</em> <em>of</em> <em>exact</em> <em>shortest</em> <em>paths</em>
                  <em>however</em> <em>saw</em> <em>no</em> <em>algorithmic</em> <em>progress</em> <em>for</em> <em>decades</em>,
                  <em>until</em> <em>the</em> <em>recent</em> <em>breakthrough</em> <em>of</em> <em>Elkin</em> [<em>STOC</em>'17],
                  <em>which</em> <em>established</em> <em>a</em> <em>sublinear</em>-<em>time</em> <em>algorithm</em> <em>for</em>
                  <em>exact</em> <em>single</em> <em>source</em> <em>shortest</em> <em>paths</em> <em>on</em> <em>undirected</em>
                  <em>graphs</em>. <em>Shortly</em> <em>after</em>, <em>Huang</em> <em>et</em> <em>al</em>. [<em>FOCS</em>'17]
                  <em>provided</em> <em>improved</em> <em>algorithms</em> <em>for</em> <em>exact</em> <em>all</em> <em>pairs</em>
                  <em>shortest</em> <em>paths</em> <em>problem</em> <em>on</em> <em>directed</em> <em>graphs</em>. <em>In</em> <em>this</em>
                  <em>paper</em>, <em>we</em> <em>present</em> <em>a</em> <em>new</em> <em>single</em>-<em>source</em> <em>shortest</em> <em>path</em>
                  <em>algorithm</em> <em>with</em> <em>complexity</em> 
                  O(n^3/4D^1/4). <em>For</em> <em>polylogarithmic</em> D, <em>this</em>
                  <em>improves</em> <em>on</em> <em>Elkin</em>'<em>s</em> O(n^5/6) <em>bound</em> <em>and</em>
                  <em>gets</em> <em>closer</em> <em>to</em> <em>the</em> &Omega;(n^1/2) <em>lower</em>
                  <em>bound</em> <em>of</em> <em>Peleg</em> <em>and</em> <em>Rubinovich</em> [<em>FOCS</em>'99]. <em>For</em> <em>larger</em>
                  <em>values</em> <em>of</em> D, <em>we</em> <em>present</em> <em>an</em> <em>improved</em> <em>variant</em> <em>of</em> <em>our</em>
                  <em>algorithm</em> <em>which</em> <em>achieves</em> <em>complexity</em> O(
                  n^3/4+o(1)+ min{
                  n^3/4D^1/6,n^6/7}+D), <em>and</em> <em>thus</em>
                  <em>compares</em> <em>favorably</em> <em>with</em> <em>Elkin</em>'<em>s</em> <em>bound</em> <em>of</em>
                  O(n^5/6 + n^2/3D^1/3 + D )  <em>in</em>
                  <em>essentially</em> <em>the</em> <em>entire</em> <em>range</em> <em>of</em> <em>parameters</em>. <em>This</em>
                  <em>algorithm</em> <em>provides</em> <em>also</em> <em>a</em> <em>qualitative</em> <em>improvement</em>,
                  <em>because</em> <em>it</em> <em>works</em> <em>for</em> <em>the</em> <em>more</em> <em>challenging</em> <em>case</em> <em>of</em>
                  <em>directed</em> <em>graphs</em> (<em>i</em>.<em>e</em>., <em>graphs</em> <em>where</em> <em>the</em> <em>two</em>
                  <em>directions</em> <em>of</em> <em>an</em> <em>edge</em> <em>can</em> <em>have</em> <em>different</em> <em>weights</em>),
                  <em>constituting</em> <em>the</em> <em>first</em> <em>sublinear</em>-<em>time</em> <em>algorithm</em> <em>for</em>
                  <em>directed</em> <em>graphs</em>. <em>Our</em> <em>algorithm</em> <em>also</em> <em>extends</em> <em>to</em> <em>the</em>
                  <em>case</em> <em>of</em> <em>exact</em> &kappa;-<em>source</em> <em>shortest</em> <em>paths</em>...)
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="andrychowicz-2016-learn-to">77</a>]
</td>
<td class="bibtexitem">
<em>Marcin</em> <em>Andrychowicz</em>, <em>Misha</em> <em>Denil</em>, <em>Sergio</em> <em>Gomez</em>, <em>Matthew</em>&nbsp;<em>W</em>. <em>Hoffman</em>, <em>David</em> <em>Pfau</em>,
  <em>Tom</em> <em>Schaul</em>, <em>Brendan</em> <em>Shillingford</em>, <em>and</em> <em>Nando</em>&nbsp;<em>de</em> <em>Freitas</em>.
 <em>Learning</em> <em>to</em> <em>learn</em> <em>by</em> <em>gradient</em> <em>descent</em> <em>by</em> <em>gradient</em> <em>descent</em>.
 <em>CoRR</em>, 2016.
[&nbsp;<a href="index_bib.html#andrychowicz-2016-learn-to">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1606.04474v2">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1606.04474v2">http</a>&nbsp;]
<blockquote><font size="-1">
<em>The</em> <em>move</em> <em>from</em> <em>hand</em>-<em>designed</em> <em>features</em> <em>to</em> <em>learned</em>
                  <em>features</em> <em>in</em> <em>machine</em> <em>learning</em> <em>has</em> <em>been</em> <em>wildly</em>
                  <em>successful</em>. <em>In</em> <em>spite</em> <em>of</em> <em>this</em>, <em>optimization</em>
                  <em>algorithms</em> <em>are</em> <em>still</em> <em>designed</em> <em>by</em> <em>hand</em>. <em>In</em> <em>this</em> <em>paper</em>
                  <em>we</em> <em>show</em> <em>how</em> <em>the</em> <em>design</em> <em>of</em> <em>an</em> <em>optimization</em> <em>algorithm</em>
                  <em>can</em> <em>be</em> <em>cast</em> <em>as</em> <em>a</em> <em>learning</em> <em>problem</em>, <em>allowing</em> <em>the</em>
                  <em>algorithm</em> <em>to</em> <em>learn</em> <em>to</em> <em>exploit</em> <em>structure</em> <em>in</em> <em>the</em>
                  <em>problems</em> <em>of</em> <em>interest</em> <em>in</em> <em>an</em> <em>automatic</em> <em>way</em>. <em>Our</em>
                  <em>learned</em> <em>algorithms</em>, <em>implemented</em> <em>by</em> <em>LSTMs</em>, <em>outperform</em>
                  <em>generic</em>, <em>hand</em>-<em>designed</em> <em>competitors</em> <em>on</em> <em>the</em> <em>tasks</em> <em>for</em>
                  <em>which</em> <em>they</em> <em>are</em> <em>trained</em>, <em>and</em> <em>also</em> <em>generalize</em> <em>well</em> <em>to</em>
                  <em>new</em> <em>tasks</em> <em>with</em> <em>similar</em> <em>structure</em>. <em>We</em> <em>demonstrate</em>
                  <em>this</em> <em>on</em> <em>a</em> <em>number</em> <em>of</em> <em>tasks</em>, <em>including</em> <em>simple</em> <em>convex</em>
                  <em>problems</em>, <em>training</em> <em>neural</em> <em>networks</em>, <em>and</em> <em>styling</em>
                  <em>images</em> <em>with</em> <em>neural</em> <em>art</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="chen-2017-cnn-is">78</a>]
</td>
<td class="bibtexitem">
<em>Qiming</em> <em>Chen</em> <em>and</em> <em>Ren</em> <em>Wu</em>.
 <em>Cnn</em> <em>is</em> <em>all</em> <em>you</em> <em>need</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#chen-2017-cnn-is">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09662v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09662v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>The</em> <em>Convolution</em> <em>Neural</em> <em>Network</em> (<em>CNN</em>) <em>has</em>
                  <em>demonstrated</em> <em>the</em> <em>unique</em> <em>advantage</em> <em>in</em> <em>audio</em>, <em>image</em>
                  <em>and</em> <em>text</em> <em>learning</em>; <em>recently</em> <em>it</em> <em>has</em> <em>also</em> <em>challenged</em>
                  <em>Recurrent</em> <em>Neural</em> <em>Networks</em> (<em>RNNs</em>) <em>with</em> <em>long</em>
                  <em>short</em>-<em>term</em> <em>memory</em> <em>cells</em> (<em>LSTM</em>) <em>in</em>
                  <em>sequence</em>-<em>to</em>-<em>sequence</em> <em>learning</em>, <em>since</em> <em>the</em>
                  <em>computations</em> <em>involved</em> <em>in</em> <em>CNN</em> <em>are</em> <em>easily</em>
                  <em>parallelizable</em> <em>whereas</em> <em>those</em> <em>involved</em> <em>in</em> <em>RNN</em> <em>are</em>
                  <em>mostly</em> <em>sequential</em>, <em>leading</em> <em>to</em> <em>a</em> <em>performance</em>
                  <em>bottleneck</em>. <em>However</em>, <em>unlike</em> <em>RNN</em>, <em>the</em> <em>native</em> <em>CNN</em>
                  <em>lacks</em> <em>the</em> <em>history</em> <em>sensitivity</em> <em>required</em> <em>for</em> <em>sequence</em>
                  <em>transformation</em>; <em>therefore</em> <em>enhancing</em> <em>the</em> <em>sequential</em>
                  <em>order</em> <em>awareness</em>, <em>or</em> <em>position</em>-<em>sensitivity</em>, <em>becomes</em>
                  <em>the</em> <em>key</em> <em>to</em> <em>make</em> <em>CNN</em> <em>the</em> <em>general</em> <em>deep</em> <em>learning</em> <em>model</em>.
                  <em>In</em> <em>this</em> <em>work</em> <em>we</em> <em>introduce</em> <em>an</em> <em>extended</em> <em>CNN</em> <em>model</em> <em>with</em>
                  <em>strengthen</em> <em>position</em>-<em>sensitivity</em>, <em>called</em> <em>PoseNet</em>. <em>A</em>
                  <em>notable</em> <em>feature</em> <em>of</em> <em>PoseNet</em> <em>is</em> <em>the</em> <em>asymmetric</em>
                  <em>treatment</em> <em>of</em> <em>position</em> <em>information</em> <em>in</em> <em>the</em> <em>encoder</em> <em>and</em>
                  <em>the</em> <em>decoder</em>. <em>Experiments</em> <em>shows</em> <em>that</em> <em>PoseNet</em> <em>allows</em>
                  <em>us</em> <em>to</em> <em>improve</em> <em>the</em> <em>accuracy</em> <em>of</em> <em>CNN</em> <em>based</em>
                  <em>sequence</em>-<em>to</em>-<em>sequence</em> <em>learning</em> <em>significantly</em>,
                  <em>achieving</em> <em>around</em> 33-36 <em>BLEU</em> <em>scores</em> <em>on</em> <em>the</em> <em>WMT</em> 2014
                  <em>English</em>-<em>to</em>-<em>German</em> <em>translation</em> <em>task</em>, <em>and</em> <em>around</em> 44-46
                  <em>BLEU</em> <em>scores</em> <em>on</em> <em>the</em> <em>English</em>-<em>to</em>-<em>French</em> <em>translation</em>
                  <em>task</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="grewal-2017-chall-detec">79</a>]
</td>
<td class="bibtexitem">
<em>Karan</em> <em>Grewal</em> <em>and</em> <em>Khai</em>&nbsp;<em>N</em>. <em>Truong</em>.
 <em>On</em> <em>the</em> <em>challenges</em> <em>of</em> <em>detecting</em> <em>rude</em> <em>conversational</em> <em>behaviour</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#grewal-2017-chall-detec">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09929v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09929v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>In</em> <em>this</em> <em>study</em>, <em>we</em> <em>aim</em> <em>to</em> <em>identify</em> <em>moments</em> <em>of</em>
                  <em>rudeness</em> <em>between</em> <em>two</em> <em>individuals</em>. <em>In</em> <em>particular</em>, <em>we</em>
                  <em>segment</em> <em>all</em> <em>occurrences</em> <em>of</em> <em>rudeness</em> <em>in</em> <em>conversations</em>
                  <em>into</em> <em>three</em> <em>broad</em>, <em>distinct</em> <em>categories</em> <em>and</em> <em>try</em> <em>to</em>
                  <em>identify</em> <em>each</em>. <em>We</em> <em>show</em> <em>how</em> <em>machine</em> <em>learning</em>
                  <em>algorithms</em> <em>can</em> <em>be</em> <em>used</em> <em>to</em> <em>identify</em> <em>rudeness</em> <em>based</em> <em>on</em>
                  <em>acoustic</em> <em>and</em> <em>semantic</em> <em>signals</em> <em>extracted</em> <em>from</em>
                  <em>conversations</em>. <em>Furthermore</em>, <em>we</em> <em>make</em> <em>note</em> <em>of</em> <em>our</em>
                  <em>shortcomings</em> <em>in</em> <em>this</em> <em>task</em> <em>and</em> <em>highlight</em> <em>what</em> <em>makes</em>
                  <em>this</em> <em>problem</em> <em>inherently</em> <em>difficult</em>. <em>Finally</em>, <em>we</em>
                  <em>provide</em> <em>next</em> <em>steps</em> <em>which</em> <em>are</em> <em>needed</em> <em>to</em> <em>ensure</em>
                  <em>further</em> <em>success</em> <em>in</em> <em>identifying</em> <em>rudeness</em> <em>in</em>
                  <em>conversations</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="ghosh-2017-robus-loss">80</a>]
</td>
<td class="bibtexitem">
<em>Aritra</em> <em>Ghosh</em>, <em>Himanshu</em> <em>Kumar</em>, <em>and</em> <em>P</em>.&nbsp;<em>S</em>. <em>Sastry</em>.
 <em>Robust</em> <em>loss</em> <em>functions</em> <em>under</em> <em>label</em> <em>noise</em> <em>for</em> <em>deep</em> <em>neural</em> <em>networks</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#ghosh-2017-robus-loss">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09482v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09482v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>In</em> <em>many</em> <em>applications</em> <em>of</em> <em>classifier</em> <em>learning</em>,
                  <em>training</em> <em>data</em> <em>suffers</em> <em>from</em> <em>label</em> <em>noise</em>. <em>Deep</em>
                  <em>networks</em> <em>are</em> <em>learned</em> <em>using</em> <em>huge</em> <em>training</em> <em>data</em> <em>where</em>
                  <em>the</em> <em>problem</em> <em>of</em> <em>noisy</em> <em>labels</em> <em>is</em> <em>particularly</em>
                  <em>relevant</em>. <em>The</em> <em>current</em> <em>techniques</em> <em>proposed</em> <em>for</em>
                  <em>learning</em> <em>deep</em> <em>networks</em> <em>under</em> <em>label</em> <em>noise</em> <em>focus</em> <em>on</em>
                  <em>modifying</em> <em>the</em> <em>network</em> <em>architecture</em> <em>and</em> <em>on</em> <em>algorithms</em>
                  <em>for</em> <em>estimating</em> <em>true</em> <em>labels</em> <em>from</em> <em>noisy</em> <em>labels</em>. <em>An</em>
                  <em>alternate</em> <em>approach</em> <em>would</em> <em>be</em> <em>to</em> <em>look</em> <em>for</em> <em>loss</em>
                  <em>functions</em> <em>that</em> <em>are</em> <em>inherently</em> <em>noise</em>-<em>tolerant</em>. <em>For</em>
                  <em>binary</em> <em>classification</em> <em>there</em> <em>exist</em> <em>theoretical</em>
                  <em>results</em> <em>on</em> <em>loss</em> <em>functions</em> <em>that</em> <em>are</em> <em>robust</em> <em>to</em> <em>label</em>
                  <em>noise</em>. <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>provide</em> <em>some</em> <em>sufficient</em>
                  <em>conditions</em> <em>on</em> <em>a</em> <em>loss</em> <em>function</em> <em>so</em> <em>that</em> <em>risk</em>
                  <em>minimization</em> <em>under</em> <em>that</em> <em>loss</em> <em>function</em> <em>would</em> <em>be</em>
                  <em>inherently</em> <em>tolerant</em> <em>to</em> <em>label</em> <em>noise</em> <em>for</em> <em>multiclass</em>
                  <em>classification</em> <em>problems</em>. <em>These</em> <em>results</em> <em>generalize</em>
                  <em>the</em> <em>existing</em> <em>results</em> <em>on</em> <em>noise</em>-<em>tolerant</em> <em>loss</em>
                  <em>functions</em> <em>for</em> <em>binary</em> <em>classification</em>. <em>We</em> <em>study</em> <em>some</em>
                  <em>of</em> <em>the</em> <em>widely</em> <em>used</em> <em>loss</em> <em>functions</em> <em>in</em> <em>deep</em> <em>networks</em>
                  <em>and</em> <em>show</em> <em>that</em> <em>the</em> <em>loss</em> <em>function</em> <em>based</em> <em>on</em> <em>mean</em>
                  <em>absolute</em> <em>value</em> <em>of</em> <em>error</em> <em>is</em> <em>inherently</em> <em>robust</em> <em>to</em>
                  <em>label</em> <em>noise</em>. <em>Thus</em> <em>standard</em> <em>back</em> <em>propagation</em> <em>is</em>
                  <em>enough</em> <em>to</em> <em>learn</em> <em>the</em> <em>true</em> <em>classifier</em> <em>even</em> <em>under</em> <em>label</em>
                  <em>noise</em>. <em>Through</em> <em>experiments</em>, <em>we</em> <em>illustrate</em> <em>the</em>
                  <em>robustness</em> <em>of</em> <em>risk</em> <em>minimization</em> <em>with</em> <em>such</em> <em>loss</em>
                  <em>functions</em> <em>for</em> <em>learning</em> <em>neural</em> <em>networks</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="lampinen-2017-one-shot">81</a>]
</td>
<td class="bibtexitem">
<em>Andrew</em>&nbsp;<em>K</em>. <em>Lampinen</em> <em>and</em> <em>James</em>&nbsp;<em>L</em>. <em>McClelland</em>.
 <em>One</em>-<em>shot</em> <em>and</em> <em>few</em>-<em>shot</em> <em>learning</em> <em>of</em> <em>word</em> <em>embeddings</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#lampinen-2017-one-shot">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.10280">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.10280v2">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Standard</em> <em>deep</em> <em>learning</em> <em>systems</em> <em>require</em> <em>thousands</em> <em>or</em>
                  <em>millions</em> <em>of</em> <em>examples</em> <em>to</em> <em>learn</em> <em>a</em> <em>concept</em>, <em>and</em> <em>cannot</em>
                  <em>integrate</em> <em>new</em> <em>concepts</em> <em>easily</em>. <em>By</em> <em>contrast</em>, <em>humans</em>
                  <em>have</em> <em>an</em> <em>incredible</em> <em>ability</em> <em>to</em> <em>do</em> <em>one</em>-<em>shot</em> <em>or</em>
                  <em>few</em>-<em>shot</em> <em>learning</em>. <em>For</em> <em>instance</em>, <em>from</em> <em>just</em> <em>hearing</em> <em>a</em>
                  <em>word</em> <em>used</em> <em>in</em> <em>a</em> <em>sentence</em>, <em>humans</em> <em>can</em> <em>infer</em> <em>a</em> <em>great</em>
                  <em>deal</em> <em>about</em> <em>it</em>, <em>by</em> <em>leveraging</em> <em>what</em> <em>the</em> <em>syntax</em> <em>and</em>
                  <em>semantics</em> <em>of</em> <em>the</em> <em>surrounding</em> <em>words</em> <em>tells</em> <em>us</em>. <em>Here</em>,
                  <em>we</em> <em>draw</em> <em>inspiration</em> <em>from</em> <em>this</em> <em>to</em> <em>highlight</em> <em>a</em> <em>simple</em>
                  <em>technique</em> <em>by</em> <em>which</em> <em>deep</em> <em>recurrent</em> <em>networks</em> <em>can</em>
                  <em>similarly</em> <em>exploit</em> <em>their</em> <em>prior</em> <em>knowledge</em> <em>to</em> <em>learn</em> <em>a</em>
                  <em>useful</em> <em>representation</em> <em>for</em> <em>a</em> <em>new</em> <em>word</em> <em>from</em> <em>little</em>
                  <em>data</em>. <em>This</em> <em>could</em> <em>make</em> <em>natural</em> <em>language</em> <em>processing</em>
                  <em>systems</em> <em>much</em> <em>more</em> <em>flexible</em>, <em>by</em> <em>allowing</em> <em>them</em> <em>to</em>
                  <em>learn</em> <em>continually</em> <em>from</em> <em>the</em> <em>new</em> <em>words</em> <em>they</em>
                  <em>encounter</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="shalaby-2018-beyon-word-embed">82</a>]
</td>
<td class="bibtexitem">
<em>Walid</em> <em>Shalaby</em>, <em>Wlodek</em> <em>Zadrozny</em>, <em>and</em> <em>Hongxia</em> <em>Jin</em>.
 <em>Beyond</em> <em>word</em> <em>embeddings</em>: <em>Learning</em> <em>entity</em> <em>and</em> <em>concept</em> <em>representations</em>
  <em>from</em> <em>large</em> <em>scale</em> <em>knowledge</em> <em>bases</em>.
 <em>CoRR</em>, 2018.
[&nbsp;<a href="index_bib.html#shalaby-2018-beyon-word-embed">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00388">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00388v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Text</em> <em>representation</em> <em>using</em> <em>neural</em> <em>word</em> <em>embeddings</em> <em>has</em>
                  <em>proven</em> <em>efficacy</em> <em>in</em> <em>many</em> <em>NLP</em> <em>applications</em>. <em>Recently</em>,
                  <em>a</em> <em>lot</em> <em>of</em> <em>research</em> <em>interest</em> <em>goes</em> <em>beyond</em> <em>word</em>
                  <em>embeddings</em> <em>by</em> <em>adapting</em> <em>the</em> <em>traditional</em> <em>word</em>
                  <em>embedding</em> <em>models</em> <em>to</em> <em>learn</em> <em>vectors</em> <em>of</em> <em>multiword</em>
                  <em>expressions</em> (<em>concepts</em>/<em>entities</em>). <em>However</em>, <em>current</em>
                  <em>methods</em> <em>are</em> <em>limited</em> <em>to</em> <em>textual</em> <em>knowledge</em> <em>bases</em> <em>only</em>
                  (<em>e</em>.<em>g</em>., <em>Wikipedia</em>). <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>propose</em> <em>a</em> <em>novel</em>
                  <em>approach</em> <em>for</em> <em>learning</em> <em>concept</em> <em>vectors</em> <em>from</em> <em>two</em> <em>large</em>
                  <em>scale</em> <em>knowledge</em> <em>bases</em> (<em>Wikipedia</em>, <em>and</em> <em>Probase</em>). <em>We</em>
                  <em>adapt</em> <em>the</em> <em>skip</em>-<em>gram</em> <em>model</em> <em>to</em> <em>seamlessly</em> <em>learn</em> <em>from</em>
                  <em>the</em> <em>knowledge</em> <em>in</em> <em>Wikipedia</em> <em>text</em> <em>and</em> <em>Probase</em> <em>concept</em>
                  <em>graph</em>. <em>We</em> <em>evaluate</em> <em>our</em> <em>concept</em> <em>embedding</em> <em>models</em>
                  <em>intrinsically</em> <em>on</em> <em>two</em> <em>tasks</em>: 1) <em>analogical</em> <em>reasoning</em>
                  <em>where</em> <em>we</em> <em>achieve</em> <em>a</em> <em>state</em>-<em>of</em>-<em>the</em>-<em>art</em> <em>performance</em> <em>of</em>
                  91 % <em>on</em> <em>semantic</em> <em>analogies</em>, 2) <em>concept</em>
                  <em>categorization</em> <em>where</em> <em>we</em> <em>achieve</em> <em>a</em> <em>state</em>-<em>of</em>-<em>the</em>-<em>art</em>
                  <em>performance</em> <em>on</em> <em>two</em> <em>benchmark</em> <em>datasets</em> <em>achieving</em>
                  <em>categorization</em> <em>accuracy</em> <em>of</em> 100 % <em>on</em> <em>one</em> <em>and</em> 98 %
                  <em>on</em> <em>the</em> <em>other</em>. <em>Additionally</em>, <em>we</em> <em>present</em> <em>a</em> <em>case</em> <em>study</em>
                  <em>to</em> <em>extrinsically</em> <em>evaluate</em> <em>our</em> <em>model</em> <em>on</em> <em>unsupervised</em>
                  <em>argument</em> <em>type</em> <em>identification</em> <em>for</em> <em>neural</em> <em>semantic</em>
                  <em>parsing</em>. <em>We</em> <em>demonstrate</em> <em>the</em> <em>competitive</em> <em>accuracy</em> <em>of</em>
                  <em>our</em> <em>unsupervised</em> <em>method</em> <em>and</em> <em>its</em> <em>ability</em> <em>to</em> <em>better</em>
                  <em>generalize</em> <em>to</em> <em>out</em> <em>of</em> <em>vocabulary</em> <em>entity</em> <em>mentions</em>
                  <em>compared</em> <em>to</em> <em>the</em> <em>tedious</em> <em>and</em> <em>error</em> <em>prone</em> <em>methods</em>
                  <em>which</em> <em>depend</em> <em>on</em> <em>gazetteers</em> <em>and</em> <em>regular</em> <em>expressions</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="poggio-2017-theor-deep">83</a>]
</td>
<td class="bibtexitem">
<em>Tomaso</em> <em>Poggio</em>, <em>Kenji</em> <em>Kawaguchi</em>, <em>Qianli</em> <em>Liao</em>, <em>Brando</em> <em>Miranda</em>, <em>Lorenzo</em> <em>Rosasco</em>,
  <em>Xavier</em> <em>Boix</em>, <em>Jack</em> <em>Hidary</em>, <em>and</em> <em>Hrushikesh</em> <em>Mhaskar</em>.
 <em>Theory</em> <em>of</em> <em>deep</em> <em>learning</em> <em>iii</em>: <em>Explaining</em> <em>the</em> <em>non</em>-<em>overfitting</em> <em>puzzle</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#poggio-2017-theor-deep">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00173">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00173v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>A</em> <em>main</em> <em>puzzle</em> <em>of</em> <em>deep</em> <em>networks</em> <em>revolves</em> <em>around</em> <em>the</em>
                  <em>absence</em> <em>of</em> <em>overfitting</em> <em>despite</em> <em>overparametrization</em>
                  <em>and</em> <em>despite</em> <em>the</em> <em>large</em> <em>capacity</em> <em>demonstrated</em> <em>by</em> <em>zero</em>
                  <em>training</em> <em>error</em> <em>on</em> <em>randomly</em> <em>labeled</em> <em>data</em>. <em>In</em> <em>this</em>
                  <em>note</em>, <em>we</em> <em>show</em> <em>that</em> <em>the</em> <em>dynamical</em> <em>systems</em> <em>associated</em>
                  <em>with</em> <em>gradient</em> <em>descent</em> <em>minimization</em> <em>of</em> <em>nonlinear</em>
                  <em>networks</em> <em>behave</em> <em>near</em> <em>zero</em> <em>stable</em> <em>minima</em> <em>of</em> <em>the</em>
                  <em>empirical</em> <em>error</em> <em>as</em> <em>gradient</em> <em>system</em> <em>in</em> <em>a</em> <em>quadratic</em>
                  <em>potential</em> <em>with</em> <em>degenerate</em> <em>Hessian</em>. <em>The</em> <em>proposition</em>
                  <em>is</em> <em>supported</em> <em>by</em> <em>theoretical</em> <em>and</em> <em>numerical</em> <em>results</em>,
                  <em>under</em> <em>the</em> <em>assumption</em> <em>of</em> <em>stable</em> <em>minima</em> <em>of</em> <em>the</em>
                  <em>gradient</em>. <em>Our</em> <em>proposition</em> <em>provides</em> <em>the</em> <em>extension</em> <em>to</em>
                  <em>deep</em> <em>networks</em> <em>of</em> <em>key</em> <em>properties</em> <em>of</em> <em>gradient</em> <em>descent</em>
                  <em>methods</em> <em>for</em> <em>linear</em> <em>networks</em>, <em>that</em> <em>as</em>, <em>suggested</em> <em>in</em>
                  (1), <em>can</em> <em>be</em> <em>the</em> <em>key</em> <em>to</em> <em>understand</em> <em>generalization</em>.
                  <em>Gradient</em> <em>descent</em> <em>enforces</em> <em>a</em> <em>form</em> <em>of</em> <em>implicit</em>
                  <em>regularization</em> <em>controlled</em> <em>by</em> <em>the</em> <em>number</em> <em>of</em>
                  <em>iterations</em>, <em>and</em> <em>asymptotically</em> <em>converging</em> <em>to</em> <em>the</em>
                  <em>minimum</em> <em>norm</em> <em>solution</em>. <em>This</em> <em>implies</em> <em>that</em> <em>there</em> <em>is</em>
                  <em>usually</em> <em>an</em> <em>optimum</em> <em>early</em> <em>stopping</em> <em>that</em> <em>avoids</em>
                  <em>overfitting</em> <em>of</em> <em>the</em> <em>loss</em> (<em>this</em> <em>is</em> <em>relevant</em> <em>mainly</em> <em>for</em>
                  <em>regression</em>). <em>For</em> <em>classification</em>, <em>the</em> <em>asymptotic</em>
                  <em>convergence</em> <em>to</em> <em>the</em> <em>minimum</em> <em>norm</em> <em>solution</em> <em>implies</em>
                  <em>convergence</em> <em>to</em> <em>the</em> <em>maximum</em> <em>margin</em> <em>solution</em> <em>which</em>
                  <em>guarantees</em> <em>good</em> <em>classification</em> <em>error</em> <em>for</em> "<em>low</em> <em>noise</em>"
                  <em>datasets</em>. <em>The</em> <em>implied</em> <em>robustness</em> <em>to</em>
                  <em>overparametrization</em> <em>has</em> <em>suggestive</em> <em>implications</em> <em>for</em>
                  <em>the</em> <em>robustness</em> <em>of</em> <em>deep</em> <em>hierarchically</em> <em>local</em> <em>networks</em>
                  <em>to</em> <em>variations</em> <em>of</em> <em>the</em> <em>architecture</em> <em>with</em> <em>respect</em> <em>to</em>
                  <em>the</em> <em>curse</em> <em>of</em> <em>dimensionality</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="holzinger-2017-what-do">84</a>]
</td>
<td class="bibtexitem">
<em>Andreas</em> <em>Holzinger</em>, <em>Chris</em> <em>Biemann</em>, <em>Constantinos</em>&nbsp;<em>S</em>. <em>Pattichis</em>, <em>and</em> <em>Douglas</em>&nbsp;<em>B</em>.
  <em>Kell</em>.
 <em>What</em> <em>do</em> <em>we</em> <em>need</em> <em>to</em> <em>build</em> <em>explainable</em> <em>ai</em> <em>systems</em> <em>for</em> <em>the</em> <em>medical</em>
  <em>domain</em>?
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#holzinger-2017-what-do">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09923v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.09923v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Artificial</em> <em>intelligence</em> (<em>AI</em>) <em>generally</em> <em>and</em> <em>machine</em>
                  <em>learning</em> (<em>ML</em>) <em>specifically</em> <em>demonstrate</em> <em>impressive</em>
                  <em>practical</em> <em>success</em> <em>in</em> <em>many</em> <em>different</em> <em>application</em>
                  <em>domains</em>, <em>e</em>.<em>g</em>. <em>in</em> <em>autonomous</em> <em>driving</em>, <em>speech</em>
                  <em>recognition</em>, <em>or</em> <em>recommender</em> <em>systems</em>. <em>Deep</em> <em>learning</em>
                  <em>approaches</em>, <em>trained</em> <em>on</em> <em>extremely</em> <em>large</em> <em>data</em> <em>sets</em> <em>or</em>
                  <em>using</em> <em>reinforcement</em> <em>learning</em> <em>methods</em> <em>have</em> <em>even</em>
                  <em>exceeded</em> <em>human</em> <em>performance</em> <em>in</em> <em>visual</em> <em>tasks</em>,
                  <em>particularly</em> <em>on</em> <em>playing</em> <em>games</em> <em>such</em> <em>as</em> <em>Atari</em>, <em>or</em>
                  <em>mastering</em> <em>the</em> <em>game</em> <em>of</em> <em>Go</em>. <em>Even</em> <em>in</em> <em>the</em> <em>medical</em> <em>domain</em>
                  <em>there</em> <em>are</em> <em>remarkable</em> <em>results</em>. <em>The</em> <em>central</em> <em>problem</em> <em>of</em>
                  <em>such</em> <em>models</em> <em>is</em> <em>that</em> <em>they</em> <em>are</em> <em>regarded</em> <em>as</em> <em>black</em>-<em>box</em>
                  <em>models</em> <em>and</em> <em>even</em> <em>if</em> <em>we</em> <em>understand</em> <em>the</em> <em>underlying</em>
                  <em>mathematical</em> <em>principles</em>, <em>they</em> <em>lack</em> <em>an</em> <em>explicit</em>
                  <em>declarative</em> <em>knowledge</em> <em>representation</em>, <em>hence</em> <em>have</em>
                  <em>difficulty</em> <em>in</em> <em>generating</em> <em>the</em> <em>underlying</em> <em>explanatory</em>
                  <em>structures</em>. <em>This</em> <em>calls</em> <em>for</em> <em>systems</em> <em>enabling</em> <em>to</em> <em>make</em>
                  <em>decisions</em> <em>transparent</em>, <em>understandable</em> <em>and</em>
                  <em>explainable</em>. <em>A</em> <em>huge</em> <em>motivation</em> <em>for</em> <em>our</em> <em>approach</em> <em>are</em>
                  <em>rising</em> <em>legal</em> <em>and</em> <em>privacy</em> <em>aspects</em>. <em>The</em> <em>new</em> <em>European</em>
                  <em>General</em> <em>Data</em> <em>Protection</em> <em>Regulation</em> <em>entering</em> <em>into</em>
                  <em>force</em> <em>on</em> <em>May</em> 25<em>th</em> 2018, <em>will</em> <em>make</em> <em>black</em>-<em>box</em>
                  <em>approaches</em> <em>difficult</em> <em>to</em> <em>use</em> <em>in</em> <em>business</em>. <em>This</em> <em>does</em>
                  <em>not</em> <em>imply</em> <em>a</em> <em>ban</em> <em>on</em> <em>automatic</em> <em>learning</em> <em>approaches</em> <em>or</em>
                  <em>an</em> <em>obligation</em> <em>to</em> <em>explain</em> <em>everything</em> <em>all</em> <em>the</em> <em>time</em>,
                  <em>however</em>, <em>there</em> <em>must</em> <em>be</em> <em>a</em> <em>possibility</em> <em>to</em> <em>make</em> <em>the</em>
                  <em>results</em> <em>re</em>-<em>traceable</em> <em>on</em> <em>demand</em>. <em>In</em> <em>this</em> <em>paper</em> <em>we</em>
                  <em>outline</em> <em>some</em> <em>of</em> <em>our</em> <em>research</em> <em>topics</em> <em>in</em> <em>the</em> <em>context</em>
                  <em>of</em> <em>the</em> <em>relatively</em> <em>new</em> <em>area</em> <em>of</em> <em>explainable</em>-<em>AI</em> <em>with</em> <em>a</em>
                  <em>focus</em> <em>on</em> <em>the</em> <em>application</em> <em>in</em> <em>medicine</em>, <em>which</em> <em>is</em> <em>a</em>
                  <em>very</em> <em>special</em> <em>domain</em>. <em>This</em> <em>is</em> <em>due</em> <em>to</em> <em>the</em> <em>fact</em> <em>that</em>
                  <em>medical</em> <em>professionals</em> <em>are</em> <em>working</em> <em>mostly</em> <em>with</em>
                  <em>distributed</em> <em>heterogeneous</em> <em>and</em> <em>complex</em> <em>sources</em> <em>of</em>
                  <em>data</em>. <em>In</em> <em>this</em> <em>paper</em> <em>we</em> <em>concentrate</em> <em>on</em> <em>three</em> <em>sources</em>:
                  <em>images</em>, *<em>omics</em> <em>data</em> <em>and</em> <em>text</em>. <em>We</em> <em>argue</em> <em>that</em> <em>research</em>
                  <em>in</em> <em>explainable</em>-<em>AI</em> <em>would</em> <em>generally</em> <em>help</em> <em>to</em> <em>facilitate</em>
                  <em>the</em> <em>implementation</em> <em>of</em> <em>AI</em>/<em>ML</em> <em>in</em> <em>the</em> <em>medical</em> <em>domain</em>,
                  <em>and</em> <em>specifically</em> <em>help</em> <em>to</em> <em>facilitate</em> <em>transparency</em> <em>and</em>
                  <em>trust</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="schulze-2018-vizdoom">85</a>]
</td>
<td class="bibtexitem">
<em>Christopher</em> <em>Schulze</em> <em>and</em> <em>Marcus</em> <em>Schulze</em>.
 <em>Vizdoom</em>: <em>Drqn</em> <em>with</em> <em>prioritized</em> <em>experience</em> <em>replay</em>, <em>double</em>-<em>q</em> <em>learning</em>,
  & <em>snapshot</em> <em>ensembling</em>.
 <em>CoRR</em>, 2018.
[&nbsp;<a href="index_bib.html#schulze-2018-vizdoom">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.01000">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.01000v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>ViZDoom</em> <em>is</em> <em>a</em> <em>robust</em>, <em>first</em>-<em>person</em> <em>shooter</em>
                  <em>reinforcement</em> <em>learning</em> <em>environment</em>, <em>characterized</em> <em>by</em>
                  <em>a</em> <em>significant</em> <em>degree</em> <em>of</em> <em>latent</em> <em>state</em> <em>information</em>. <em>In</em>
                  <em>this</em> <em>paper</em>, <em>double</em>-<em>Q</em> <em>learning</em> <em>and</em> <em>prioritized</em>
                  <em>experience</em> <em>replay</em> <em>methods</em> <em>are</em> <em>tested</em> <em>under</em> <em>a</em> <em>certain</em>
                  <em>ViZDoom</em> <em>combat</em> <em>scenario</em> <em>using</em> <em>a</em> <em>competitive</em> <em>deep</em>
                  <em>recurrent</em> <em>Q</em>-<em>network</em> (<em>DRQN</em>) <em>architecture</em>. <em>In</em>
                  <em>addition</em>, <em>an</em> <em>ensembling</em> <em>technique</em> <em>known</em> <em>as</em> <em>snapshot</em>
                  <em>ensembling</em> <em>is</em> <em>employed</em> <em>using</em> <em>a</em> <em>specific</em> <em>annealed</em>
                  <em>learning</em> <em>rate</em> <em>to</em> <em>observe</em> <em>differences</em> <em>in</em> <em>ensembling</em>
                  <em>efficacy</em> <em>under</em> <em>these</em> <em>two</em> <em>methods</em>. <em>Annealed</em> <em>learning</em>
                  <em>rates</em> <em>are</em> <em>important</em> <em>in</em> <em>general</em> <em>to</em> <em>the</em> <em>training</em> <em>of</em>
                  <em>deep</em> <em>neural</em> <em>network</em> <em>models</em>, <em>as</em> <em>they</em> <em>shake</em> <em>up</em> <em>the</em>
                  <em>status</em>-<em>quo</em> <em>and</em> <em>counter</em> <em>a</em> <em>model</em>'<em>s</em> <em>tending</em> <em>towards</em>
                  <em>local</em> <em>optima</em>. <em>While</em> <em>both</em> <em>variants</em> <em>show</em> <em>performance</em>
                  <em>exceeding</em> <em>those</em> <em>of</em> <em>built</em>-<em>in</em> <em>AI</em> <em>agents</em> <em>of</em> <em>the</em> <em>game</em>,
                  <em>the</em> <em>known</em> <em>stabilizing</em> <em>effects</em> <em>of</em> <em>double</em>-<em>Q</em> <em>learning</em>
                  <em>are</em> <em>illustrated</em>, <em>and</em> <em>priority</em> <em>experience</em> <em>replay</em> <em>is</em>
                  <em>again</em> <em>validated</em> <em>in</em> <em>its</em> <em>usefulness</em> <em>by</em> <em>showing</em>
                  <em>immediate</em> <em>results</em> <em>early</em> <em>on</em> <em>in</em> <em>agent</em> <em>development</em>,
                  <em>with</em> <em>the</em> <em>caveat</em> <em>that</em> <em>value</em> <em>overestimation</em> <em>is</em>
                  <em>accelerated</em> <em>in</em> <em>this</em> <em>case</em>. <em>In</em> <em>addition</em>, <em>some</em> <em>unique</em>
                  <em>behaviors</em> <em>are</em> <em>observed</em> <em>to</em> <em>develop</em> <em>for</em> <em>priority</em>
                  <em>experience</em> <em>replay</em> (<em>PER</em>) <em>and</em> <em>double</em>-<em>Q</em> (<em>DDQ</em>) <em>variants</em>,
                  <em>and</em> <em>snapshot</em> <em>ensembling</em> <em>of</em> <em>both</em> <em>PER</em> <em>and</em> <em>DDQ</em> <em>proves</em> <em>a</em>
                  <em>valuable</em> <em>method</em> <em>for</em> <em>improving</em> <em>performance</em> <em>of</em> <em>the</em>
                  <em>ViZDoom</em> <em>Marine</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="mao-2018-deepj">86</a>]
</td>
<td class="bibtexitem">
<em>Huanru</em>&nbsp;<em>Henry</em> <em>Mao</em>, <em>Taylor</em> <em>Shin</em>, <em>and</em> <em>Garrison</em>&nbsp;<em>W</em>. <em>Cottrell</em>.
 <em>Deepj</em>: <em>Style</em>-<em>specific</em> <em>music</em> <em>generation</em>.
 <em>CoRR</em>, 2018.
[&nbsp;<a href="index_bib.html#mao-2018-deepj">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00887">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00887v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Recent</em> <em>advances</em> <em>in</em> <em>deep</em> <em>neural</em> <em>networks</em> <em>have</em> <em>enabled</em>
                  <em>algorithms</em> <em>to</em> <em>compose</em> <em>music</em> <em>that</em> <em>is</em> <em>comparable</em> <em>to</em>
                  <em>music</em> <em>composed</em> <em>by</em> <em>humans</em>. <em>However</em>, <em>few</em> <em>algorithms</em>
                  <em>allow</em> <em>the</em> <em>user</em> <em>to</em> <em>generate</em> <em>music</em> <em>with</em> <em>tunable</em>
                  <em>parameters</em>. <em>The</em> <em>ability</em> <em>to</em> <em>tune</em> <em>properties</em> <em>of</em>
                  <em>generated</em> <em>music</em> <em>will</em> <em>yield</em> <em>more</em> <em>practical</em> <em>benefits</em>
                  <em>for</em> <em>aiding</em> <em>artists</em>, <em>filmmakers</em>, <em>and</em> <em>composers</em> <em>in</em>
                  <em>their</em> <em>creative</em> <em>tasks</em>. <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>introduce</em>
                  <em>DeepJ</em> - <em>an</em> <em>end</em>-<em>to</em>-<em>end</em> <em>generative</em> <em>model</em> <em>that</em> <em>is</em>
                  <em>capable</em> <em>of</em> <em>composing</em> <em>music</em> <em>conditioned</em> <em>on</em> <em>a</em> <em>specific</em>
                  <em>mixture</em> <em>of</em> <em>composer</em> <em>styles</em>. <em>Our</em> <em>innovations</em> <em>include</em>
                  <em>methods</em> <em>to</em> <em>learn</em> <em>musical</em> <em>style</em> <em>and</em> <em>music</em> <em>dynamics</em>.
                  <em>We</em> <em>use</em> <em>our</em> <em>model</em> <em>to</em> <em>demonstrate</em> <em>a</em> <em>simple</em> <em>technique</em>
                  <em>for</em> <em>controlling</em> <em>the</em> <em>style</em> <em>of</em> <em>generated</em> <em>music</em> <em>as</em> <em>a</em>
                  <em>proof</em> <em>of</em> <em>concept</em>. <em>Evaluation</em> <em>of</em> <em>our</em> <em>model</em> <em>using</em>
                  <em>human</em> <em>raters</em> <em>shows</em> <em>that</em> <em>we</em> <em>have</em> <em>improved</em> <em>over</em> <em>the</em>
                  <em>Biaxial</em> <em>LSTM</em> <em>approach</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="marcus-2018-deep-learn">87</a>]
</td>
<td class="bibtexitem">
<em>Gary</em> <em>Marcus</em>.
 <em>Deep</em> <em>learning</em>: <em>A</em> <em>critical</em> <em>appraisal</em>.
 <em>CoRR</em>, 2018.
[&nbsp;<a href="index_bib.html#marcus-2018-deep-learn">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00631v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.00631v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Although</em> <em>deep</em> <em>learning</em> <em>has</em> <em>historical</em> <em>roots</em> <em>going</em>
                  <em>back</em> <em>decades</em>, <em>neither</em> <em>the</em> <em>term</em> "<em>deep</em> <em>learning</em>" <em>nor</em>
                  <em>the</em> <em>approach</em> <em>was</em> <em>popular</em> <em>just</em> <em>over</em> <em>five</em> <em>years</em> <em>ago</em>,
                  <em>when</em> <em>the</em> <em>field</em> <em>was</em> <em>reignited</em> <em>by</em> <em>papers</em> <em>such</em> <em>as</em>
                  <em>Krizhevsky</em>, <em>Sutskever</em> <em>and</em> <em>Hinton</em>'<em>s</em> <em>now</em> <em>classic</em>
                  (2012) <em>deep</em> <em>network</em> <em>model</em> <em>of</em> <em>Imagenet</em>. <em>What</em> <em>has</em> <em>the</em>
                  <em>field</em> <em>discovered</em> <em>in</em> <em>the</em> <em>five</em> <em>subsequent</em> <em>years</em>?
                  <em>Against</em> <em>a</em> <em>background</em> <em>of</em> <em>considerable</em> <em>progress</em> <em>in</em>
                  <em>areas</em> <em>such</em> <em>as</em> <em>speech</em> <em>recognition</em>, <em>image</em> <em>recognition</em>,
                  <em>and</em> <em>game</em> <em>playing</em>, <em>and</em> <em>considerable</em> <em>enthusiasm</em> <em>in</em> <em>the</em>
                  <em>popular</em> <em>press</em>, <em>I</em> <em>present</em> <em>ten</em> <em>concerns</em> <em>for</em> <em>deep</em>
                  <em>learning</em>, <em>and</em> <em>suggest</em> <em>that</em> <em>deep</em> <em>learning</em> <em>must</em> <em>be</em>
                  <em>supplemented</em> <em>by</em> <em>other</em> <em>techniques</em> <em>if</em> <em>we</em> <em>are</em> <em>to</em> <em>reach</em>
                  <em>artificial</em> <em>general</em> <em>intelligence</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="salehinejad-2017-recen-advan">88</a>]
</td>
<td class="bibtexitem">
<em>Hojjat</em> <em>Salehinejad</em>, <em>Julianne</em> <em>Baarbe</em>, <em>Sharan</em> <em>Sankar</em>, <em>Joseph</em> <em>Barfett</em>, <em>Errol</em>
  <em>Colak</em>, <em>and</em> <em>Shahrokh</em> <em>Valaee</em>.
 <em>Recent</em> <em>advances</em> <em>in</em> <em>recurrent</em> <em>neural</em> <em>networks</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#salehinejad-2017-recen-advan">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.01078v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1801.01078v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Recurrent</em> <em>neural</em> <em>networks</em> (<em>RNNs</em>) <em>are</em> <em>capable</em> <em>of</em>
                  <em>learning</em> <em>features</em> <em>and</em> <em>long</em> <em>term</em> <em>dependencies</em> <em>from</em>
                  <em>sequential</em> <em>and</em> <em>time</em>-<em>series</em> <em>data</em>. <em>The</em> <em>RNNs</em> <em>have</em> <em>a</em>
                  <em>stack</em> <em>of</em> <em>non</em>-<em>linear</em> <em>units</em> <em>where</em> <em>at</em> <em>least</em> <em>one</em>
                  <em>connection</em> <em>between</em> <em>units</em> <em>forms</em> <em>a</em> <em>directed</em> <em>cycle</em>. <em>A</em>
                  <em>well</em>-<em>trained</em> <em>RNN</em> <em>can</em> <em>model</em> <em>any</em> <em>dynamical</em> <em>system</em>;
                  <em>however</em>, <em>training</em> <em>RNNs</em> <em>is</em> <em>mostly</em> <em>plagued</em> <em>by</em> <em>issues</em>
                  <em>in</em> <em>learning</em> <em>long</em>-<em>term</em> <em>dependencies</em>. <em>In</em> <em>this</em> <em>paper</em>,
                  <em>we</em> <em>present</em> <em>a</em> <em>survey</em> <em>on</em> <em>RNNs</em> <em>and</em> <em>several</em> <em>new</em> <em>advances</em>
                  <em>for</em> <em>newcomers</em> <em>and</em> <em>professionals</em> <em>in</em> <em>the</em> <em>field</em>. <em>The</em>
                  <em>fundamentals</em> <em>and</em> <em>recent</em> <em>advances</em> <em>are</em> <em>explained</em> <em>and</em>
                  <em>the</em> <em>research</em> <em>challenges</em> <em>are</em> <em>introduced</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="cheng-2017-survey-model">89</a>]
</td>
<td class="bibtexitem">
<em>Yu</em>&nbsp;<em>Cheng</em>, <em>Duo</em> <em>Wang</em>, <em>Pan</em> <em>Zhou</em>, <em>and</em> <em>Tao</em> <em>Zhang</em>.
 <em>A</em> <em>survey</em> <em>of</em> <em>model</em> <em>compression</em> <em>and</em> <em>acceleration</em> <em>for</em> <em>deep</em> <em>neural</em>
  <em>networks</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#cheng-2017-survey-model">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.09282v5">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1710.09282v5">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Deep</em> <em>convolutional</em> <em>neural</em> <em>networks</em> (<em>CNNs</em>) <em>have</em>
                  <em>recently</em> <em>achieved</em> <em>great</em> <em>success</em> <em>in</em> <em>many</em> <em>visual</em>
                  <em>recognition</em> <em>tasks</em>. <em>However</em>, <em>existing</em> <em>deep</em>
                  <em>convolutional</em> <em>neural</em> <em>network</em> <em>models</em> <em>are</em>
                  <em>computationally</em> <em>expensive</em> <em>and</em> <em>memory</em> <em>intensive</em>,
                  <em>hindering</em> <em>their</em> <em>deployment</em> <em>in</em> <em>devices</em> <em>with</em> <em>low</em>
                  <em>memory</em> <em>resources</em> <em>or</em> <em>in</em> <em>applications</em> <em>with</em> <em>strict</em>
                  <em>latency</em> <em>requirements</em>. <em>Therefore</em>, <em>a</em> <em>natural</em> <em>thought</em>
                  <em>is</em> <em>to</em> <em>perform</em> <em>model</em> <em>compression</em> <em>and</em> <em>acceleration</em> <em>in</em>
                  <em>deep</em> <em>networks</em> <em>without</em> <em>significantly</em> <em>decreasing</em> <em>the</em>
                  <em>model</em> <em>performance</em>. <em>During</em> <em>the</em> <em>past</em> <em>few</em> <em>years</em>,
                  <em>tremendous</em> <em>progresses</em> <em>have</em> <em>been</em> <em>made</em> <em>in</em> <em>this</em> <em>area</em>.
                  <em>In</em> <em>this</em> <em>paper</em>, <em>we</em> <em>survey</em> <em>the</em> <em>recent</em> <em>advanced</em>
                  <em>techniques</em> <em>for</em> <em>compacting</em> <em>and</em> <em>accelerating</em> <em>CNNs</em>
                  <em>model</em> <em>developed</em>. <em>These</em> <em>techniques</em> <em>are</em> <em>roughly</em>
                  <em>categorized</em> <em>into</em> <em>four</em> <em>schemes</em>: <em>parameter</em> <em>pruning</em> <em>and</em>
                  <em>sharing</em>, <em>low</em>-<em>rank</em> <em>factorization</em>, <em>transfered</em>/<em>compact</em>
                  <em>convolutional</em> <em>filters</em> <em>and</em> <em>knowledge</em> <em>distillation</em>.
                  <em>Methods</em> <em>of</em> <em>parameter</em> <em>pruning</em> <em>and</em> <em>sharing</em> <em>will</em> <em>be</em>
                  <em>described</em> <em>at</em> <em>the</em> <em>beginning</em>, <em>after</em> <em>that</em> <em>the</em> <em>other</em>
                  <em>techniques</em> <em>will</em> <em>be</em> <em>introduced</em>. <em>For</em> <em>each</em> <em>scheme</em>, <em>we</em>
                  <em>provide</em> <em>insightful</em> <em>analysis</em> <em>regarding</em> <em>the</em>
                  <em>performance</em>, <em>related</em> <em>applications</em>, <em>advantages</em> <em>and</em>
                  <em>drawbacks</em> <em>etc</em>. <em>Then</em> <em>we</em> <em>will</em> <em>go</em> <em>through</em> <em>a</em> <em>few</em> <em>very</em>
                  <em>recent</em> <em>additional</em> <em>successful</em> <em>methods</em>, <em>for</em> <em>example</em>,
                  <em>dynamic</em> <em>networks</em> <em>and</em> <em>stochastic</em> <em>depths</em> <em>networks</em>.
                  <em>After</em> <em>that</em>, <em>we</em> <em>survey</em> <em>the</em> <em>evaluation</em> <em>matrix</em>, <em>main</em>
                  <em>datasets</em> <em>used</em> <em>for</em> <em>evaluating</em> <em>the</em> <em>model</em> <em>performance</em>
                  <em>and</em> <em>recent</em> <em>benchmarking</em> <em>efforts</em>. <em>Finally</em> <em>we</em> <em>conclude</em>
                  <em>this</em> <em>paper</em>, <em>discuss</em> <em>remaining</em> <em>challenges</em> <em>and</em>
                  <em>possible</em> <em>directions</em> <em>in</em> <em>this</em> <em>topic</em>.
</font></blockquote>
<p>
</td>
</tr>


<tr valign="top">
<td align="right" class="bibtexnumber">
[<a name="yu-2017-artif-intel-statis">90</a>]
</td>
<td class="bibtexitem">
<em>Bin</em> <em>Yu</em> <em>and</em> <em>Karl</em> <em>Kumbier</em>.
 <em>Artificial</em> <em>intelligence</em> <em>and</em> <em>statistics</em>.
 <em>CoRR</em>, 2017.
[&nbsp;<a href="index_bib.html#yu-2017-artif-intel-statis">bib</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.03779v1">arXiv</a>&nbsp;| 
<a href="http://arxiv.org/abs/1712.03779v1">http</a>&nbsp;]
<blockquote><font size="-1">
<em>Artificial</em> <em>intelligence</em> (<em>AI</em>) <em>is</em> <em>intrinsically</em>
                  <em>data</em>-<em>driven</em>. <em>It</em> <em>calls</em> <em>for</em> <em>the</em> <em>application</em> <em>of</em>
                  <em>statistical</em> <em>concepts</em> <em>through</em> <em>human</em>-<em>machine</em>
                  <em>collaboration</em> <em>during</em> <em>generation</em> <em>of</em> <em>data</em>, <em>development</em>
                  <em>of</em> <em>algorithms</em>, <em>and</em> <em>evaluation</em> <em>of</em> <em>results</em>. <em>This</em> <em>paper</em>
                  <em>discusses</em> <em>how</em> <em>such</em> <em>human</em>-<em>machine</em> <em>collaboration</em> <em>can</em>
                  <em>be</em> <em>approached</em> <em>through</em> <em>the</em> <em>statistical</em> <em>concepts</em> <em>of</em>
                  <em>population</em>, <em>question</em> <em>of</em> <em>interest</em>, <em>representativeness</em>
                  <em>of</em> <em>training</em> <em>data</em>, <em>and</em> <em>scrutiny</em> <em>of</em> <em>results</em> (<em>PQRS</em>).
                  <em>The</em> <em>PQRS</em> <em>workflow</em> <em>provides</em> <em>a</em> <em>conceptual</em> <em>framework</em>
                  <em>for</em> <em>integrating</em> <em>statistical</em> <em>ideas</em> <em>with</em> <em>human</em> <em>input</em>
                  <em>into</em> <em>AI</em> <em>products</em> <em>and</em> <em>research</em>. <em>These</em> <em>ideas</em> <em>include</em>
                  <em>experimental</em> <em>design</em> <em>principles</em> <em>of</em> <em>randomization</em> <em>and</em>
                  <em>local</em> <em>control</em> <em>as</em> <em>well</em> <em>as</em> <em>the</em> <em>principle</em> <em>of</em> <em>stability</em>
                  <em>to</em> <em>gain</em> <em>reproducibility</em> <em>and</em> <em>interpretability</em> <em>of</em>
                  <em>algorithms</em> <em>and</em> <em>data</em> <em>results</em>. <em>We</em> <em>discuss</em> <em>the</em> <em>use</em> <em>of</em>
                  <em>these</em> <em>principles</em> <em>in</em> <em>the</em> <em>contexts</em> <em>of</em> <em>self</em>-<em>driving</em>
                  <em>cars</em>, <em>automated</em> <em>medical</em> <em>diagnoses</em>, <em>and</em> <em>examples</em> <em>from</em>
                  <em>the</em> <em>authors</em>' <em>collaborative</em> <em>research</em>.
</font></blockquote>
<p>
</td>
</tr>
</table><hr><p><em>This file was generated by
<a href="http://www.lri.fr/~filliatr/bibtex2html/">bibtex2html</a> 1.98.</em></p>
</body>
</html>
